{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pytorch BERT baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, I convert https://www.kaggle.com/akensert/bert-base-tf2-0-minimalistic into pytorch version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please upvote the kernel if you find it helpful**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are not allowed to use internet I've created required datasets and commands to setup Hugging Face Transformers setup in offline mode. You can find the required github codebases in the datasets.\n",
    "\n",
    "* sacremoses dependency - https://www.kaggle.com/axel81/sacremoses\n",
    "* transformers - https://www.kaggle.com/axel81/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from albumentations.core.transforms_interface import DualTransform, BasicTransform\n",
    "# !pip install ./sacremoses/sacremoses-master/\n",
    "# !pip install ./transformers/transformers-master/\n",
    "\n",
    "STRIDE = 1\n",
    "def is_jupyter():\n",
    "    try:\n",
    "        ipy_str = str(type(get_ipython()))\n",
    "        if 'zmqshell' in ipy_str:\n",
    "            return True\n",
    "        \n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Imports\n",
    "\n",
    "I've added imports that will be used in training too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kb/jig_env/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "/home/kb/jig_env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n",
      "/home/kb/jig_env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from random import shuffle as shfl\n",
    "from auc import MyAUCCallback\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max.columns', 500)\n",
    "import numpy as np\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] ='1'\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-10.1/lib64'\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from shutil import copyfile\n",
    "from catalyst.dl import SupervisedRunner, AlchemyLogger, CriterionCallback\n",
    "from catalyst.dl.callbacks.metrics import AUCCallback\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler,Dataset\n",
    "batch_size =96\n",
    "token = \"d1dd16f08d518293bcbeddd313b49aa4\"\n",
    "DATA_DIR = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on desktop\n"
     ]
    }
   ],
   "source": [
    "if os.uname()[1] == 'kb-Z370P-D3':\n",
    "    # desktop\n",
    "    LOG_PATH = '/media/ssd/logs/jigsaw'\n",
    "    SERVER = False\n",
    "    print('Working on desktop')\n",
    "elif os.uname()[1] == 'kb-server':\n",
    "    # server\n",
    "    LOG_PATH = '/home/kb/logs/jigsaw'\n",
    "    SERVER = True\n",
    "    print('Working on server')\n",
    "else:\n",
    "    raise Exception('which hostname???')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\[\n",
      "\n",
      "<>:5: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\d\n",
      "\n",
      "<>:6: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = text.fillna(\"fillna\")#.str.lower()\n",
    "    text = text.map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "    text = text.map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "    text = text.map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    text = text.map(lambda x: re.sub(\"\\(http://.*?\\s\\(http://.*\\)\",'',str(x)))\n",
    "    return text\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"couldnt\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"doesnt\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"havent\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"shouldnt\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"thats\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"theres\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"theyre\":  \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def handle_contractions(x):\n",
    "    x = tokenizer.tokenize(x)\n",
    "    return x\n",
    "\n",
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x).replace(\"\\n\",\"\")\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def clean_data(df, columns: list):\n",
    "    for col in columns:\n",
    "        pass\n",
    "        df[col] = df[col].apply(lambda x: clean_numbers(x))\n",
    "        df[col] = clean(df[col])\n",
    "        df[col] = df[col].apply(lambda x: clean_text(x)) \n",
    "        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n",
    "# #         df[col] = df[col].apply(lambda x: handle_contractions(x))  \n",
    "#         df[col] = df[col].apply(lambda x: fix_quote(x))   \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPTransform(BasicTransform):\n",
    "    \"\"\" Transform for nlp task.\"\"\"\n",
    "    LANGS = {\n",
    "        'en': 'english',\n",
    "        'it': 'italian', \n",
    "        'fr': 'french', \n",
    "        'es': 'spanish',\n",
    "        'tr': 'turkish', \n",
    "        'ru': 'russian',\n",
    "        'pt': 'portuguese'\n",
    "    }\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return {\"data\": self.apply}\n",
    "    \n",
    "    def update_params(self, params, **kwargs):\n",
    "        if hasattr(self, \"interpolation\"):\n",
    "            params[\"interpolation\"] = self.interpolation\n",
    "        if hasattr(self, \"fill_value\"):\n",
    "            params[\"fill_value\"] = self.fill_value\n",
    "        return params\n",
    "\n",
    "    def get_sentences(self, text, lang='en'):\n",
    "        return sent_tokenize(text, self.LANGS.get(lang, 'english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Sentence4>. <Sentence1>. <Sentence5>. <Sentence2>. <Sentence3>. <Sentence6>.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ShuffleSentencesTransform(NLPTransform):\n",
    "    \"\"\" Do shuffle by sentence \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ShuffleSentencesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        sentences = self.get_sentences(text, lang)\n",
    "        random.shuffle(sentences)\n",
    "        return ' '.join(sentences), lang\n",
    "    \n",
    "transform = ShuffleSentencesTransform(p=1.0)\n",
    "\n",
    "text = '<Sentence1>. <Sentence2>. <Sentence3>. <Sentence4>. <Sentence5>. <Sentence6>.'\n",
    "lang = 'en'\n",
    "\n",
    "transform(data=(text, lang))['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Sentence1>. <Sentence2>. <Sentence4>. <Sentence5>.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ExcludeDuplicateSentencesTransform(NLPTransform):\n",
    "    \"\"\" Exclude equal sentences \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeDuplicateSentencesTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        sentences = []\n",
    "        for sentence in self.get_sentences(text, lang):\n",
    "            sentence = sentence.strip()\n",
    "            if sentence not in sentences:\n",
    "                sentences.append(sentence)\n",
    "        return ' '.join(sentences), lang\n",
    "transform = ExcludeDuplicateSentencesTransform(p=1.0)\n",
    "\n",
    "text = '<Sentence1>. <Sentence2>. <Sentence4>. <Sentence4>. <Sentence5>. <Sentence5>.'\n",
    "lang = 'en'\n",
    "\n",
    "transform(data=(text, lang))['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Word> <Word> <Word> <Word> <Word> <Word> <Word> <Word> <Word> <Word>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ExcludeNumbersTransform(NLPTransform):\n",
    "    \"\"\" exclude any numbers \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeNumbersTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'[0-9]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text, lang\n",
    "transform = ExcludeNumbersTransform(p=1.0)\n",
    "\n",
    "text = '<Word1> <Word2> <Word3> <Word4> <Word5> <Word6> <Word7> <Word8> <Word9> <Word10>'\n",
    "lang = 'en'\n",
    "\n",
    "transform(data=(text, lang))['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Word1> <Word2> <Word3> <Word4> <Word5> <Word6> <Word7> <Word8> <Word9> <Word10>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ExcludeHashtagsTransform(NLPTransform):\n",
    "    \"\"\" Exclude any hashtags with # \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeHashtagsTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'#[\\S]+\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text, lang\n",
    "transform = ExcludeHashtagsTransform(p=1.0)\n",
    "\n",
    "text = '<Word1> <Word2> <Word3> #kaggle <Word4> <Word5> <Word6> <Word7> <Word8> <Word9> <Word10>'\n",
    "lang = 'en'\n",
    "\n",
    "transform(data=(text, lang))['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Word1> <Word2> <Word3> <Word4> <Word5> <Word6> <Word7> <Word8> <Word9> <Word10>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ExcludeUsersMentionedTransform(NLPTransform):\n",
    "    \"\"\" Exclude @users \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeUsersMentionedTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'@[\\S]+\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text, lang\n",
    "transform = ExcludeUsersMentionedTransform(p=1.0)\n",
    "\n",
    "text = '<Word1> <Word2> <Word3> @kaggle <Word4> <Word5> <Word6> <Word7> <Word8> <Word9> <Word10>'\n",
    "lang = 'en'\n",
    "\n",
    "transform(data=(text, lang))['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Word1> <Word2> <Word3> <Word4> <Word6> <Word7> <Word8> <Word9> <Word10>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ExcludeUrlsTransform(NLPTransform):\n",
    "    \"\"\" Exclude urls \"\"\"\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        super(ExcludeUrlsTransform, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        text = re.sub(r'https?\\S+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text, lang\n",
    "transform = ExcludeUrlsTransform(p=1.0)\n",
    "\n",
    "text = '<Word1> <Word2> <Word3> <Word4> https://www.kaggle.com/shonenkov/nlp-albumentations/ <Word6> <Word7> <Word8> <Word9> <Word10>'\n",
    "lang = 'en'\n",
    "\n",
    "transform(data=(text, lang))['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Word1> <Word2> <Word3> <Word4> <Word6> <Word5> <Word7> <Word8> <Word9> <Word10>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SwapWordsTransform(NLPTransform):\n",
    "    \"\"\" Swap words next to each other \"\"\"\n",
    "    def __init__(self, swap_distance=1, swap_probability=0.1, always_apply=False, p=0.5):\n",
    "        \"\"\"  \n",
    "        swap_distance - distance for swapping words\n",
    "        swap_probability - probability of swapping for one word\n",
    "        \"\"\"\n",
    "        super(SwapWordsTransform, self).__init__(always_apply, p)\n",
    "        self.swap_distance = swap_distance\n",
    "        self.swap_probability = swap_probability\n",
    "        self.swap_range_list = list(range(1, swap_distance+1))\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        words = text.split()\n",
    "        words_count = len(words)\n",
    "        if words_count <= 1:\n",
    "            return text, lang\n",
    "\n",
    "        new_words = {}\n",
    "        for i in range(words_count):\n",
    "            if random.random() > self.swap_probability:\n",
    "                new_words[i] = words[i]\n",
    "                continue\n",
    "    \n",
    "            if i < self.swap_distance:\n",
    "                new_words[i] = words[i]\n",
    "                continue\n",
    "    \n",
    "            swap_idx = i - random.choice(self.swap_range_list)\n",
    "            new_words[i] = new_words[swap_idx]\n",
    "            new_words[swap_idx] = words[i]\n",
    "\n",
    "        return ' '.join([v for k, v in sorted(new_words.items(), key=lambda x: x[0])]), lang\n",
    "\n",
    "transform = SwapWordsTransform(p=1.0, swap_distance=1, swap_probability=0.2)\n",
    "\n",
    "text = '<Word1> <Word2> <Word3> <Word4> <Word5> <Word6> <Word7> <Word8> <Word9> <Word10>'\n",
    "lang = 'en'\n",
    "\n",
    "transform(data=(text, lang))['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Word1> <Word2> <Word3> <Word4> <Word6> <Word7> <Word8> <Word9> <Word10>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CutOutWordsTransform(NLPTransform):\n",
    "    \"\"\" Remove random words \"\"\"\n",
    "    def __init__(self, cutout_probability=0.05, always_apply=False, p=0.5):\n",
    "        super(CutOutWordsTransform, self).__init__(always_apply, p)\n",
    "        self.cutout_probability = cutout_probability\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "        words = text.split()\n",
    "        words_count = len(words)\n",
    "        if words_count <= 1:\n",
    "            return text, lang\n",
    "        \n",
    "        new_words = []\n",
    "        for i in range(words_count):\n",
    "            if random.random() < self.cutout_probability:\n",
    "                continue\n",
    "            new_words.append(words[i])\n",
    "\n",
    "        if len(new_words) == 0:\n",
    "            return words[random.randint(0, words_count-1)], lang\n",
    "\n",
    "        return ' '.join(new_words), lang\n",
    "transform = CutOutWordsTransform(p=1.0, cutout_probability=0.2)\n",
    "\n",
    "text = '<Word1> <Word2> <Word3> <Word4> <Word5> <Word6> <Word7> <Word8> <Word9> <Word10>'\n",
    "lang = 'en'\n",
    "\n",
    "transform(data=(text, lang))['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNonToxicSentencesTransform(NLPTransform):\n",
    "    \"\"\" Add random non toxic statement \"\"\"\n",
    "    def __init__(self, non_toxic_sentences, sentence_range=(1, 3), always_apply=False, p=0.5):\n",
    "        super(AddNonToxicSentencesTransform, self).__init__(always_apply, p)\n",
    "        self.sentence_range = sentence_range\n",
    "        self.non_toxic_sentences = non_toxic_sentences\n",
    "\n",
    "    def apply(self, data, **params):\n",
    "        text, lang = data\n",
    "\n",
    "        sentences = self.get_sentences(text, lang)\n",
    "        for i in range(random.randint(*self.sentence_range)):\n",
    "            sentences.append(random.choice(self.non_toxic_sentences))\n",
    "        \n",
    "        random.shuffle(sentences)\n",
    "        return ' '.join(sentences), lang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 895/895 [00:00<00:00, 7591.82it/s]\n"
     ]
    }
   ],
   "source": [
    "nlp_transform = NLPTransform()\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv', nrows=1000)\n",
    "df = df[df.toxic == 0]\n",
    "df['lang'] = 'en'\n",
    "non_toxic_sentences = set()\n",
    "for comment_text in tqdm(df['comment_text'], total=df.shape[0]):\n",
    "    non_toxic_sentences.update(nlp_transform.get_sentences(comment_text), 'en')\n",
    "\n",
    "transform = AddNonToxicSentencesTransform(non_toxic_sentences=list(non_toxic_sentences), p=1.0, sentence_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "\n",
    "def get_train_transforms():\n",
    "    return albumentations.Compose([\n",
    "        ExcludeDuplicateSentencesTransform(p=0.9),  # here not p=1.0 because your nets should get some difficulties\n",
    "        albumentations.OneOf([\n",
    "            AddNonToxicSentencesTransform(non_toxic_sentences=list(non_toxic_sentences), p=0.8, sentence_range=(1,3)),\n",
    "            ShuffleSentencesTransform(p=0.8),\n",
    "        ]),\n",
    "        ExcludeNumbersTransform(p=0.8),\n",
    "        ExcludeHashtagsTransform(p=0.5),\n",
    "        ExcludeUsersMentionedTransform(p=0.9),\n",
    "        ExcludeUrlsTransform(p=0.9),\n",
    "        CutOutWordsTransform(p=0.1),\n",
    "        SwapWordsTransform(p=0.1),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n",
    "train2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\n",
    "train2.toxic = train2.toxic.round().astype(int)\n",
    "\n",
    "df_valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
    "sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train1 with a subset of train2\n",
    "df_train = pd.concat([\n",
    "    train1[['comment_text', 'toxic']],\n",
    "    train2[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>toxic</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>640</td>\n",
       "      <td>tr</td>\n",
       "      <td>0</td>\n",
       "      <td>For some reason I changed as a source, the Tur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6902</th>\n",
       "      <td>6902</td>\n",
       "      <td>tr</td>\n",
       "      <td>0</td>\n",
       "      <td>There is a problem in the exclusive pattern of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>1554</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "      <td>I read but mostly Madaki is always arrogant an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>6664</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>Brutes are characterized as very fools enemies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>626</td>\n",
       "      <td>tr</td>\n",
       "      <td>1</td>\n",
       "      <td>Gay men in gay men, gay women women. Is that ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id lang  toxic                                       comment_text\n",
       "640    640   tr      0  For some reason I changed as a source, the Tur...\n",
       "6902  6902   tr      0  There is a problem in the exclusive pattern of...\n",
       "1554  1554   it      0  I read but mostly Madaki is always arrogant an...\n",
       "6664  6664   es      0  Brutes are characterized as very fools enemies...\n",
       "626    626   tr      1   Gay men in gay men, gay women women. Is that ..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_en = shuffle(pd.read_csv(DATA_DIR + 'validation_en.csv'))\n",
    "df_valid_en = df_valid_en.drop(['comment_text'],axis=1).rename(columns={'comment_text_en':'comment_text'})\n",
    "# df_valid_en = clean_data(df_valid_en, ['comment_text'])\n",
    "df_valid_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th>toxic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">es</th>\n",
       "      <th>0</th>\n",
       "      <td>2078</td>\n",
       "      <td>2078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>422</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">it</th>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tr</th>\n",
       "      <th>0</th>\n",
       "      <td>2680</td>\n",
       "      <td>2680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>320</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  comment_text\n",
       "lang toxic                    \n",
       "es   0      2078          2078\n",
       "     1       422           422\n",
       "it   0      2012          2012\n",
       "     1       488           488\n",
       "tr   0      2680          2680\n",
       "     1       320           320"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_en.groupby(['lang', 'toxic']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv(DATA_DIR + 'test.csv')\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.groupby(['lang']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  toxic\n",
       "0   0    0.5\n",
       "1   1    0.5\n",
       "2   2    0.5\n",
       "3   3    0.5\n",
       "4   4    0.5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'toxic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import time\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "#import torch.utils.data as data\n",
    "from torchvision import datasets, models, transforms\n",
    "from transformers import *\n",
    "import random\n",
    "from math import floor, ceil\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "MAX_LEN = 96#192#192#512\n",
    "SEP_TOKEN_ID = 102\n",
    "\n",
    "class QuestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, train_mode=True, labeled=True, train_transforms=None):\n",
    "        \n",
    "        self.train_transforms = train_transforms    \n",
    "        self.df = df\n",
    "        if train_mode:\n",
    "            self.labels = df.toxic.values\n",
    "            self.toxic_inds = np.where(self.labels==1)[0]\n",
    "            self.normal_inds = np.where(self.labels==0)[0]            \n",
    "            \n",
    "            \n",
    "            print(f'Here is {len(self.labels)} samples, {len(self.toxic_inds)} samples and {len(self.normal_inds)} samples')\n",
    "            print(f'Class balance is {len(self.toxic_inds)/len(self.labels):.2f}')\n",
    "            \n",
    "        self.train_mode = train_mode\n",
    "        self.labeled = labeled\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#         self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "#         self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')#, \n",
    "#                                                              return_attention_masks=False, \n",
    "#                                                                 return_token_type_ids=False,\n",
    "#                                                                 pad_to_max_length=True,\n",
    "#                                                                 max_length=MAX_LEN)\n",
    "#         self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased',\n",
    "#                                                                 do_lower_case=False,\n",
    "#                                                                 do_basic_tokenize=True,\n",
    "#                                                                 never_split=None,\n",
    "#                                                                 unk_token='[UNK]',\n",
    "#                                                                 sep_token='[SEP]',\n",
    "#                                                                 pad_token='[PAD]',\n",
    "#                                                                 cls_token='[CLS]',\n",
    "#                                                                 mask_token='[MASK]',\n",
    "#                                                                 tokenize_chinese_chars=True,)\n",
    "        #distil\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        token_ids = self.get_token_ids(row)\n",
    "        \n",
    "        if self.labeled:\n",
    "            labels = self.get_label(row)\n",
    "            return {'features': token_ids, 'targets': labels}\n",
    "\n",
    "        else:\n",
    "            return {'features': token_ids}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def trim_input(self, text, max_sequence_length=MAX_LEN):\n",
    "        t = self.tokenizer.tokenize(text)\n",
    "        t_len = len(t)\n",
    "\n",
    "        if t_len + 2 > max_sequence_length:\n",
    "\n",
    "            t_new_len = int(max_sequence_length) - 2\n",
    "\n",
    "            t = t[:t_new_len]\n",
    "\n",
    "        return t\n",
    "        \n",
    "    def get_token_ids(self, row):\n",
    "        text = row.comment_text\n",
    "        if self.train_transforms:\n",
    "            lang = 'en' if 'lang' not in list(row.keys().values) else row.lang\n",
    "            text, _ = self.train_transforms(data=(row.comment_text, lang))['data']\n",
    "        token_ids = self.tokenizer.encode(text, max_length=1024)\n",
    "        if len(token_ids) < MAX_LEN:\n",
    "            ids = torch.tensor(token_ids + [0] * (MAX_LEN - len(token_ids)))\n",
    "        else:\n",
    "            ind_beg = random.randint(0, len(token_ids)-MAX_LEN)\n",
    "            ids = torch.tensor(token_ids[ind_beg:ind_beg+MAX_LEN])\n",
    "            \n",
    "        return ids\n",
    "\n",
    "    def get_label(self, row):\n",
    "#         label = torch.tensor(row[target_column].astype(np.long))\n",
    "        label = np.round(row[target_column])\n",
    "        return torch.tensor([1-label, label]).float()\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        token_ids = torch.stack([x[0] for x in batch])\n",
    "\n",
    "        if self.labeled:\n",
    "            labels = torch.stack([x[1] for x in batch])\n",
    "            return {'features': token_ids, 'targets': labels}\n",
    "        else:\n",
    "            return {'features': token_ids}\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class QuestModel(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        super(QuestModel, self).__init__()\n",
    "        self.model_name = 'QuestModel'\n",
    "        \n",
    "#         self.bert_model = BertModel.from_pretrained('bert-base-uncased') \n",
    "#         self.bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "#         self.bert_model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "#         self.bert_model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "#        self.bert_model = XLMRobertaModel.from_pretrained('xlm-roberta-base', \n",
    "                                                          #output_hidden_states=False, \n",
    "                                                          #output_attentions=False)\n",
    "#         self.bert_model = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
    "        self.bert_model = XLMRobertaModel.from_pretrained('xlm-roberta-large')#,\n",
    "#                                                           output_hidden_states=False, \n",
    "#                                                           output_attentions=False)\n",
    "    \n",
    "#         self.fc = nn.Linear(768, n_classes)\n",
    "        self.fc = nn.Linear(1024, n_classes)\n",
    "\n",
    "    def forward(self, ids):\n",
    "#         attention_mask = (ids > 0)\n",
    "#         print(ids.shape)\n",
    "        layers = self.bert_model(input_ids=ids)#, attention_mask=attention_mask)\n",
    "#         print(layers[0].shape)\n",
    "#         out = F.dropout(layers[0][:, 0, :], p=0.2, training=self.training)\n",
    "#         print(layers[0].shape)\n",
    "#         print([l.shape for l in layers])\n",
    "#         out = F.dropout(layers[-1][:, 0, :], p=0.35, training=self.training)\n",
    "        out = F.dropout(layers[0][:, 0, :], p=0.2, training=self.training)\n",
    "        logit = self.fc(out)#.unsqueeze(1)\n",
    "        return logit #, 'for_auc': logit[:, 1]}#[:,1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model = XLMRobertaModel.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, dataset):\n",
    "       \n",
    "        self.toxic_inds = dataset.toxic_inds.copy()\n",
    "        self.normal_inds = dataset.normal_inds.copy()\n",
    "        \n",
    "        self.num_samples = 2*min(len(self.toxic_inds), len(self.normal_inds))\n",
    "        \n",
    "        shfl(self.toxic_inds)\n",
    "        shfl(self.normal_inds)\n",
    "        \n",
    "        self.inds = []\n",
    "        for i in range(min(len(self.toxic_inds), len(self.normal_inds))):\n",
    "            self.inds.append(self.normal_inds[i%len(self.normal_inds)])\n",
    "            self.inds.append(self.toxic_inds[i%len(self.toxic_inds)])\n",
    "\n",
    "    def __iter__(self):\n",
    "        #print ('\\tcalling Sampler:__iter__')\n",
    "        return iter(self.inds)\n",
    "\n",
    "    def __len__(self):\n",
    "        #print ('\\tcalling Sampler:__len__')\n",
    "        return self.num_samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "        callback_get_label func: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None, callback_get_label=None):\n",
    "                \n",
    "        # if indices is not provided, \n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # define custom callback\n",
    "        self.callback_get_label = callback_get_label\n",
    "\n",
    "        # if num_samples is not provided, \n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "            \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "                \n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        if self.callback_get_label:\n",
    "            return self.callback_get_label(dataset, idx)\n",
    "        else:\n",
    "            dataset.labels[idx]\n",
    "#             raise NotImplementedError\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "            self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(to_balance=True, shuffle_before=True):\n",
    "    if SERVER:\n",
    "        workers=1\n",
    "    else:\n",
    "        workers = 6    \n",
    "    \n",
    "#     train_dataset = QuestDataset(df_train.iloc[::STRIDE], train_mode=True)\n",
    "    \n",
    "    if STRIDE == 1:\n",
    "        train_dataset = QuestDataset(df_train.iloc[::STRIDE], train_mode=True, train_transforms=get_train_transforms())\n",
    "    elif STRIDE<=10:\n",
    "        df_train_pos = df_train[df_train[target_column]==1].reset_index(drop=True)\n",
    "        df_train_neg = df_train[df_train[target_column]==0].reset_index(drop=True).iloc[::STRIDE]\n",
    "        train_dataset = QuestDataset(df_train_pos.append(df_train_neg).reset_index(drop=True), train_mode=True, train_transforms=get_train_transforms())\n",
    "    else:\n",
    "        df_train_pos = df_train[df_train[target_column]==1].reset_index(drop=True).iloc[::int(STRIDE/10)]\n",
    "        df_train_neg = df_train[df_train[target_column]==0].reset_index(drop=True).iloc[::STRIDE]\n",
    "        train_dataset = QuestDataset(df_train_pos.append(df_train_neg).reset_index(drop=True), train_mode=True, train_transforms=get_train_transforms())\n",
    "        \n",
    "    \n",
    "    valid_dataset = QuestDataset(df_valid, train_mode=False)\n",
    "   \n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        num_workers=workers,\n",
    "        sampler=BalancedSampler(train_dataset) if to_balance else None,#ImbalancedDatasetSampler(train_dataset) if to_balance else None,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        num_workers=workers,\n",
    "        batch_size=batch_size,\n",
    "    )    \n",
    "    \n",
    "       \n",
    "    loaders = {}\n",
    "    loaders['train'] = train_loader\n",
    "    \n",
    "    loaders['valid'] = valid_loader\n",
    "    \n",
    "    \n",
    "    for i in ['es', 'it', 'tr']:\n",
    "        df = df_valid\n",
    "        df = df[df['lang']==i]\n",
    "\n",
    "        loaders['valid_'+ i] = DataLoader(\n",
    "            QuestDataset(df, train_mode=False),\n",
    "            num_workers=workers,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is 435775 samples, 133610 samples and 302165 samples\n",
      "Class balance is 0.31\n",
      "----------------Experiment: simple\n",
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
      "1/300 * Epoch (train):   0% 0/2784 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kb/jig_env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning:\n",
      "\n",
      "size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/300 * Epoch (train):   0% 1/2784 [00:01<47:00,  1.01s/it, loss=0.063]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning:\n",
      "\n",
      "This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/300 * Epoch (train):   2% 58/2784 [00:22<17:35,  2.58it/s, loss=0.053]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "1/300 * Epoch (train):   3% 84/2784 [00:32<17:30,  2.57it/s, loss=0.042]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "1/300 * Epoch (train): 100% 2784/2784 [18:07<00:00,  2.56it/s, loss=0.016]\n",
      "1/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.73it/s, loss=0.016]\n",
      "1/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.52it/s, loss=0.007]\n",
      "1/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.55it/s, loss=0.007]\n",
      "1/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.47it/s, loss=0.003]\n",
      "[2020-05-04 00:15:22,038] \n",
      "1/300 * Epoch 1 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "1/300 * Epoch 1 (train): auc/_mean=0.9351 | auc/class_0=0.9351 | loss=0.0238\n",
      "1/300 * Epoch 1 (valid): auc/_mean=0.9031 | auc/class_0=0.9031 | es_auc/_mean=0.8805 | es_auc/class_0=0.8805 | es_loss=0.0297 | it_auc/_mean=0.8581 | it_auc/class_0=0.8581 | it_loss=0.0325 | loss=0.0237 | tr_auc/_mean=0.9711 | tr_auc/class_0=0.9711 | tr_loss=0.0097\n",
      "1/300 * Epoch 1 (valid_es): auc/_mean=0.8805 | auc/class_0=0.8805 | loss=0.0297\n",
      "1/300 * Epoch 1 (valid_it): auc/_mean=0.8581 | auc/class_0=0.8581 | loss=0.0325\n",
      "1/300 * Epoch 1 (valid_tr): auc/_mean=0.9711 | auc/class_0=0.9711 | loss=0.0097\n",
      "2/300 * Epoch (train): 100% 2784/2784 [18:06<00:00,  2.56it/s, loss=0.014]\n",
      "2/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.74it/s, loss=0.009]\n",
      "2/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.46it/s, loss=0.004]\n",
      "2/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.46it/s, loss=0.008]\n",
      "2/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.48it/s, loss=0.003]\n",
      "[2020-05-04 00:34:26,819] \n",
      "2/300 * Epoch 2 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "2/300 * Epoch 2 (train): auc/_mean=0.9609 | auc/class_0=0.9609 | loss=0.0185\n",
      "2/300 * Epoch 2 (valid): auc/_mean=0.9031 | auc/class_0=0.9031 | es_auc/_mean=0.8762 | es_auc/class_0=0.8762 | es_loss=0.0274 | it_auc/_mean=0.8590 | it_auc/class_0=0.8590 | it_loss=0.0298 | loss=0.0222 | tr_auc/_mean=0.9725 | tr_auc/class_0=0.9725 | tr_loss=0.0103\n",
      "2/300 * Epoch 2 (valid_es): auc/_mean=0.8762 | auc/class_0=0.8762 | loss=0.0274\n",
      "2/300 * Epoch 2 (valid_it): auc/_mean=0.8590 | auc/class_0=0.8590 | loss=0.0298\n",
      "2/300 * Epoch 2 (valid_tr): auc/_mean=0.9725 | auc/class_0=0.9725 | loss=0.0103\n",
      "3/300 * Epoch (train):  96% 2680/2784 [17:25<00:40,  2.56it/s, loss=0.017]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "3/300 * Epoch (train):  97% 2694/2784 [17:30<00:35,  2.56it/s, loss=0.022]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "3/300 * Epoch (train): 100% 2784/2784 [18:06<00:00,  2.56it/s, loss=0.009]\n",
      "3/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.74it/s, loss=0.011]\n",
      "3/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.52it/s, loss=0.006]\n",
      "3/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.51it/s, loss=0.006]\n",
      "3/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.50it/s, loss=0.002]\n",
      "[2020-05-04 00:53:41,995] \n",
      "3/300 * Epoch 3 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "3/300 * Epoch 3 (train): auc/_mean=0.9665 | auc/class_0=0.9665 | loss=0.0172\n",
      "3/300 * Epoch 3 (valid): auc/_mean=0.9034 | auc/class_0=0.9034 | es_auc/_mean=0.8750 | es_auc/class_0=0.8750 | es_loss=0.0264 | it_auc/_mean=0.8540 | it_auc/class_0=0.8540 | it_loss=0.0291 | loss=0.0218 | tr_auc/_mean=0.9739 | tr_auc/class_0=0.9739 | tr_loss=0.0109\n",
      "3/300 * Epoch 3 (valid_es): auc/_mean=0.8750 | auc/class_0=0.8750 | loss=0.0264\n",
      "3/300 * Epoch 3 (valid_it): auc/_mean=0.8540 | auc/class_0=0.8540 | loss=0.0291\n",
      "3/300 * Epoch 3 (valid_tr): auc/_mean=0.9739 | auc/class_0=0.9739 | loss=0.0109\n",
      "4/300 * Epoch (train):  95% 2639/2784 [17:10<00:56,  2.56it/s, loss=0.020]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "4/300 * Epoch (train): 100% 2784/2784 [18:07<00:00,  2.56it/s, loss=0.010]\n",
      "4/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.76it/s, loss=0.011]\n",
      "4/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.52it/s, loss=0.010]\n",
      "4/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.47it/s, loss=0.009]\n",
      "4/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.49it/s, loss=0.003]\n",
      "[2020-05-04 01:12:57,432] \n",
      "4/300 * Epoch 4 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "4/300 * Epoch 4 (train): auc/_mean=0.9700 | auc/class_0=0.9700 | loss=0.0162\n",
      "4/300 * Epoch 4 (valid): auc/_mean=0.9043 | auc/class_0=0.9043 | es_auc/_mean=0.8727 | es_auc/class_0=0.8727 | es_loss=0.0261 | it_auc/_mean=0.8576 | it_auc/class_0=0.8576 | it_loss=0.0288 | loss=0.0221 | tr_auc/_mean=0.9719 | tr_auc/class_0=0.9719 | tr_loss=0.0128\n",
      "4/300 * Epoch 4 (valid_es): auc/_mean=0.8727 | auc/class_0=0.8727 | loss=0.0261\n",
      "4/300 * Epoch 4 (valid_it): auc/_mean=0.8576 | auc/class_0=0.8576 | loss=0.0288\n",
      "4/300 * Epoch 4 (valid_tr): auc/_mean=0.9719 | auc/class_0=0.9719 | loss=0.0128\n",
      "5/300 * Epoch (train): 100% 2784/2784 [18:06<00:00,  2.56it/s, loss=0.011]\n",
      "5/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.74it/s, loss=0.013]\n",
      "5/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.47it/s, loss=0.005]\n",
      "5/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.51it/s, loss=0.009]\n",
      "5/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.48it/s, loss=0.005]\n",
      "[2020-05-04 01:32:12,690] \n",
      "5/300 * Epoch 5 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "5/300 * Epoch 5 (train): auc/_mean=0.9733 | auc/class_0=0.9733 | loss=0.0154\n",
      "5/300 * Epoch 5 (valid): auc/_mean=0.9006 | auc/class_0=0.9006 | es_auc/_mean=0.8737 | es_auc/class_0=0.8737 | es_loss=0.0262 | it_auc/_mean=0.8537 | it_auc/class_0=0.8537 | it_loss=0.0311 | loss=0.0240 | tr_auc/_mean=0.9701 | tr_auc/class_0=0.9701 | tr_loss=0.0147\n",
      "5/300 * Epoch 5 (valid_es): auc/_mean=0.8737 | auc/class_0=0.8737 | loss=0.0262\n",
      "5/300 * Epoch 5 (valid_it): auc/_mean=0.8537 | auc/class_0=0.8537 | loss=0.0311\n",
      "5/300 * Epoch 5 (valid_tr): auc/_mean=0.9701 | auc/class_0=0.9701 | loss=0.0147\n",
      "6/300 * Epoch (train):   4% 116/2784 [00:45<17:17,  2.57it/s, loss=0.016]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "6/300 * Epoch (train): 100% 2784/2784 [18:06<00:00,  2.56it/s, loss=0.009]\n",
      "6/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.72it/s, loss=0.010]\n",
      "6/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.53it/s, loss=0.004]\n",
      "6/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.49it/s, loss=0.013]\n",
      "6/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.47it/s, loss=0.004]\n",
      "[2020-05-04 01:51:12,369] \n",
      "6/300 * Epoch 6 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "6/300 * Epoch 6 (train): auc/_mean=0.9753 | auc/class_0=0.9753 | loss=0.0148\n",
      "6/300 * Epoch 6 (valid): auc/_mean=0.9039 | auc/class_0=0.9039 | es_auc/_mean=0.8772 | es_auc/class_0=0.8772 | es_loss=0.0255 | it_auc/_mean=0.8586 | it_auc/class_0=0.8586 | it_loss=0.0288 | loss=0.0224 | tr_auc/_mean=0.9690 | tr_auc/class_0=0.9690 | tr_loss=0.0135\n",
      "6/300 * Epoch 6 (valid_es): auc/_mean=0.8772 | auc/class_0=0.8772 | loss=0.0255\n",
      "6/300 * Epoch 6 (valid_it): auc/_mean=0.8586 | auc/class_0=0.8586 | loss=0.0288\n",
      "6/300 * Epoch 6 (valid_tr): auc/_mean=0.9690 | auc/class_0=0.9690 | loss=0.0135\n",
      "7/300 * Epoch (train):  63% 1745/2784 [11:22<06:46,  2.56it/s, loss=0.016]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "7/300 * Epoch (train):  71% 1985/2784 [12:55<05:12,  2.56it/s, loss=0.011]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "7/300 * Epoch (train): 100% 2784/2784 [18:08<00:00,  2.56it/s, loss=0.008]\n",
      "7/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.73it/s, loss=0.018]\n",
      "7/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.51it/s, loss=0.003]\n",
      "7/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.51it/s, loss=0.010]\n",
      "7/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.51it/s, loss=0.010]\n",
      "[2020-05-04 02:10:13,707] \n",
      "7/300 * Epoch 7 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "7/300 * Epoch 7 (train): auc/_mean=0.9775 | auc/class_0=0.9775 | loss=0.0141\n",
      "7/300 * Epoch 7 (valid): auc/_mean=0.8992 | auc/class_0=0.8992 | es_auc/_mean=0.8698 | es_auc/class_0=0.8698 | es_loss=0.0282 | it_auc/_mean=0.8559 | it_auc/class_0=0.8559 | it_loss=0.0325 | loss=0.0260 | tr_auc/_mean=0.9685 | tr_auc/class_0=0.9685 | tr_loss=0.0172\n",
      "7/300 * Epoch 7 (valid_es): auc/_mean=0.8698 | auc/class_0=0.8698 | loss=0.0282\n",
      "7/300 * Epoch 7 (valid_it): auc/_mean=0.8559 | auc/class_0=0.8559 | loss=0.0325\n",
      "7/300 * Epoch 7 (valid_tr): auc/_mean=0.9685 | auc/class_0=0.9685 | loss=0.0172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/300 * Epoch (train): 100% 2784/2784 [18:08<00:00,  2.56it/s, loss=0.012]\n",
      "8/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.74it/s, loss=0.021]\n",
      "8/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.47it/s, loss=0.004]\n",
      "8/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.46it/s, loss=0.008]\n",
      "8/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.54it/s, loss=0.011]\n",
      "[2020-05-04 02:29:14,960] \n",
      "8/300 * Epoch 8 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "8/300 * Epoch 8 (train): auc/_mean=0.9797 | auc/class_0=0.9797 | loss=0.0134\n",
      "8/300 * Epoch 8 (valid): auc/_mean=0.8920 | auc/class_0=0.8920 | es_auc/_mean=0.8624 | es_auc/class_0=0.8624 | es_loss=0.0290 | it_auc/_mean=0.8411 | it_auc/class_0=0.8411 | it_loss=0.0346 | loss=0.0266 | tr_auc/_mean=0.9621 | tr_auc/class_0=0.9621 | tr_loss=0.0165\n",
      "8/300 * Epoch 8 (valid_es): auc/_mean=0.8624 | auc/class_0=0.8624 | loss=0.0290\n",
      "8/300 * Epoch 8 (valid_it): auc/_mean=0.8411 | auc/class_0=0.8411 | loss=0.0346\n",
      "8/300 * Epoch 8 (valid_tr): auc/_mean=0.9621 | auc/class_0=0.9621 | loss=0.0165\n",
      "9/300 * Epoch (train):  37% 1036/2784 [06:45<11:23,  2.56it/s, loss=0.012]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "9/300 * Epoch (train): 100% 2784/2784 [18:08<00:00,  2.56it/s, loss=0.002]\n",
      "9/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.76it/s, loss=0.019]\n",
      "9/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.50it/s, loss=0.003]\n",
      "9/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.47it/s, loss=0.014]\n",
      "9/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.52it/s, loss=0.012]\n",
      "[2020-05-04 02:48:15,830] \n",
      "9/300 * Epoch 9 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "9/300 * Epoch 9 (train): auc/_mean=0.9814 | auc/class_0=0.9814 | loss=0.0129\n",
      "9/300 * Epoch 9 (valid): auc/_mean=0.8909 | auc/class_0=0.8909 | es_auc/_mean=0.8622 | es_auc/class_0=0.8622 | es_loss=0.0324 | it_auc/_mean=0.8407 | it_auc/class_0=0.8407 | it_loss=0.0398 | loss=0.0305 | tr_auc/_mean=0.9635 | tr_auc/class_0=0.9635 | tr_loss=0.0194\n",
      "9/300 * Epoch 9 (valid_es): auc/_mean=0.8622 | auc/class_0=0.8622 | loss=0.0324\n",
      "9/300 * Epoch 9 (valid_it): auc/_mean=0.8407 | auc/class_0=0.8407 | loss=0.0398\n",
      "9/300 * Epoch 9 (valid_tr): auc/_mean=0.9635 | auc/class_0=0.9635 | loss=0.0194\n",
      "10/300 * Epoch (train):  10% 286/2784 [01:51<16:14,  2.56it/s, loss=0.011]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "10/300 * Epoch (train):  51% 1430/2784 [09:18<08:49,  2.56it/s, loss=0.014]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "10/300 * Epoch (train): 100% 2784/2784 [18:07<00:00,  2.56it/s, loss=0.006]\n",
      "10/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.74it/s, loss=0.019]\n",
      "10/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.47it/s, loss=0.009]\n",
      "10/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.52it/s, loss=0.018]\n",
      "10/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.51it/s, loss=0.011]\n",
      "[2020-05-04 03:07:15,918] \n",
      "10/300 * Epoch 10 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "10/300 * Epoch 10 (train): auc/_mean=0.9831 | auc/class_0=0.9831 | loss=0.0123\n",
      "10/300 * Epoch 10 (valid): auc/_mean=0.8903 | auc/class_0=0.8903 | es_auc/_mean=0.8646 | es_auc/class_0=0.8646 | es_loss=0.0300 | it_auc/_mean=0.8388 | it_auc/class_0=0.8388 | it_loss=0.0358 | loss=0.0283 | tr_auc/_mean=0.9603 | tr_auc/class_0=0.9603 | tr_loss=0.0197\n",
      "10/300 * Epoch 10 (valid_es): auc/_mean=0.8646 | auc/class_0=0.8646 | loss=0.0300\n",
      "10/300 * Epoch 10 (valid_it): auc/_mean=0.8388 | auc/class_0=0.8388 | loss=0.0358\n",
      "10/300 * Epoch 10 (valid_tr): auc/_mean=0.9603 | auc/class_0=0.9603 | loss=0.0197\n",
      "11/300 * Epoch (train): 100% 2782/2784 [18:05<00:00,  2.56it/s, loss=0.011]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "11/300 * Epoch (train): 100% 2784/2784 [18:06<00:00,  2.56it/s, loss=0.007]\n",
      "11/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.75it/s, loss=0.016]\n",
      "11/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.52it/s, loss=0.004]\n",
      "11/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.53it/s, loss=0.016]\n",
      "11/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.51it/s, loss=0.014]\n",
      "[2020-05-04 03:26:15,039] \n",
      "11/300 * Epoch 11 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "11/300 * Epoch 11 (train): auc/_mean=0.9848 | auc/class_0=0.9848 | loss=0.0116\n",
      "11/300 * Epoch 11 (valid): auc/_mean=0.8861 | auc/class_0=0.8861 | es_auc/_mean=0.8583 | es_auc/class_0=0.8583 | es_loss=0.0327 | it_auc/_mean=0.8330 | it_auc/class_0=0.8330 | it_loss=0.0399 | loss=0.0306 | tr_auc/_mean=0.9559 | tr_auc/class_0=0.9559 | tr_loss=0.0203\n",
      "11/300 * Epoch 11 (valid_es): auc/_mean=0.8583 | auc/class_0=0.8583 | loss=0.0327\n",
      "11/300 * Epoch 11 (valid_it): auc/_mean=0.8330 | auc/class_0=0.8330 | loss=0.0399\n",
      "11/300 * Epoch 11 (valid_tr): auc/_mean=0.9559 | auc/class_0=0.9559 | loss=0.0203\n",
      "12/300 * Epoch (train):  86% 2398/2784 [15:37<02:31,  2.56it/s, loss=0.017]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "12/300 * Epoch (train):  97% 2702/2784 [17:36<00:32,  2.55it/s, loss=0.008]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "12/300 * Epoch (train): 100% 2784/2784 [18:08<00:00,  2.56it/s, loss=0.007]\n",
      "12/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.72it/s, loss=0.016]\n",
      "12/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.48it/s, loss=0.003]\n",
      "12/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.49it/s, loss=0.006]\n",
      "12/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.50it/s, loss=0.013]\n",
      "[2020-05-04 03:45:16,384] \n",
      "12/300 * Epoch 12 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "12/300 * Epoch 12 (train): auc/_mean=0.9857 | auc/class_0=0.9857 | loss=0.0113\n",
      "12/300 * Epoch 12 (valid): auc/_mean=0.8829 | auc/class_0=0.8829 | es_auc/_mean=0.8582 | es_auc/class_0=0.8582 | es_loss=0.0356 | it_auc/_mean=0.8336 | it_auc/class_0=0.8336 | it_loss=0.0421 | loss=0.0335 | tr_auc/_mean=0.9537 | tr_auc/class_0=0.9537 | tr_loss=0.0221\n",
      "12/300 * Epoch 12 (valid_es): auc/_mean=0.8582 | auc/class_0=0.8582 | loss=0.0356\n",
      "12/300 * Epoch 12 (valid_it): auc/_mean=0.8336 | auc/class_0=0.8336 | loss=0.0421\n",
      "12/300 * Epoch 12 (valid_tr): auc/_mean=0.9537 | auc/class_0=0.9537 | loss=0.0221\n",
      "13/300 * Epoch (train):  91% 2536/2784 [16:30<01:37,  2.56it/s, loss=0.011]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "13/300 * Epoch (train): 100% 2784/2784 [18:08<00:00,  2.56it/s, loss=0.008]\n",
      "13/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.73it/s, loss=0.018]\n",
      "13/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.46it/s, loss=0.003]\n",
      "13/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.46it/s, loss=0.013]\n",
      "13/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.50it/s, loss=0.013]\n",
      "[2020-05-04 04:04:17,704] \n",
      "13/300 * Epoch 13 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "13/300 * Epoch 13 (train): auc/_mean=0.9872 | auc/class_0=0.9872 | loss=0.0107\n",
      "13/300 * Epoch 13 (valid): auc/_mean=0.8806 | auc/class_0=0.8806 | es_auc/_mean=0.8602 | es_auc/class_0=0.8602 | es_loss=0.0353 | it_auc/_mean=0.8297 | it_auc/class_0=0.8297 | it_loss=0.0444 | loss=0.0339 | tr_auc/_mean=0.9499 | tr_auc/class_0=0.9499 | tr_loss=0.0213\n",
      "13/300 * Epoch 13 (valid_es): auc/_mean=0.8602 | auc/class_0=0.8602 | loss=0.0353\n",
      "13/300 * Epoch 13 (valid_it): auc/_mean=0.8297 | auc/class_0=0.8297 | loss=0.0444\n",
      "13/300 * Epoch 13 (valid_tr): auc/_mean=0.9499 | auc/class_0=0.9499 | loss=0.0213\n",
      "14/300 * Epoch (train): 100% 2784/2784 [18:07<00:00,  2.56it/s, loss=0.006]\n",
      "14/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.73it/s, loss=0.016]\n",
      "14/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.50it/s, loss=0.002]\n",
      "14/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.49it/s, loss=0.012]\n",
      "14/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.49it/s, loss=0.015]\n",
      "[2020-05-04 04:23:18,225] \n",
      "14/300 * Epoch 14 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "14/300 * Epoch 14 (train): auc/_mean=0.9882 | auc/class_0=0.9882 | loss=0.0103\n",
      "14/300 * Epoch 14 (valid): auc/_mean=0.8752 | auc/class_0=0.8752 | es_auc/_mean=0.8518 | es_auc/class_0=0.8518 | es_loss=0.0396 | it_auc/_mean=0.8200 | it_auc/class_0=0.8200 | it_loss=0.0483 | loss=0.0358 | tr_auc/_mean=0.9482 | tr_auc/class_0=0.9482 | tr_loss=0.0200\n",
      "14/300 * Epoch 14 (valid_es): auc/_mean=0.8518 | auc/class_0=0.8518 | loss=0.0396\n",
      "14/300 * Epoch 14 (valid_it): auc/_mean=0.8200 | auc/class_0=0.8200 | loss=0.0483\n",
      "14/300 * Epoch 14 (valid_tr): auc/_mean=0.9482 | auc/class_0=0.9482 | loss=0.0200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/300 * Epoch (train):  35% 973/2784 [06:19<11:46,  2.56it/s, loss=0.007]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "15/300 * Epoch (train):  91% 2536/2784 [16:30<01:36,  2.56it/s, loss=0.012]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "15/300 * Epoch (train): 100% 2784/2784 [18:07<00:00,  2.56it/s, loss=0.003]\n",
      "15/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.75it/s, loss=0.018]\n",
      "15/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.48it/s, loss=0.001]\n",
      "15/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.50it/s, loss=0.011]\n",
      "15/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.50it/s, loss=0.011]\n",
      "[2020-05-04 04:42:18,033] \n",
      "15/300 * Epoch 15 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "15/300 * Epoch 15 (train): auc/_mean=0.9893 | auc/class_0=0.9893 | loss=0.0098\n",
      "15/300 * Epoch 15 (valid): auc/_mean=0.8740 | auc/class_0=0.8740 | es_auc/_mean=0.8464 | es_auc/class_0=0.8464 | es_loss=0.0396 | it_auc/_mean=0.8233 | it_auc/class_0=0.8233 | it_loss=0.0466 | loss=0.0352 | tr_auc/_mean=0.9437 | tr_auc/class_0=0.9437 | tr_loss=0.0206\n",
      "15/300 * Epoch 15 (valid_es): auc/_mean=0.8464 | auc/class_0=0.8464 | loss=0.0396\n",
      "15/300 * Epoch 15 (valid_it): auc/_mean=0.8233 | auc/class_0=0.8233 | loss=0.0466\n",
      "15/300 * Epoch 15 (valid_tr): auc/_mean=0.9437 | auc/class_0=0.9437 | loss=0.0206\n",
      "16/300 * Epoch (train):  61% 1690/2784 [10:59<07:07,  2.56it/s, loss=0.011]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "16/300 * Epoch (train): 100% 2784/2784 [18:06<00:00,  2.56it/s, loss=0.016]\n",
      "16/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.73it/s, loss=0.016]\n",
      "16/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.47it/s, loss=5.016e-04]\n",
      "16/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.52it/s, loss=0.004]\n",
      "16/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.50it/s, loss=0.014]\n",
      "[2020-05-04 05:01:17,067] \n",
      "16/300 * Epoch 16 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "16/300 * Epoch 16 (train): auc/_mean=0.9902 | auc/class_0=0.9902 | loss=0.0094\n",
      "16/300 * Epoch 16 (valid): auc/_mean=0.8775 | auc/class_0=0.8775 | es_auc/_mean=0.8514 | es_auc/class_0=0.8514 | es_loss=0.0440 | it_auc/_mean=0.8253 | it_auc/class_0=0.8253 | it_loss=0.0535 | loss=0.0398 | tr_auc/_mean=0.9487 | tr_auc/class_0=0.9487 | tr_loss=0.0229\n",
      "16/300 * Epoch 16 (valid_es): auc/_mean=0.8514 | auc/class_0=0.8514 | loss=0.0440\n",
      "16/300 * Epoch 16 (valid_it): auc/_mean=0.8253 | auc/class_0=0.8253 | loss=0.0535\n",
      "16/300 * Epoch 16 (valid_tr): auc/_mean=0.9487 | auc/class_0=0.9487 | loss=0.0229\n",
      "17/300 * Epoch (train): 100% 2784/2784 [18:06<00:00,  2.56it/s, loss=0.006]\n",
      "17/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.74it/s, loss=0.024]\n",
      "17/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.49it/s, loss=3.670e-04]\n",
      "17/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.46it/s, loss=0.005]\n",
      "17/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.51it/s, loss=0.016]\n",
      "[2020-05-04 05:20:16,012] \n",
      "17/300 * Epoch 17 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "17/300 * Epoch 17 (train): auc/_mean=0.9906 | auc/class_0=0.9906 | loss=0.0091\n",
      "17/300 * Epoch 17 (valid): auc/_mean=0.8717 | auc/class_0=0.8717 | es_auc/_mean=0.8494 | es_auc/class_0=0.8494 | es_loss=0.0422 | it_auc/_mean=0.8143 | it_auc/class_0=0.8143 | it_loss=0.0524 | loss=0.0393 | tr_auc/_mean=0.9402 | tr_auc/class_0=0.9402 | tr_loss=0.0245\n",
      "17/300 * Epoch 17 (valid_es): auc/_mean=0.8494 | auc/class_0=0.8494 | loss=0.0422\n",
      "17/300 * Epoch 17 (valid_it): auc/_mean=0.8143 | auc/class_0=0.8143 | loss=0.0524\n",
      "17/300 * Epoch 17 (valid_tr): auc/_mean=0.9402 | auc/class_0=0.9402 | loss=0.0245\n",
      "18/300 * Epoch (train):  69% 1923/2784 [12:30<05:35,  2.57it/s, loss=0.007]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "18/300 * Epoch (train): 100% 2784/2784 [18:05<00:00,  2.56it/s, loss=0.005]\n",
      "18/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.72it/s, loss=0.024]\n",
      "18/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.49it/s, loss=6.666e-04]\n",
      "18/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.44it/s, loss=0.013]\n",
      "18/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.52it/s, loss=0.015]\n",
      "[2020-05-04 05:39:15,024] \n",
      "18/300 * Epoch 18 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "18/300 * Epoch 18 (train): auc/_mean=0.9915 | auc/class_0=0.9915 | loss=0.0087\n",
      "18/300 * Epoch 18 (valid): auc/_mean=0.8730 | auc/class_0=0.8730 | es_auc/_mean=0.8484 | es_auc/class_0=0.8484 | es_loss=0.0411 | it_auc/_mean=0.8159 | it_auc/class_0=0.8159 | it_loss=0.0504 | loss=0.0381 | tr_auc/_mean=0.9451 | tr_auc/class_0=0.9451 | tr_loss=0.0229\n",
      "18/300 * Epoch 18 (valid_es): auc/_mean=0.8484 | auc/class_0=0.8484 | loss=0.0411\n",
      "18/300 * Epoch 18 (valid_it): auc/_mean=0.8159 | auc/class_0=0.8159 | loss=0.0504\n",
      "18/300 * Epoch 18 (valid_tr): auc/_mean=0.9451 | auc/class_0=0.9451 | loss=0.0229\n",
      "19/300 * Epoch (train):  68% 1902/2784 [12:21<05:44,  2.56it/s, loss=0.009]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "19/300 * Epoch (train): 100% 2784/2784 [18:06<00:00,  2.56it/s, loss=0.003]\n",
      "19/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.78it/s, loss=0.014]\n",
      "19/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.58it/s, loss=7.743e-04]\n",
      "19/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.52it/s, loss=0.008]\n",
      "19/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.51it/s, loss=0.012]\n",
      "[2020-05-04 05:58:13,476] \n",
      "19/300 * Epoch 19 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "19/300 * Epoch 19 (train): auc/_mean=0.9919 | auc/class_0=0.9919 | loss=0.0084\n",
      "19/300 * Epoch 19 (valid): auc/_mean=0.8650 | auc/class_0=0.8650 | es_auc/_mean=0.8390 | es_auc/class_0=0.8390 | es_loss=0.0482 | it_auc/_mean=0.8059 | it_auc/class_0=0.8059 | it_loss=0.0577 | loss=0.0433 | tr_auc/_mean=0.9440 | tr_auc/class_0=0.9440 | tr_loss=0.0242\n",
      "19/300 * Epoch 19 (valid_es): auc/_mean=0.8390 | auc/class_0=0.8390 | loss=0.0482\n",
      "19/300 * Epoch 19 (valid_it): auc/_mean=0.8059 | auc/class_0=0.8059 | loss=0.0577\n",
      "19/300 * Epoch 19 (valid_tr): auc/_mean=0.9440 | auc/class_0=0.9440 | loss=0.0242\n",
      "20/300 * Epoch (train):  57% 1587/2784 [10:19<07:47,  2.56it/s, loss=0.004]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "20/300 * Epoch (train): 100% 2784/2784 [18:06<00:00,  2.56it/s, loss=0.010]\n",
      "20/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.73it/s, loss=0.020]\n",
      "20/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.47it/s, loss=8.935e-04]\n",
      "20/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.44it/s, loss=0.004]\n",
      "20/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.49it/s, loss=0.015]\n",
      "[2020-05-04 06:17:12,455] \n",
      "20/300 * Epoch 20 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "20/300 * Epoch 20 (train): auc/_mean=0.9924 | auc/class_0=0.9924 | loss=0.0082\n",
      "20/300 * Epoch 20 (valid): auc/_mean=0.8627 | auc/class_0=0.8627 | es_auc/_mean=0.8401 | es_auc/class_0=0.8401 | es_loss=0.0503 | it_auc/_mean=0.8107 | it_auc/class_0=0.8107 | it_loss=0.0585 | loss=0.0441 | tr_auc/_mean=0.9338 | tr_auc/class_0=0.9338 | tr_loss=0.0237\n",
      "20/300 * Epoch 20 (valid_es): auc/_mean=0.8401 | auc/class_0=0.8401 | loss=0.0503\n",
      "20/300 * Epoch 20 (valid_it): auc/_mean=0.8107 | auc/class_0=0.8107 | loss=0.0585\n",
      "20/300 * Epoch 20 (valid_tr): auc/_mean=0.9338 | auc/class_0=0.9338 | loss=0.0237\n",
      "21/300 * Epoch (train):  14% 397/2784 [02:35<15:31,  2.56it/s, loss=0.006]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "21/300 * Epoch (train): 100% 2784/2784 [18:06<00:00,  2.56it/s, loss=0.005]\n",
      "21/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.73it/s, loss=0.018]\n",
      "21/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.52it/s, loss=8.869e-04]\n",
      "21/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.47it/s, loss=0.012]\n",
      "21/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.51it/s, loss=0.014]\n",
      "[2020-05-04 06:36:11,285] \n",
      "21/300 * Epoch 21 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "21/300 * Epoch 21 (train): auc/_mean=0.9929 | auc/class_0=0.9929 | loss=0.0079\n",
      "21/300 * Epoch 21 (valid): auc/_mean=0.8646 | auc/class_0=0.8646 | es_auc/_mean=0.8363 | es_auc/class_0=0.8363 | es_loss=0.0513 | it_auc/_mean=0.8031 | it_auc/class_0=0.8031 | it_loss=0.0594 | loss=0.0436 | tr_auc/_mean=0.9341 | tr_auc/class_0=0.9341 | tr_loss=0.0229\n",
      "21/300 * Epoch 21 (valid_es): auc/_mean=0.8363 | auc/class_0=0.8363 | loss=0.0513\n",
      "21/300 * Epoch 21 (valid_it): auc/_mean=0.8031 | auc/class_0=0.8031 | loss=0.0594\n",
      "21/300 * Epoch 21 (valid_tr): auc/_mean=0.9341 | auc/class_0=0.9341 | loss=0.0229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/300 * Epoch (train):  61% 1707/2784 [11:06<07:00,  2.56it/s, loss=0.005]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "22/300 * Epoch (train):  98% 2730/2784 [17:45<00:21,  2.56it/s, loss=0.010]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "22/300 * Epoch (train): 100% 2784/2784 [18:06<00:00,  2.56it/s, loss=0.002]\n",
      "22/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.73it/s, loss=0.019]\n",
      "22/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.51it/s, loss=0.001]\n",
      "22/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.48it/s, loss=0.009]\n",
      "22/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.47it/s, loss=0.012]\n",
      "[2020-05-04 06:55:10,631] \n",
      "22/300 * Epoch 22 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "22/300 * Epoch 22 (train): auc/_mean=0.9931 | auc/class_0=0.9931 | loss=0.0078\n",
      "22/300 * Epoch 22 (valid): auc/_mean=0.8680 | auc/class_0=0.8680 | es_auc/_mean=0.8382 | es_auc/class_0=0.8382 | es_loss=0.0478 | it_auc/_mean=0.8120 | it_auc/class_0=0.8120 | it_loss=0.0560 | loss=0.0425 | tr_auc/_mean=0.9403 | tr_auc/class_0=0.9403 | tr_loss=0.0252\n",
      "22/300 * Epoch 22 (valid_es): auc/_mean=0.8382 | auc/class_0=0.8382 | loss=0.0478\n",
      "22/300 * Epoch 22 (valid_it): auc/_mean=0.8120 | auc/class_0=0.8120 | loss=0.0560\n",
      "22/300 * Epoch 22 (valid_tr): auc/_mean=0.9403 | auc/class_0=0.9403 | loss=0.0252\n",
      "23/300 * Epoch (train):  94% 2618/2784 [17:01<01:04,  2.56it/s, loss=0.010]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "23/300 * Epoch (train): 100% 2784/2784 [18:06<00:00,  2.56it/s, loss=0.004]\n",
      "23/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.76it/s, loss=0.010]\n",
      "23/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.56it/s, loss=1.324e-04]\n",
      "23/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.56it/s, loss=0.014]\n",
      "23/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.55it/s, loss=0.011]\n",
      "[2020-05-04 07:14:09,544] \n",
      "23/300 * Epoch 23 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "23/300 * Epoch 23 (train): auc/_mean=0.9937 | auc/class_0=0.9937 | loss=0.0074\n",
      "23/300 * Epoch 23 (valid): auc/_mean=0.8652 | auc/class_0=0.8652 | es_auc/_mean=0.8446 | es_auc/class_0=0.8446 | es_loss=0.0549 | it_auc/_mean=0.8092 | it_auc/class_0=0.8092 | it_loss=0.0632 | loss=0.0468 | tr_auc/_mean=0.9357 | tr_auc/class_0=0.9357 | tr_loss=0.0238\n",
      "23/300 * Epoch 23 (valid_es): auc/_mean=0.8446 | auc/class_0=0.8446 | loss=0.0549\n",
      "23/300 * Epoch 23 (valid_it): auc/_mean=0.8092 | auc/class_0=0.8092 | loss=0.0632\n",
      "23/300 * Epoch 23 (valid_tr): auc/_mean=0.9357 | auc/class_0=0.9357 | loss=0.0238\n",
      "24/300 * Epoch (train): 100% 2784/2784 [18:06<00:00,  2.56it/s, loss=0.006]   \n",
      "24/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.76it/s, loss=0.013]\n",
      "24/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.56it/s, loss=6.606e-04]\n",
      "24/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.53it/s, loss=0.003]\n",
      "24/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.57it/s, loss=0.011]\n",
      "[2020-05-04 07:33:08,102] \n",
      "24/300 * Epoch 24 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "24/300 * Epoch 24 (train): auc/_mean=0.9938 | auc/class_0=0.9938 | loss=0.0073\n",
      "24/300 * Epoch 24 (valid): auc/_mean=0.8641 | auc/class_0=0.8641 | es_auc/_mean=0.8442 | es_auc/class_0=0.8442 | es_loss=0.0500 | it_auc/_mean=0.8103 | it_auc/class_0=0.8103 | it_loss=0.0567 | loss=0.0429 | tr_auc/_mean=0.9336 | tr_auc/class_0=0.9336 | tr_loss=0.0220\n",
      "24/300 * Epoch 24 (valid_es): auc/_mean=0.8442 | auc/class_0=0.8442 | loss=0.0500\n",
      "24/300 * Epoch 24 (valid_it): auc/_mean=0.8103 | auc/class_0=0.8103 | loss=0.0567\n",
      "24/300 * Epoch 24 (valid_tr): auc/_mean=0.9336 | auc/class_0=0.9336 | loss=0.0220\n",
      "25/300 * Epoch (train):  63% 1746/2784 [11:21<06:45,  2.56it/s, loss=0.010]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "25/300 * Epoch (train): 100% 2784/2784 [18:06<00:00,  2.56it/s, loss=0.008]\n",
      "25/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.76it/s, loss=0.015]\n",
      "25/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.58it/s, loss=0.001]\n",
      "25/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.51it/s, loss=0.008]\n",
      "25/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.55it/s, loss=0.013]\n",
      "[2020-05-04 07:52:07,428] \n",
      "25/300 * Epoch 25 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "25/300 * Epoch 25 (train): auc/_mean=0.9943 | auc/class_0=0.9943 | loss=0.0071\n",
      "25/300 * Epoch 25 (valid): auc/_mean=0.8626 | auc/class_0=0.8626 | es_auc/_mean=0.8455 | es_auc/class_0=0.8455 | es_loss=0.0511 | it_auc/_mean=0.8033 | it_auc/class_0=0.8033 | it_loss=0.0622 | loss=0.0473 | tr_auc/_mean=0.9344 | tr_auc/class_0=0.9344 | tr_loss=0.0285\n",
      "25/300 * Epoch 25 (valid_es): auc/_mean=0.8455 | auc/class_0=0.8455 | loss=0.0511\n",
      "25/300 * Epoch 25 (valid_it): auc/_mean=0.8033 | auc/class_0=0.8033 | loss=0.0622\n",
      "25/300 * Epoch 25 (valid_tr): auc/_mean=0.9344 | auc/class_0=0.9344 | loss=0.0285\n",
      "26/300 * Epoch (train):  87% 2418/2784 [15:44<02:22,  2.56it/s, loss=0.007]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "26/300 * Epoch (train): 100% 2784/2784 [18:07<00:00,  2.56it/s, loss=0.003]\n",
      "26/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.78it/s, loss=0.031]\n",
      "26/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.56it/s, loss=7.523e-04]\n",
      "26/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.53it/s, loss=0.026]\n",
      "26/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.58it/s, loss=0.022]\n",
      "[2020-05-04 08:11:06,875] \n",
      "26/300 * Epoch 26 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "26/300 * Epoch 26 (train): auc/_mean=0.9944 | auc/class_0=0.9944 | loss=0.0070\n",
      "26/300 * Epoch 26 (valid): auc/_mean=0.8575 | auc/class_0=0.8575 | es_auc/_mean=0.8307 | es_auc/class_0=0.8307 | es_loss=0.0598 | it_auc/_mean=0.8048 | it_auc/class_0=0.8048 | it_loss=0.0692 | loss=0.0547 | tr_auc/_mean=0.9227 | tr_auc/class_0=0.9227 | tr_loss=0.0367\n",
      "26/300 * Epoch 26 (valid_es): auc/_mean=0.8307 | auc/class_0=0.8307 | loss=0.0598\n",
      "26/300 * Epoch 26 (valid_it): auc/_mean=0.8048 | auc/class_0=0.8048 | loss=0.0692\n",
      "26/300 * Epoch 26 (valid_tr): auc/_mean=0.9227 | auc/class_0=0.9227 | loss=0.0367\n",
      "27/300 * Epoch (train):   7% 183/2784 [01:11<16:54,  2.56it/s, loss=0.009]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "27/300 * Epoch (train):  55% 1518/2784 [09:51<08:12,  2.57it/s, loss=0.006]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "27/300 * Epoch (train): 100% 2784/2784 [18:05<00:00,  2.57it/s, loss=0.002]\n",
      "27/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.81it/s, loss=0.029]\n",
      "27/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.59it/s, loss=0.002]\n",
      "27/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.62it/s, loss=0.017]\n",
      "27/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.58it/s, loss=0.017]\n",
      "[2020-05-04 08:30:04,093] \n",
      "27/300 * Epoch 27 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "27/300 * Epoch 27 (train): auc/_mean=0.9948 | auc/class_0=0.9948 | loss=0.0067\n",
      "27/300 * Epoch 27 (valid): auc/_mean=0.8592 | auc/class_0=0.8592 | es_auc/_mean=0.8399 | es_auc/class_0=0.8399 | es_loss=0.0572 | it_auc/_mean=0.8015 | it_auc/class_0=0.8015 | it_loss=0.0711 | loss=0.0524 | tr_auc/_mean=0.9295 | tr_auc/class_0=0.9295 | tr_loss=0.0299\n",
      "27/300 * Epoch 27 (valid_es): auc/_mean=0.8399 | auc/class_0=0.8399 | loss=0.0572\n",
      "27/300 * Epoch 27 (valid_it): auc/_mean=0.8015 | auc/class_0=0.8015 | loss=0.0711\n",
      "27/300 * Epoch 27 (valid_tr): auc/_mean=0.9295 | auc/class_0=0.9295 | loss=0.0299\n",
      "28/300 * Epoch (train): 100% 2784/2784 [18:05<00:00,  2.57it/s, loss=0.010]\n",
      "28/300 * Epoch (valid): 100% 84/84 [00:14<00:00,  5.77it/s, loss=0.032]\n",
      "28/300 * Epoch (valid_es): 100% 27/27 [00:04<00:00,  5.58it/s, loss=0.005]\n",
      "28/300 * Epoch (valid_it): 100% 27/27 [00:04<00:00,  5.53it/s, loss=0.041]\n",
      "28/300 * Epoch (valid_tr): 100% 32/32 [00:05<00:00,  5.57it/s, loss=0.021]\n",
      "[2020-05-04 08:49:01,760] \n",
      "28/300 * Epoch 28 (_base): lr=5.000e-06 | momentum=0.9000\n",
      "28/300 * Epoch 28 (train): auc/_mean=0.9951 | auc/class_0=0.9951 | loss=0.0065\n",
      "28/300 * Epoch 28 (valid): auc/_mean=0.8421 | auc/class_0=0.8421 | es_auc/_mean=0.8263 | es_auc/class_0=0.8263 | es_loss=0.0574 | it_auc/_mean=0.7897 | it_auc/class_0=0.7897 | it_loss=0.0705 | loss=0.0551 | tr_auc/_mean=0.9022 | tr_auc/class_0=0.9022 | tr_loss=0.0381\n",
      "28/300 * Epoch 28 (valid_es): auc/_mean=0.8263 | auc/class_0=0.8263 | loss=0.0574\n",
      "28/300 * Epoch 28 (valid_it): auc/_mean=0.7897 | auc/class_0=0.7897 | loss=0.0705\n",
      "28/300 * Epoch 28 (valid_tr): auc/_mean=0.9022 | auc/class_0=0.9022 | loss=0.0381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early exiting                                                             \n",
      "29/300 * Epoch (train):  14% 390/2784 [02:32<15:36,  2.56it/s, loss=0.002]"
     ]
    }
   ],
   "source": [
    "def freeze_until(net, param_name):\n",
    "    found_name = False\n",
    "    for name, params in net.named_parameters():\n",
    "        if name == param_name:\n",
    "            found_name = True\n",
    "        params.requires_grad = found_name\n",
    "param_name = 'bert_model.encoder.layer.10.attention.self.query.weight'\n",
    "\n",
    "CHECKPOINT = './checkpoints/v93_xlm_roberta_large_ml96/best.pth'\n",
    "project = \"jigsaw_v94_xlm_roberta_large_fp02_layer10fr\"\n",
    "num_epochs = 300\n",
    "\n",
    "group = datetime.now().strftime(\"%m_%d_%Y__%H_%M_%S\")\n",
    "\n",
    "    \n",
    "if SERVER:\n",
    "    group = f'{group}_server'\n",
    "    \n",
    "if STRIDE > 1:\n",
    "    group = f'{group}_str{STRIDE}'\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "lr = 5e-6#1e-5#0.0001\n",
    "group += f'_lr{lr}_dr0.2_base_toofp16_96_myaug'\n",
    "\n",
    "group = group.replace('.', '')\n",
    "\n",
    "runner = SupervisedRunner(input_key=('features'), input_target_key=('targets'), output_key=('logits'))\n",
    "\n",
    "experiment = 'simple'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "logdir = f\"{LOG_PATH}/{project}/{group}/{experiment}\"\n",
    "\n",
    "model = QuestModel(2)\n",
    "freeze_until(model, param_name)\n",
    "#     checkpoint = torch.load(CHECKPOINT)#, map_location=device)   \n",
    "\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     del checkpoint\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "loaders = get_loaders(to_balance=True)\n",
    "\n",
    "\n",
    "t_total = len(loaders['train'])//gradient_accumulation_steps*num_epochs\n",
    "warmup_proportion = 0.01\n",
    "num_warmup_steps = t_total * warmup_proportion\n",
    "\n",
    "#     criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion = FocalLoss(alpha=0.2, gamma=1.5, logits=True, reduce=True)\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "#     criterion = torch.nn.BCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr = lr)\n",
    "#    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=t_total) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.25)\n",
    "print(f'----------------Experiment: {experiment}')\n",
    "\n",
    "runner.train(\n",
    "    fp16=dict(opt_level=\"O2\") ,\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    verbose=True,\n",
    "    distributed=False if is_jupyter() else True,\n",
    "    callbacks=[\n",
    "        AlchemyLogger(\n",
    "                token=token, # your Alchemy token\n",
    "                project=project,\n",
    "                experiment=experiment,\n",
    "                group=group,\n",
    "            ),\n",
    "        MyAUCCallback()\n",
    "\n",
    "    ],\n",
    "    main_metric='auc/_mean',\n",
    "    minimize_metric=False,\n",
    "    \n",
    "    #fp16={\"opt_level\": \"O1\"}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, params in model.named_parameters():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.146779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.216815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.656836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.065074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.067358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     toxic\n",
       "0   0  0.146779\n",
       "1   1  0.216815\n",
       "2   2  0.656836\n",
       "3   3  0.065074\n",
       "4   4  0.067358"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from apex import amp\n",
    "df_tst = pd.read_csv('/media/ssd/Downloads/submission.csv')\n",
    "df_tst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Doctor Who adlı viki başlığına 12. doctor olar...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вполне возможно, но я пока не вижу необходимо...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Quindi tu sei uno di quelli   conservativi  , ...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Malesef gerçekleştirilmedi ancak şöyle bir şey...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>:Resim:Seldabagcan.jpg resminde kaynak sorunu ...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            content lang\n",
       "0   0  Doctor Who adlı viki başlığına 12. doctor olar...   tr\n",
       "1   1   Вполне возможно, но я пока не вижу необходимо...   ru\n",
       "2   2  Quindi tu sei uno di quelli   conservativi  , ...   it\n",
       "3   3  Malesef gerçekleştirilmedi ancak şöyle bir şey...   tr\n",
       "4   4  :Resim:Seldabagcan.jpg resminde kaynak sorunu ...   tr"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(DATA_DIR + 'test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63812"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.146779</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.216815</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.656836</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.065074</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.067358</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     toxic lang\n",
       "0   0  0.146779   tr\n",
       "1   1  0.216815   ru\n",
       "2   2  0.656836   it\n",
       "3   3  0.065074   tr\n",
       "4   4  0.067358   tr"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tst = df_tst.merge(df_test[['id', 'lang']], on='id')\n",
    "df_tst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63812"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst.loc[df_tst['lang'].isin(['it']), 'toxic'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tst['lang'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst[['id', 'toxic']].to_csv('/media/ssd/Downloads/submission_test_it.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jig_env",
   "language": "python",
   "name": "jig_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "086421f7eec44c769f08f5f68fafafdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0e7c81c0da784e04b530236bad005c33": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18b951cd8c1447eaa1e12f972ac37d42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "  9%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9e743ecd26cd4877a539f9bbbd23d114",
       "max": 308,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e34fafda1e234d9c981322beff1068e7",
       "value": 29
      }
     },
     "35645b239e914aee807cfdc234fc5b75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dfdee9b109bc4176a0bc3a26ae38f7a3",
       "placeholder": "​",
       "style": "IPY_MODEL_086421f7eec44c769f08f5f68fafafdc",
       "value": " 29/308 [7:12:46&lt;66:18:40, 855.63s/it]"
      }
     },
     "554271f3ed5d48efa844608a0b72fdac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "685ecca481e3463d83a2465f15f7b33f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_80ae0d356c3e4b1bbe511c5f5a2694b5",
       "max": 4059,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a34804b28be74d7c9972a0a13410ba77",
       "value": 4059
      }
     },
     "7f49d7d685554687bc2d131959058cb3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80ae0d356c3e4b1bbe511c5f5a2694b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "840e74b11eb44a58b1ed0433e924dc82": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9e743ecd26cd4877a539f9bbbd23d114": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a34804b28be74d7c9972a0a13410ba77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "a61c8f56dbd74e29a155fad0d7095f79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_18b951cd8c1447eaa1e12f972ac37d42",
        "IPY_MODEL_35645b239e914aee807cfdc234fc5b75"
       ],
       "layout": "IPY_MODEL_7f49d7d685554687bc2d131959058cb3"
      }
     },
     "c9ab84b07a32426282543b5ff054ddec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_840e74b11eb44a58b1ed0433e924dc82",
       "placeholder": "​",
       "style": "IPY_MODEL_554271f3ed5d48efa844608a0b72fdac",
       "value": " 4059/4059 [14:53&lt;00:00,  4.54it/s]"
      }
     },
     "dfdee9b109bc4176a0bc3a26ae38f7a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e34fafda1e234d9c981322beff1068e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "fd692cff30e343aa8afa007a05d66acc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_685ecca481e3463d83a2465f15f7b33f",
        "IPY_MODEL_c9ab84b07a32426282543b5ff054ddec"
       ],
       "layout": "IPY_MODEL_0e7c81c0da784e04b530236bad005c33"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
