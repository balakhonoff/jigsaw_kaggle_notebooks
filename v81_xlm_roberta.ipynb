{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pytorch BERT baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, I convert https://www.kaggle.com/akensert/bert-base-tf2-0-minimalistic into pytorch version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please upvote the kernel if you find it helpful**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install HuggingFace transformers & sacremoses dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are not allowed to use internet I've created required datasets and commands to setup Hugging Face Transformers setup in offline mode. You can find the required github codebases in the datasets.\n",
    "\n",
    "* sacremoses dependency - https://www.kaggle.com/axel81/sacremoses\n",
    "* transformers - https://www.kaggle.com/axel81/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# !pip install ./sacremoses/sacremoses-master/\n",
    "# !pip install ./transformers/transformers-master/\n",
    "\n",
    "STRIDE = 1\n",
    "def is_jupyter():\n",
    "    try:\n",
    "        ipy_str = str(type(get_ipython()))\n",
    "        if 'zmqshell' in ipy_str:\n",
    "            return True\n",
    "        \n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Imports\n",
    "\n",
    "I've added imports that will be used in training too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kb/jig_env/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "/home/kb/jig_env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n",
      "/home/kb/jig_env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n",
      "/home/kb/jig_env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from random import shuffle as shfl\n",
    "from auc import MyAUCCallback\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max.columns', 500)\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-10.1/lib64'\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from shutil import copyfile\n",
    "from catalyst.dl import SupervisedRunner, AlchemyLogger, CriterionCallback\n",
    "from catalyst.dl.callbacks.metrics import AUCCallback\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler,Dataset\n",
    "batch_size = 7\n",
    "token = \"d1dd16f08d518293bcbeddd313b49aa4\"\n",
    "DATA_DIR = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on desktop\n"
     ]
    }
   ],
   "source": [
    "if os.uname()[1] == 'kb-Z370P-D3':\n",
    "    # desktop\n",
    "    LOG_PATH = '/media/ssd/logs/jigsaw'\n",
    "    SERVER = False\n",
    "    print('Working on desktop')\n",
    "elif os.uname()[1] == 'kb-server':\n",
    "    # server\n",
    "    LOG_PATH = '/home/kb/logs/jigsaw'\n",
    "    SERVER = True\n",
    "    print('Working on server')\n",
    "else:\n",
    "    raise Exception('which hostname???')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\[\n",
      "\n",
      "<>:5: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\d\n",
      "\n",
      "<>:6: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\(\n",
      "\n",
      "/home/kb/jig_env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = text.fillna(\"fillna\")#.str.lower()\n",
    "    text = text.map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "    text = text.map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "    text = text.map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    text = text.map(lambda x: re.sub(\"\\(http://.*?\\s\\(http://.*\\)\",'',str(x)))\n",
    "    return text\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"couldnt\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"doesnt\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"havent\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"shouldnt\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"thats\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"theres\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"theyre\":  \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def handle_contractions(x):\n",
    "    x = tokenizer.tokenize(x)\n",
    "    return x\n",
    "\n",
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x).replace(\"\\n\",\"\")\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def clean_data(df, columns: list):\n",
    "    for col in columns:\n",
    "        pass\n",
    "        df[col] = df[col].apply(lambda x: clean_numbers(x))\n",
    "        df[col] = clean(df[col])\n",
    "        df[col] = df[col].apply(lambda x: clean_text(x)) \n",
    "        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n",
    "# #         df[col] = df[col].apply(lambda x: handle_contractions(x))  \n",
    "#         df[col] = df[col].apply(lambda x: fix_quote(x))   \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D ' aww !  He matches this background colour I...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man ,  I ' m really not trying to edit war...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"  More I can ' t make any real suggestions o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You ,  sir ,  are my hero .  Any chance you re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation Why the edits made under my userna...      0   \n",
       "1  000103f0d9cfb60f  D ' aww !  He matches this background colour I...      0   \n",
       "2  000113f07ec002fd  Hey man ,  I ' m really not trying to edit war...      0   \n",
       "3  0001b41b1c6bb37e   \"  More I can ' t make any real suggestions o...      0   \n",
       "4  0001d958c54c6e35  You ,  sir ,  are my hero .  Any chance you re...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_toxic = pd.read_csv(DATA_DIR+'jigsaw-toxic-comment-train.csv')\n",
    "df_train_toxic = clean_data(df_train_toxic, ['comment_text'])\n",
    "df_train_toxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:41.987077+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:42.870083+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:45.222647+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:47.601894+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:48.488476+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                       comment_text     toxic  \\\n",
       "0  59848  This is so cool. It's like, 'would you want yo...  0.000000   \n",
       "1  59849  Thank you!! This would make my life a lot less...  0.000000   \n",
       "2  59852  This is such an urgent design problem; kudos t...  0.000000   \n",
       "3  59855  Is this something I'll be able to install on m...  0.000000   \n",
       "4  59856               haha you guys are a bunch of losers.  0.893617   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
       "0         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "1         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "2         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "3         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "4         0.021277      0.0         0.021277  0.87234     0.0    0.0      0.0   \n",
       "\n",
       "   bisexual  black  buddhist  christian  female  heterosexual  hindu  \\\n",
       "0       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "1       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "2       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "3       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "4       0.0    0.0       0.0        0.0     0.0           0.0    0.0   \n",
       "\n",
       "   homosexual_gay_or_lesbian  intellectual_or_learning_disability  jewish  \\\n",
       "0                        NaN                                  NaN     NaN   \n",
       "1                        NaN                                  NaN     NaN   \n",
       "2                        NaN                                  NaN     NaN   \n",
       "3                        NaN                                  NaN     NaN   \n",
       "4                        0.0                                 0.25     0.0   \n",
       "\n",
       "   latino  male  muslim  other_disability  other_gender  \\\n",
       "0     NaN   NaN     NaN               NaN           NaN   \n",
       "1     NaN   NaN     NaN               NaN           NaN   \n",
       "2     NaN   NaN     NaN               NaN           NaN   \n",
       "3     NaN   NaN     NaN               NaN           NaN   \n",
       "4     0.0   0.0     0.0               0.0           0.0   \n",
       "\n",
       "   other_race_or_ethnicity  other_religion  other_sexual_orientation  \\\n",
       "0                      NaN             NaN                       NaN   \n",
       "1                      NaN             NaN                       NaN   \n",
       "2                      NaN             NaN                       NaN   \n",
       "3                      NaN             NaN                       NaN   \n",
       "4                      0.0             0.0                       0.0   \n",
       "\n",
       "   physical_disability  psychiatric_or_mental_illness  transgender  white  \\\n",
       "0                  NaN                            NaN          NaN    NaN   \n",
       "1                  NaN                            NaN          NaN    NaN   \n",
       "2                  NaN                            NaN          NaN    NaN   \n",
       "3                  NaN                            NaN          NaN    NaN   \n",
       "4                  0.0                            0.0          0.0    0.0   \n",
       "\n",
       "                    created_date  publication_id  parent_id  article_id  \\\n",
       "0  2015-09-29 10:50:41.987077+00               2        NaN        2006   \n",
       "1  2015-09-29 10:50:42.870083+00               2        NaN        2006   \n",
       "2  2015-09-29 10:50:45.222647+00               2        NaN        2006   \n",
       "3  2015-09-29 10:50:47.601894+00               2        NaN        2006   \n",
       "4  2015-09-29 10:50:48.488476+00               2        NaN        2006   \n",
       "\n",
       "     rating  funny  wow  sad  likes  disagree  sexual_explicit  \\\n",
       "0  rejected      0    0    0      0         0              0.0   \n",
       "1  rejected      0    0    0      0         0              0.0   \n",
       "2  rejected      0    0    0      0         0              0.0   \n",
       "3  rejected      0    0    0      0         0              0.0   \n",
       "4  rejected      0    0    0      1         0              0.0   \n",
       "\n",
       "   identity_annotator_count  toxicity_annotator_count  \n",
       "0                         0                         4  \n",
       "1                         0                         4  \n",
       "2                         0                         4  \n",
       "3                         0                         4  \n",
       "4                         4                        47  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_bias = pd.read_csv(DATA_DIR + 'jigsaw-unintended-bias-train.csv')\n",
    "df_train_bias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66651.75, 25124)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(df_train_toxic[df_train_toxic['toxic']==1]),len(df_train_toxic[df_train_toxic['toxic']==0])\n",
    "# len(df_train_bias[df_train_bias['toxic']<0.1])/7,len(df_train_bias[df_train_bias['toxic']>0.80])\n",
    "# len(df_train_bias[df_train_bias['toxic']==1]),len(df_train_bias[df_train_bias['toxic']==0])\n",
    "len(df_train_bias[df_train_bias['toxic']==0])/20,len(df_train_bias[df_train_bias['toxic']>0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_bias_pos = df_train_bias[df_train_bias['toxic']>0.8].reset_index(drop=True)\n",
    "df_train_bias_pos['toxic'] = 1\n",
    "df_train_bias_neg = shuffle(df_train_bias[df_train_bias['toxic']<0.1].reset_index(drop=True)).reset_index(drop=True).iloc[::20]\n",
    "df_train_bias_neg['toxic'] = 0\n",
    "df_train_bias =  df_train_bias_pos.append(df_train_bias_neg).reset_index(drop=True)\n",
    "df_train_bias = clean_data(df_train_bias, ['comment_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid = shuffle(pd.read_csv(DATA_DIR + 'validation.csv'))\n",
    "# df_valid = clean_data(df_valid, ['comment_text'])\n",
    "# df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>toxic</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>2586</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "      <td>sorry but I am afraid of the Jews  .  .  .  . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>587</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "      <td>My name is Luigi Barchi ,  I ' d put months ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>1318</td>\n",
       "      <td>tr</td>\n",
       "      <td>0</td>\n",
       "      <td>I do not find much to contribute to the wiki ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6578</th>\n",
       "      <td>6578</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>OLa terricolas in 5 years ducks dominate the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>4972</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "      <td>hello ,  it could definitely use a source for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id lang  toxic                                       comment_text\n",
       "2586  2586   it      0  sorry but I am afraid of the Jews  .  .  .  . ...\n",
       "587    587   it      0  My name is Luigi Barchi ,  I ' d put months ag...\n",
       "1318  1318   tr      0  I do not find much to contribute to the wiki ....\n",
       "6578  6578   es      0  OLa terricolas in 5 years ducks dominate the w...\n",
       "4972  4972   it      0  hello ,  it could definitely use a source for ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_en = shuffle(pd.read_csv(DATA_DIR + 'validation_en.csv'))\n",
    "df_valid_en = df_valid_en.drop(['comment_text'],axis=1).rename(columns={'comment_text_en':'comment_text'})\n",
    "df_valid_en = clean_data(df_valid_en, ['comment_text'])\n",
    "df_valid_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th>toxic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">es</th>\n",
       "      <th>0</th>\n",
       "      <td>2078</td>\n",
       "      <td>2078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>422</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">it</th>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tr</th>\n",
       "      <th>0</th>\n",
       "      <td>2680</td>\n",
       "      <td>2680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>320</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  comment_text\n",
       "lang toxic                    \n",
       "es   0      2078          2078\n",
       "     1       422           422\n",
       "it   0      2012          2012\n",
       "     1       488           488\n",
       "tr   0      2680          2680\n",
       "     1       320           320"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_en.groupby(['lang', 'toxic']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv(DATA_DIR + 'test.csv')\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.groupby(['lang']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  toxic\n",
       "0   0    0.5\n",
       "1   1    0.5\n",
       "2   2    0.5\n",
       "3   3    0.5\n",
       "4   4    0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'toxic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kb/jig_env/lib/python3.7/site-packages/torchvision/extension.py:11: ResourceWarning:\n",
      "\n",
      "unclosed file <_io.BufferedReader name='/home/kb/jig_env/lib/python3.7/site-packages/torchvision/_C.so'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import time\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "#import torch.utils.data as data\n",
    "from torchvision import datasets, models, transforms\n",
    "from transformers import *\n",
    "import random\n",
    "from math import floor, ceil\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "MAX_LEN = 512\n",
    "SEP_TOKEN_ID = 102\n",
    "\n",
    "class QuestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, train_mode=True, labeled=True):\n",
    "        \n",
    "            \n",
    "        self.df = df\n",
    "        if train_mode:\n",
    "            self.labels = df.toxic.values\n",
    "            self.toxic_inds = np.where(self.labels==1)[0]\n",
    "            self.normal_inds = np.where(self.labels==0)[0]            \n",
    "            \n",
    "            \n",
    "            print(f'Here is {len(self.labels)} samples, {len(self.toxic_inds)} samples and {len(self.normal_inds)} samples')\n",
    "            print(f'Class balance is {len(self.toxic_inds)/len(self.labels):.2f}')\n",
    "            \n",
    "        self.train_mode = train_mode\n",
    "        self.labeled = labeled\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#         self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "#         self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "#         self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased',\n",
    "#                                                                 do_lower_case=False,\n",
    "#                                                                 do_basic_tokenize=True,\n",
    "#                                                                 never_split=None,\n",
    "#                                                                 unk_token='[UNK]',\n",
    "#                                                                 sep_token='[SEP]',\n",
    "#                                                                 pad_token='[PAD]',\n",
    "#                                                                 cls_token='[CLS]',\n",
    "#                                                                 mask_token='[MASK]',\n",
    "#                                                                 tokenize_chinese_chars=True,)\n",
    "        #distil\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        token_ids = self.get_token_ids(row)\n",
    "        \n",
    "        if self.labeled:\n",
    "            labels = self.get_label(row)\n",
    "            return {'features': token_ids, 'targets': labels}\n",
    "\n",
    "        else:\n",
    "            return {'features': token_ids}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def trim_input(self, text, max_sequence_length=MAX_LEN):\n",
    "        t = self.tokenizer.tokenize(text)\n",
    "        t_len = len(t)\n",
    "\n",
    "        if t_len + 2 > max_sequence_length:\n",
    "\n",
    "            t_new_len = int(max_sequence_length) - 2\n",
    "\n",
    "            t = t[:t_new_len]\n",
    "\n",
    "        return t\n",
    "        \n",
    "    def get_token_ids(self, row):\n",
    "        token_ids = self.tokenizer.encode(row.comment_text, max_length=MAX_LEN)\n",
    "#         print(token_ids)\n",
    "#         print(token_ids + [0] * (MAX_LEN - len(token_ids)))\n",
    "        if len(token_ids) < MAX_LEN:\n",
    "            ids = torch.tensor(token_ids + [0] * (MAX_LEN - len(token_ids)))\n",
    "        else:\n",
    "            ids = torch.tensor(token_ids[:MAX_LEN])\n",
    "#         print(ids.shape)\n",
    "        \n",
    "#         t_tokens = self.trim_input(row.comment_text)\n",
    "\n",
    "# #         tokens = ['[CLS]'] + t_tokens  + ['[SEP]']+ t_tokens[-1::-1]+ ['[SEP]']\n",
    "#         tokens = ['[CLS]'] + t_tokens  + ['[SEP]']\n",
    "#         token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "#         if len(token_ids) < MAX_LEN:\n",
    "#             token_ids += [0] * (MAX_LEN - len(token_ids))\n",
    "            \n",
    "#         ids = torch.tensor(token_ids)\n",
    "#         print(ids.shape, torch.tensor(token_ids).shape)\n",
    "#         print(torch.tensor(token_ids))\n",
    "#         print(len(ids))\n",
    "        return ids\n",
    "\n",
    "    def get_label(self, row):\n",
    "#         label = torch.tensor(row[target_column].astype(np.long))\n",
    "        label = np.round(row[target_column])\n",
    "        return torch.tensor([1-label, label]).float()\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        token_ids = torch.stack([x[0] for x in batch])\n",
    "\n",
    "        if self.labeled:\n",
    "            labels = torch.stack([x[1] for x in batch])\n",
    "            return {'features': token_ids, 'targets': labels}\n",
    "        else:\n",
    "            return {'features': token_ids}\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class QuestModel(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        super(QuestModel, self).__init__()\n",
    "        self.model_name = 'QuestModel'\n",
    "        \n",
    "#         self.bert_model = BertModel.from_pretrained('bert-base-uncased') \n",
    "#         self.bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "#         self.bert_model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "#         self.bert_model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "  #      self.bert_model = XLMRobertaModel.from_pretrained('xlm-roberta-base', \n",
    "                                                          #output_hidden_states=False, \n",
    "                                                          #output_attentions=False)\n",
    "#         self.bert_model = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
    "        self.bert_model = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "    \n",
    "        self.fc = nn.Linear(768, n_classes)\n",
    "#         self.fc = nn.Linear(1024, n_classes)\n",
    "\n",
    "    def forward(self, ids):\n",
    "#         attention_mask = (ids > 0)\n",
    "#         print(ids.shape)\n",
    "        layers = self.bert_model(input_ids=ids)#, attention_mask=attention_mask)\n",
    "#         print(layers[0].shape)\n",
    "#         out = F.dropout(layers[0][:, 0, :], p=0.2, training=self.training)\n",
    "#         print(layers[0].shape)\n",
    "#         print([l.shape for l in layers])\n",
    "#         out = F.dropout(layers[-1][:, 0, :], p=0.35, training=self.training)\n",
    "        out = F.dropout(layers[0][:, 0, :], p=0.35, training=self.training)\n",
    "        logit = self.fc(out)#.unsqueeze(1)\n",
    "        return logit #, 'for_auc': logit[:, 1]}#[:,1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model = XLMRobertaModel.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, dataset):\n",
    "       \n",
    "        self.toxic_inds = dataset.toxic_inds.copy()\n",
    "        self.normal_inds = dataset.normal_inds.copy()\n",
    "        \n",
    "        self.num_samples = 2*min(len(self.toxic_inds), len(self.normal_inds))\n",
    "        \n",
    "        shfl(self.toxic_inds)\n",
    "        shfl(self.normal_inds)\n",
    "        \n",
    "        self.inds = []\n",
    "        for i in range(min(len(self.toxic_inds), len(self.normal_inds))):\n",
    "            self.inds.append(self.normal_inds[i%len(self.normal_inds)])\n",
    "            self.inds.append(self.toxic_inds[i%len(self.toxic_inds)])\n",
    "\n",
    "    def __iter__(self):\n",
    "        #print ('\\tcalling Sampler:__iter__')\n",
    "        return iter(self.inds)\n",
    "\n",
    "    def __len__(self):\n",
    "        #print ('\\tcalling Sampler:__len__')\n",
    "        return self.num_samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "        callback_get_label func: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None, callback_get_label=None):\n",
    "                \n",
    "        # if indices is not provided, \n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # define custom callback\n",
    "        self.callback_get_label = callback_get_label\n",
    "\n",
    "        # if num_samples is not provided, \n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "            \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "                \n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        if self.callback_get_label:\n",
    "            return self.callback_get_label(dataset, idx)\n",
    "        else:\n",
    "            dataset.labels[idx]\n",
    "#             raise NotImplementedError\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "            self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'bias': df_train_bias,\n",
    "    'toxic': df_train_toxic,\n",
    "    'valid': df_valid_en,\n",
    "#     'valid2': df_test\n",
    "}\n",
    "def get_loaders(train_pt='bias', valid_pt='valid', test_pt=None, to_balance=True, shuffle_before=True):\n",
    "    if SERVER:\n",
    "        workers=1\n",
    "    else:\n",
    "        workers = 6\n",
    "    \n",
    "    if isinstance(train_pt, list):\n",
    "        df_train = dataframes[train_pt[0]][['comment_text', 'toxic']]\n",
    "        for pt in train_pt[1:]:\n",
    "            df_train = df_train.append(dataframes[pt][['comment_text', 'toxic']]).reset_index(drop=True)\n",
    "    else:\n",
    "        df_train = dataframes[train_pt][['comment_text', 'toxic']]\n",
    "    \n",
    "    if shuffle_before:\n",
    "        df_train = shuffle(df_train).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "#     train_dataset = QuestDataset(df_train.iloc[::STRIDE], train_mode=True)\n",
    "    \n",
    "    if STRIDE == 1:\n",
    "        train_dataset = QuestDataset(df_train.iloc[::STRIDE], train_mode=True)\n",
    "    elif STRIDE<=10:\n",
    "        df_train_pos = df_train[df_train[target_column]==1].reset_index(drop=True)\n",
    "        df_train_neg = df_train[df_train[target_column]==0].reset_index(drop=True).iloc[::STRIDE]\n",
    "        train_dataset = QuestDataset(df_train_pos.append(df_train_neg).reset_index(drop=True), train_mode=True)\n",
    "    else:\n",
    "        df_train_pos = df_train[df_train[target_column]==1].reset_index(drop=True).iloc[::int(STRIDE/10)]\n",
    "        df_train_neg = df_train[df_train[target_column]==0].reset_index(drop=True).iloc[::STRIDE]\n",
    "        train_dataset = QuestDataset(df_train_pos.append(df_train_neg).reset_index(drop=True), train_mode=True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    valid_dataset = QuestDataset(dataframes[valid_pt], train_mode=False)\n",
    "    if test_pt != None:\n",
    "        test_dataset = QuestDataset(dataframes[test_pt].iloc[::STRIDE], train_mode=False)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        num_workers=workers,\n",
    "        sampler=BalancedSampler(train_dataset) if to_balance else None,#ImbalancedDatasetSampler(train_dataset) if to_balance else None,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        num_workers=workers,\n",
    "        batch_size=batch_size,\n",
    "    )    \n",
    "    if test_pt != None:\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            num_workers=workers,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "       \n",
    "    loaders = {}\n",
    "    loaders['train'] = train_loader\n",
    "    \n",
    "    loaders['valid'] = valid_loader\n",
    "    \n",
    "    if test_pt != None:\n",
    "        loaders['valid2'] = test_loader\n",
    "    \n",
    "    for i in ['es', 'it', 'tr']:\n",
    "        df = dataframes[valid_pt]\n",
    "        df = df[df['lang']==i]\n",
    "\n",
    "        loaders['valid_'+ i] = DataLoader(\n",
    "            QuestDataset(df, train_mode=False),\n",
    "            num_workers=workers,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is 315403 samples, 46508 samples and 268895 samples\n",
      "Class balance is 0.15\n",
      "----------------Experiment: learn_both\n",
      "1/300 * Epoch (train):   0% 0/13288 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kb/jig_env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning:\n",
      "\n",
      "size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/300 * Epoch (train): 100% 13288/13288 [1:02:42<00:00,  3.53it/s, loss=0.016]  \n",
      "1/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.24it/s, loss=0.067]\n",
      "1/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.16it/s, loss=0.178]\n",
      "1/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.17it/s, loss=0.003]\n",
      "1/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.16it/s, loss=0.052]\n",
      "[2020-04-29 10:39:33,100] \n",
      "1/300 * Epoch 1 (_base): lr=1.000e-05 | momentum=0.9000\n",
      "1/300 * Epoch 1 (train): auc/_mean=0.8339 | auc/class_0=0.8339 | loss=0.0355\n",
      "1/300 * Epoch 1 (valid): auc/_mean=0.8723 | auc/class_0=0.8723 | es_auc/_mean=0.8625 | es_auc/class_0=0.8625 | es_loss=0.0353 | it_auc/_mean=0.8295 | it_auc/class_0=0.8295 | it_loss=0.0445 | loss=0.0309 | tr_auc/_mean=0.9234 | tr_auc/class_0=0.9234 | tr_loss=0.0160\n",
      "1/300 * Epoch 1 (valid_es): auc/_mean=0.8625 | auc/class_0=0.8625 | loss=0.0353\n",
      "1/300 * Epoch 1 (valid_it): auc/_mean=0.8295 | auc/class_0=0.8295 | loss=0.0445\n",
      "1/300 * Epoch 1 (valid_tr): auc/_mean=0.9234 | auc/class_0=0.9234 | loss=0.0160\n",
      "2/300 * Epoch (train): 100% 13288/13288 [1:02:45<00:00,  3.53it/s, loss=0.010]    \n",
      "2/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.22it/s, loss=0.089]    \n",
      "2/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.15it/s, loss=0.327]    \n",
      "2/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.15it/s, loss=0.005]    \n",
      "2/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.15it/s, loss=0.049]    \n",
      "[2020-04-29 11:45:59,297] \n",
      "2/300 * Epoch 2 (_base): lr=1.000e-05 | momentum=0.9000\n",
      "2/300 * Epoch 2 (train): auc/_mean=0.9605 | auc/class_0=0.9605 | loss=0.0187\n",
      "2/300 * Epoch 2 (valid): auc/_mean=0.8816 | auc/class_0=0.8816 | es_auc/_mean=0.8676 | es_auc/class_0=0.8676 | es_loss=0.0474 | it_auc/_mean=0.8391 | it_auc/class_0=0.8391 | it_loss=0.0558 | loss=0.0378 | tr_auc/_mean=0.9292 | tr_auc/class_0=0.9292 | tr_loss=0.0153\n",
      "2/300 * Epoch 2 (valid_es): auc/_mean=0.8676 | auc/class_0=0.8676 | loss=0.0474\n",
      "2/300 * Epoch 2 (valid_it): auc/_mean=0.8391 | auc/class_0=0.8391 | loss=0.0558\n",
      "2/300 * Epoch 2 (valid_tr): auc/_mean=0.9292 | auc/class_0=0.9292 | loss=0.0153\n",
      "3/300 * Epoch (train): 100% 13288/13288 [1:02:49<00:00,  3.53it/s, loss=0.005]    \n",
      "3/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.16it/s, loss=0.065]    \n",
      "3/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.08it/s, loss=0.211]    \n",
      "3/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.07it/s, loss=0.002]    \n",
      "3/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.12it/s, loss=0.044]    \n",
      "[2020-04-29 12:52:58,089] \n",
      "3/300 * Epoch 3 (_base): lr=1.000e-05 | momentum=0.9000\n",
      "3/300 * Epoch 3 (train): auc/_mean=0.9717 | auc/class_0=0.9717 | loss=0.0158\n",
      "3/300 * Epoch 3 (valid): auc/_mean=0.8928 | auc/class_0=0.8928 | es_auc/_mean=0.8742 | es_auc/class_0=0.8742 | es_loss=0.0441 | it_auc/_mean=0.8490 | it_auc/class_0=0.8490 | it_loss=0.0521 | loss=0.0359 | tr_auc/_mean=0.9459 | tr_auc/class_0=0.9459 | tr_loss=0.0158\n",
      "3/300 * Epoch 3 (valid_es): auc/_mean=0.8742 | auc/class_0=0.8742 | loss=0.0441\n",
      "3/300 * Epoch 3 (valid_it): auc/_mean=0.8490 | auc/class_0=0.8490 | loss=0.0521\n",
      "3/300 * Epoch 3 (valid_tr): auc/_mean=0.9459 | auc/class_0=0.9459 | loss=0.0158\n",
      "4/300 * Epoch (train): 100% 13288/13288 [1:02:52<00:00,  3.52it/s, loss=0.017]    \n",
      "4/300 * Epoch (valid): 100% 1143/1143 [01:34<00:00, 12.15it/s, loss=0.082]    \n",
      "4/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.07it/s, loss=0.411]    \n",
      "4/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.00it/s, loss=0.002]    \n",
      "4/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.01it/s, loss=0.019]    \n",
      "[2020-04-29 13:59:59,590] \n",
      "4/300 * Epoch 4 (_base): lr=1.000e-05 | momentum=0.9000\n",
      "4/300 * Epoch 4 (train): auc/_mean=0.9775 | auc/class_0=0.9775 | loss=0.0141\n",
      "4/300 * Epoch 4 (valid): auc/_mean=0.8936 | auc/class_0=0.8936 | es_auc/_mean=0.8784 | es_auc/class_0=0.8784 | es_loss=0.0521 | it_auc/_mean=0.8483 | it_auc/class_0=0.8483 | it_loss=0.0613 | loss=0.0412 | tr_auc/_mean=0.9470 | tr_auc/class_0=0.9470 | tr_loss=0.0160\n",
      "4/300 * Epoch 4 (valid_es): auc/_mean=0.8784 | auc/class_0=0.8784 | loss=0.0521\n",
      "4/300 * Epoch 4 (valid_it): auc/_mean=0.8483 | auc/class_0=0.8483 | loss=0.0613\n",
      "4/300 * Epoch 4 (valid_tr): auc/_mean=0.9470 | auc/class_0=0.9470 | loss=0.0160\n",
      "5/300 * Epoch (train): 100% 13288/13288 [1:02:53<00:00,  3.52it/s, loss=0.002]    \n",
      "5/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.20it/s, loss=0.091]    \n",
      "5/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.12it/s, loss=0.315]    \n",
      "5/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.10it/s, loss=0.008]    \n",
      "5/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.12it/s, loss=0.055]    \n",
      "[2020-04-29 15:07:01,172] \n",
      "5/300 * Epoch 5 (_base): lr=1.000e-05 | momentum=0.9000\n",
      "5/300 * Epoch 5 (train): auc/_mean=0.9810 | auc/class_0=0.9810 | loss=0.0129\n",
      "5/300 * Epoch 5 (valid): auc/_mean=0.8959 | auc/class_0=0.8959 | es_auc/_mean=0.8785 | es_auc/class_0=0.8785 | es_loss=0.0430 | it_auc/_mean=0.8523 | it_auc/class_0=0.8523 | it_loss=0.0506 | loss=0.0341 | tr_auc/_mean=0.9505 | tr_auc/class_0=0.9505 | tr_loss=0.0133\n",
      "5/300 * Epoch 5 (valid_es): auc/_mean=0.8785 | auc/class_0=0.8785 | loss=0.0430\n",
      "5/300 * Epoch 5 (valid_it): auc/_mean=0.8523 | auc/class_0=0.8523 | loss=0.0506\n",
      "5/300 * Epoch 5 (valid_tr): auc/_mean=0.9505 | auc/class_0=0.9505 | loss=0.0133\n",
      "6/300 * Epoch (train): 100% 13288/13288 [1:02:47<00:00,  3.53it/s, loss=0.003]    \n",
      "6/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.21it/s, loss=0.108]    \n",
      "6/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.14it/s, loss=0.399]    \n",
      "6/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.14it/s, loss=0.003]    \n",
      "6/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.15it/s, loss=0.062]    \n",
      "[2020-04-29 16:13:54,826] \n",
      "6/300 * Epoch 6 (_base): lr=1.000e-05 | momentum=0.9000\n",
      "6/300 * Epoch 6 (train): auc/_mean=0.9839 | auc/class_0=0.9839 | loss=0.0119\n",
      "6/300 * Epoch 6 (valid): auc/_mean=0.8885 | auc/class_0=0.8885 | es_auc/_mean=0.8733 | es_auc/class_0=0.8733 | es_loss=0.0476 | it_auc/_mean=0.8455 | it_auc/class_0=0.8455 | it_loss=0.0570 | loss=0.0389 | tr_auc/_mean=0.9389 | tr_auc/class_0=0.9389 | tr_loss=0.0172\n",
      "6/300 * Epoch 6 (valid_es): auc/_mean=0.8733 | auc/class_0=0.8733 | loss=0.0476\n",
      "6/300 * Epoch 6 (valid_it): auc/_mean=0.8455 | auc/class_0=0.8455 | loss=0.0570\n",
      "6/300 * Epoch 6 (valid_tr): auc/_mean=0.9389 | auc/class_0=0.9389 | loss=0.0172\n",
      "7/300 * Epoch (train): 100% 13288/13288 [1:02:47<00:00,  3.53it/s, loss=6.032e-04]\n",
      "7/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.21it/s, loss=0.103]    \n",
      "7/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.14it/s, loss=0.337]    \n",
      "7/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.13it/s, loss=0.002]    \n",
      "7/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.14it/s, loss=0.069]    \n",
      "[2020-04-29 17:20:14,100] \n",
      "7/300 * Epoch 7 (_base): lr=2.500e-06 | momentum=0.9000\n",
      "7/300 * Epoch 7 (train): auc/_mean=0.9856 | auc/class_0=0.9856 | loss=0.0112\n",
      "7/300 * Epoch 7 (valid): auc/_mean=0.8928 | auc/class_0=0.8928 | es_auc/_mean=0.8785 | es_auc/class_0=0.8785 | es_loss=0.0467 | it_auc/_mean=0.8491 | it_auc/class_0=0.8491 | it_loss=0.0566 | loss=0.0383 | tr_auc/_mean=0.9444 | tr_auc/class_0=0.9444 | tr_loss=0.0166\n",
      "7/300 * Epoch 7 (valid_es): auc/_mean=0.8785 | auc/class_0=0.8785 | loss=0.0467\n",
      "7/300 * Epoch 7 (valid_it): auc/_mean=0.8491 | auc/class_0=0.8491 | loss=0.0566\n",
      "7/300 * Epoch 7 (valid_tr): auc/_mean=0.9444 | auc/class_0=0.9444 | loss=0.0166\n",
      "8/300 * Epoch (train): 100% 13288/13288 [1:02:47<00:00,  3.53it/s, loss=5.291e-04]\n",
      "8/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.22it/s, loss=0.114]    \n",
      "8/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.15it/s, loss=0.451]    \n",
      "8/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.15it/s, loss=0.002]    \n",
      "8/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.15it/s, loss=0.058]    \n",
      "[2020-04-29 18:26:32,653] \n",
      "8/300 * Epoch 8 (_base): lr=2.500e-06 | momentum=0.9000\n",
      "8/300 * Epoch 8 (train): auc/_mean=0.9891 | auc/class_0=0.9891 | loss=0.0097\n",
      "8/300 * Epoch 8 (valid): auc/_mean=0.8966 | auc/class_0=0.8966 | es_auc/_mean=0.8804 | es_auc/class_0=0.8804 | es_loss=0.0475 | it_auc/_mean=0.8540 | it_auc/class_0=0.8540 | it_loss=0.0554 | loss=0.0388 | tr_auc/_mean=0.9478 | tr_auc/class_0=0.9478 | tr_loss=0.0184\n",
      "8/300 * Epoch 8 (valid_es): auc/_mean=0.8804 | auc/class_0=0.8804 | loss=0.0475\n",
      "8/300 * Epoch 8 (valid_it): auc/_mean=0.8540 | auc/class_0=0.8540 | loss=0.0554\n",
      "8/300 * Epoch 8 (valid_tr): auc/_mean=0.9478 | auc/class_0=0.9478 | loss=0.0184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/300 * Epoch (train): 100% 13288/13288 [1:02:46<00:00,  3.53it/s, loss=0.001]    \n",
      "9/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.23it/s, loss=0.137]    \n",
      "9/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.15it/s, loss=0.542]    \n",
      "9/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.15it/s, loss=0.002]    \n",
      "9/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.16it/s, loss=0.069]    \n",
      "[2020-04-29 19:33:26,423] \n",
      "9/300 * Epoch 9 (_base): lr=2.500e-06 | momentum=0.9000\n",
      "9/300 * Epoch 9 (train): auc/_mean=0.9902 | auc/class_0=0.9902 | loss=0.0092\n",
      "9/300 * Epoch 9 (valid): auc/_mean=0.8991 | auc/class_0=0.8991 | es_auc/_mean=0.8816 | es_auc/class_0=0.8816 | es_loss=0.0503 | it_auc/_mean=0.8571 | it_auc/class_0=0.8571 | it_loss=0.0582 | loss=0.0408 | tr_auc/_mean=0.9515 | tr_auc/class_0=0.9515 | tr_loss=0.0192\n",
      "9/300 * Epoch 9 (valid_es): auc/_mean=0.8816 | auc/class_0=0.8816 | loss=0.0503\n",
      "9/300 * Epoch 9 (valid_it): auc/_mean=0.8571 | auc/class_0=0.8571 | loss=0.0582\n",
      "9/300 * Epoch 9 (valid_tr): auc/_mean=0.9515 | auc/class_0=0.9515 | loss=0.0192\n",
      "10/300 * Epoch (train): 100% 13288/13288 [1:02:48<00:00,  3.53it/s, loss=3.074e-04]\n",
      "10/300 * Epoch (valid): 100% 1143/1143 [01:34<00:00, 12.14it/s, loss=0.121]    \n",
      "10/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.06it/s, loss=0.459]    \n",
      "10/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.16it/s, loss=0.003]    \n",
      "10/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.16it/s, loss=0.066]    \n",
      "[2020-04-29 20:40:24,707] \n",
      "10/300 * Epoch 10 (_base): lr=2.500e-06 | momentum=0.9000\n",
      "10/300 * Epoch 10 (train): auc/_mean=0.9904 | auc/class_0=0.9904 | loss=0.0090\n",
      "10/300 * Epoch 10 (valid): auc/_mean=0.8982 | auc/class_0=0.8982 | es_auc/_mean=0.8808 | es_auc/class_0=0.8808 | es_loss=0.0479 | it_auc/_mean=0.8559 | it_auc/class_0=0.8559 | it_loss=0.0560 | loss=0.0394 | tr_auc/_mean=0.9502 | tr_auc/class_0=0.9502 | tr_loss=0.0192\n",
      "10/300 * Epoch 10 (valid_es): auc/_mean=0.8808 | auc/class_0=0.8808 | loss=0.0479\n",
      "10/300 * Epoch 10 (valid_it): auc/_mean=0.8559 | auc/class_0=0.8559 | loss=0.0560\n",
      "10/300 * Epoch 10 (valid_tr): auc/_mean=0.9502 | auc/class_0=0.9502 | loss=0.0192\n",
      "11/300 * Epoch (train): 100% 13288/13288 [1:02:57<00:00,  3.52it/s, loss=0.003]    \n",
      "11/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.21it/s, loss=0.120]    \n",
      "11/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.14it/s, loss=0.462]    \n",
      "11/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.14it/s, loss=0.004]    \n",
      "11/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.14it/s, loss=0.064]    \n",
      "[2020-04-29 21:46:53,507] \n",
      "11/300 * Epoch 11 (_base): lr=2.500e-06 | momentum=0.9000\n",
      "11/300 * Epoch 11 (train): auc/_mean=0.9912 | auc/class_0=0.9912 | loss=0.0087\n",
      "11/300 * Epoch 11 (valid): auc/_mean=0.8971 | auc/class_0=0.8971 | es_auc/_mean=0.8820 | es_auc/class_0=0.8820 | es_loss=0.0462 | it_auc/_mean=0.8558 | it_auc/class_0=0.8558 | it_loss=0.0533 | loss=0.0377 | tr_auc/_mean=0.9467 | tr_auc/class_0=0.9467 | tr_loss=0.0184\n",
      "11/300 * Epoch 11 (valid_es): auc/_mean=0.8820 | auc/class_0=0.8820 | loss=0.0462\n",
      "11/300 * Epoch 11 (valid_it): auc/_mean=0.8558 | auc/class_0=0.8558 | loss=0.0533\n",
      "11/300 * Epoch 11 (valid_tr): auc/_mean=0.9467 | auc/class_0=0.9467 | loss=0.0184\n",
      "12/300 * Epoch (train): 100% 13288/13288 [1:02:46<00:00,  3.53it/s, loss=5.147e-04]\n",
      "12/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.26it/s, loss=0.119]    \n",
      "12/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.19it/s, loss=0.474]    \n",
      "12/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.15it/s, loss=0.002]    \n",
      "12/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.13it/s, loss=0.060]    \n",
      "[2020-04-29 22:53:11,524] \n",
      "12/300 * Epoch 12 (_base): lr=2.500e-06 | momentum=0.9000\n",
      "12/300 * Epoch 12 (train): auc/_mean=0.9917 | auc/class_0=0.9917 | loss=0.0084\n",
      "12/300 * Epoch 12 (valid): auc/_mean=0.8962 | auc/class_0=0.8962 | es_auc/_mean=0.8803 | es_auc/class_0=0.8803 | es_loss=0.0512 | it_auc/_mean=0.8537 | it_auc/class_0=0.8537 | it_loss=0.0589 | loss=0.0416 | tr_auc/_mean=0.9474 | tr_auc/class_0=0.9474 | tr_loss=0.0199\n",
      "12/300 * Epoch 12 (valid_es): auc/_mean=0.8803 | auc/class_0=0.8803 | loss=0.0512\n",
      "12/300 * Epoch 12 (valid_it): auc/_mean=0.8537 | auc/class_0=0.8537 | loss=0.0589\n",
      "12/300 * Epoch 12 (valid_tr): auc/_mean=0.9474 | auc/class_0=0.9474 | loss=0.0199\n",
      "13/300 * Epoch (train): 100% 13288/13288 [1:02:45<00:00,  3.53it/s, loss=0.003]    \n",
      "13/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.18it/s, loss=0.120]    \n",
      "13/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.05it/s, loss=0.482]    \n",
      "13/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.10it/s, loss=0.002]    \n",
      "13/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.16it/s, loss=0.059]    \n",
      "[2020-04-29 23:59:28,648] \n",
      "13/300 * Epoch 13 (_base): lr=6.250e-07 | momentum=0.9000\n",
      "13/300 * Epoch 13 (train): auc/_mean=0.9922 | auc/class_0=0.9922 | loss=0.0082\n",
      "13/300 * Epoch 13 (valid): auc/_mean=0.8955 | auc/class_0=0.8955 | es_auc/_mean=0.8797 | es_auc/class_0=0.8797 | es_loss=0.0516 | it_auc/_mean=0.8524 | it_auc/class_0=0.8524 | it_loss=0.0590 | loss=0.0424 | tr_auc/_mean=0.9464 | tr_auc/class_0=0.9464 | tr_loss=0.0216\n",
      "13/300 * Epoch 13 (valid_es): auc/_mean=0.8797 | auc/class_0=0.8797 | loss=0.0516\n",
      "13/300 * Epoch 13 (valid_it): auc/_mean=0.8524 | auc/class_0=0.8524 | loss=0.0590\n",
      "13/300 * Epoch 13 (valid_tr): auc/_mean=0.9464 | auc/class_0=0.9464 | loss=0.0216\n",
      "14/300 * Epoch (train): 100% 13288/13288 [1:02:43<00:00,  3.53it/s, loss=4.208e-04]\n",
      "14/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.20it/s, loss=0.112]    \n",
      "14/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.18it/s, loss=0.487]    \n",
      "14/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.10it/s, loss=0.003]    \n",
      "14/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.16it/s, loss=0.046]    \n",
      "[2020-04-30 01:05:44,225] \n",
      "14/300 * Epoch 14 (_base): lr=6.250e-07 | momentum=0.9000\n",
      "14/300 * Epoch 14 (train): auc/_mean=0.9929 | auc/class_0=0.9929 | loss=0.0077\n",
      "14/300 * Epoch 14 (valid): auc/_mean=0.8957 | auc/class_0=0.8957 | es_auc/_mean=0.8783 | es_auc/class_0=0.8783 | es_loss=0.0532 | it_auc/_mean=0.8545 | it_auc/class_0=0.8545 | it_loss=0.0608 | loss=0.0428 | tr_auc/_mean=0.9457 | tr_auc/class_0=0.9457 | tr_loss=0.0199\n",
      "14/300 * Epoch 14 (valid_es): auc/_mean=0.8783 | auc/class_0=0.8783 | loss=0.0532\n",
      "14/300 * Epoch 14 (valid_it): auc/_mean=0.8545 | auc/class_0=0.8545 | loss=0.0608\n",
      "14/300 * Epoch 14 (valid_tr): auc/_mean=0.9457 | auc/class_0=0.9457 | loss=0.0199\n",
      "15/300 * Epoch (train): 100% 13288/13288 [1:02:36<00:00,  3.54it/s, loss=0.002]    \n",
      "15/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.26it/s, loss=0.117]    \n",
      "15/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.19it/s, loss=0.507]    \n",
      "15/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.18it/s, loss=0.002]    \n",
      "15/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.19it/s, loss=0.048]    \n",
      "[2020-04-30 02:11:51,477] \n",
      "15/300 * Epoch 15 (_base): lr=6.250e-07 | momentum=0.9000\n",
      "15/300 * Epoch 15 (train): auc/_mean=0.9932 | auc/class_0=0.9932 | loss=0.0076\n",
      "15/300 * Epoch 15 (valid): auc/_mean=0.8955 | auc/class_0=0.8955 | es_auc/_mean=0.8781 | es_auc/class_0=0.8781 | es_loss=0.0547 | it_auc/_mean=0.8538 | it_auc/class_0=0.8538 | it_loss=0.0625 | loss=0.0439 | tr_auc/_mean=0.9461 | tr_auc/class_0=0.9461 | tr_loss=0.0202\n",
      "15/300 * Epoch 15 (valid_es): auc/_mean=0.8781 | auc/class_0=0.8781 | loss=0.0547\n",
      "15/300 * Epoch 15 (valid_it): auc/_mean=0.8538 | auc/class_0=0.8538 | loss=0.0625\n",
      "15/300 * Epoch 15 (valid_tr): auc/_mean=0.9461 | auc/class_0=0.9461 | loss=0.0202\n",
      "16/300 * Epoch (train): 100% 13288/13288 [1:02:33<00:00,  3.54it/s, loss=5.392e-04]\n",
      "16/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.27it/s, loss=0.134]    \n",
      "16/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.19it/s, loss=0.570]    \n",
      "16/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.19it/s, loss=0.002]    \n",
      "16/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.20it/s, loss=0.059]    \n",
      "[2020-04-30 03:17:55,357] \n",
      "16/300 * Epoch 16 (_base): lr=6.250e-07 | momentum=0.9000\n",
      "16/300 * Epoch 16 (train): auc/_mean=0.9934 | auc/class_0=0.9934 | loss=0.0075\n",
      "16/300 * Epoch 16 (valid): auc/_mean=0.8951 | auc/class_0=0.8951 | es_auc/_mean=0.8777 | es_auc/class_0=0.8777 | es_loss=0.0549 | it_auc/_mean=0.8528 | it_auc/class_0=0.8528 | it_loss=0.0627 | loss=0.0442 | tr_auc/_mean=0.9460 | tr_auc/class_0=0.9460 | tr_loss=0.0209\n",
      "16/300 * Epoch 16 (valid_es): auc/_mean=0.8777 | auc/class_0=0.8777 | loss=0.0549\n",
      "16/300 * Epoch 16 (valid_it): auc/_mean=0.8528 | auc/class_0=0.8528 | loss=0.0627\n",
      "16/300 * Epoch 16 (valid_tr): auc/_mean=0.9460 | auc/class_0=0.9460 | loss=0.0209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/300 * Epoch (train): 100% 13288/13288 [1:02:31<00:00,  3.54it/s, loss=5.182e-04]\n",
      "17/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.28it/s, loss=0.134]    \n",
      "17/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.20it/s, loss=0.582]    \n",
      "17/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.20it/s, loss=0.003]    \n",
      "17/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.21it/s, loss=0.055]    \n",
      "[2020-04-30 04:23:57,418] \n",
      "17/300 * Epoch 17 (_base): lr=6.250e-07 | momentum=0.9000\n",
      "17/300 * Epoch 17 (train): auc/_mean=0.9936 | auc/class_0=0.9936 | loss=0.0073\n",
      "17/300 * Epoch 17 (valid): auc/_mean=0.8944 | auc/class_0=0.8944 | es_auc/_mean=0.8777 | es_auc/class_0=0.8777 | es_loss=0.0566 | it_auc/_mean=0.8520 | it_auc/class_0=0.8520 | it_loss=0.0652 | loss=0.0457 | tr_auc/_mean=0.9449 | tr_auc/class_0=0.9449 | tr_loss=0.0213\n",
      "17/300 * Epoch 17 (valid_es): auc/_mean=0.8777 | auc/class_0=0.8777 | loss=0.0566\n",
      "17/300 * Epoch 17 (valid_it): auc/_mean=0.8520 | auc/class_0=0.8520 | loss=0.0652\n",
      "17/300 * Epoch 17 (valid_tr): auc/_mean=0.9449 | auc/class_0=0.9449 | loss=0.0213\n",
      "18/300 * Epoch (train): 100% 13288/13288 [1:02:31<00:00,  3.54it/s, loss=2.498e-04]\n",
      "18/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.27it/s, loss=0.141]    \n",
      "18/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.21it/s, loss=0.614]    \n",
      "18/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.20it/s, loss=0.003]    \n",
      "18/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.21it/s, loss=0.058]    \n",
      "[2020-04-30 05:29:59,820] \n",
      "18/300 * Epoch 18 (_base): lr=6.250e-07 | momentum=0.9000\n",
      "18/300 * Epoch 18 (train): auc/_mean=0.9937 | auc/class_0=0.9937 | loss=0.0073\n",
      "18/300 * Epoch 18 (valid): auc/_mean=0.8946 | auc/class_0=0.8946 | es_auc/_mean=0.8773 | es_auc/class_0=0.8773 | es_loss=0.0574 | it_auc/_mean=0.8529 | it_auc/class_0=0.8529 | it_loss=0.0658 | loss=0.0461 | tr_auc/_mean=0.9449 | tr_auc/class_0=0.9449 | tr_loss=0.0212\n",
      "18/300 * Epoch 18 (valid_es): auc/_mean=0.8773 | auc/class_0=0.8773 | loss=0.0574\n",
      "18/300 * Epoch 18 (valid_it): auc/_mean=0.8529 | auc/class_0=0.8529 | loss=0.0658\n",
      "18/300 * Epoch 18 (valid_tr): auc/_mean=0.9449 | auc/class_0=0.9449 | loss=0.0212\n",
      "19/300 * Epoch (train): 100% 13288/13288 [1:02:31<00:00,  3.54it/s, loss=4.038e-04]\n",
      "19/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.28it/s, loss=0.143]    \n",
      "19/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.21it/s, loss=0.580]    \n",
      "19/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.19it/s, loss=0.002]    \n",
      "19/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.20it/s, loss=0.068]    \n",
      "[2020-04-30 06:36:02,687] \n",
      "19/300 * Epoch 19 (_base): lr=1.563e-07 | momentum=0.9000\n",
      "19/300 * Epoch 19 (train): auc/_mean=0.9937 | auc/class_0=0.9937 | loss=0.0073\n",
      "19/300 * Epoch 19 (valid): auc/_mean=0.8944 | auc/class_0=0.8944 | es_auc/_mean=0.8778 | es_auc/class_0=0.8778 | es_loss=0.0557 | it_auc/_mean=0.8524 | it_auc/class_0=0.8524 | it_loss=0.0636 | loss=0.0449 | tr_auc/_mean=0.9440 | tr_auc/class_0=0.9440 | tr_loss=0.0212\n",
      "19/300 * Epoch 19 (valid_es): auc/_mean=0.8778 | auc/class_0=0.8778 | loss=0.0557\n",
      "19/300 * Epoch 19 (valid_it): auc/_mean=0.8524 | auc/class_0=0.8524 | loss=0.0636\n",
      "19/300 * Epoch 19 (valid_tr): auc/_mean=0.9440 | auc/class_0=0.9440 | loss=0.0212\n",
      "20/300 * Epoch (train): 100% 13288/13288 [1:02:32<00:00,  3.54it/s, loss=1.891e-04]\n",
      "20/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.25it/s, loss=0.134]    \n",
      "20/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.18it/s, loss=0.573]    \n",
      "20/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.17it/s, loss=0.003]    \n",
      "20/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.18it/s, loss=0.057]    \n",
      "[2020-04-30 07:42:05,889] \n",
      "20/300 * Epoch 20 (_base): lr=1.563e-07 | momentum=0.9000\n",
      "20/300 * Epoch 20 (train): auc/_mean=0.9941 | auc/class_0=0.9941 | loss=0.0071\n",
      "20/300 * Epoch 20 (valid): auc/_mean=0.8940 | auc/class_0=0.8940 | es_auc/_mean=0.8767 | es_auc/class_0=0.8767 | es_loss=0.0573 | it_auc/_mean=0.8523 | it_auc/class_0=0.8523 | it_loss=0.0649 | loss=0.0457 | tr_auc/_mean=0.9440 | tr_auc/class_0=0.9440 | tr_loss=0.0208\n",
      "20/300 * Epoch 20 (valid_es): auc/_mean=0.8767 | auc/class_0=0.8767 | loss=0.0573\n",
      "20/300 * Epoch 20 (valid_it): auc/_mean=0.8523 | auc/class_0=0.8523 | loss=0.0649\n",
      "20/300 * Epoch 20 (valid_tr): auc/_mean=0.9440 | auc/class_0=0.9440 | loss=0.0208\n",
      "21/300 * Epoch (train): 100% 13288/13288 [1:02:40<00:00,  3.53it/s, loss=7.728e-04]\n",
      "21/300 * Epoch (valid): 100% 1143/1143 [01:33<00:00, 12.25it/s, loss=0.130]    \n",
      "21/300 * Epoch (valid_es): 100% 358/358 [00:29<00:00, 12.17it/s, loss=0.521]    \n",
      "21/300 * Epoch (valid_it): 100% 358/358 [00:29<00:00, 12.17it/s, loss=0.003]    \n",
      "21/300 * Epoch (valid_tr): 100% 429/429 [00:35<00:00, 12.19it/s, loss=0.064]    \n",
      "[2020-04-30 08:48:17,464] \n",
      "21/300 * Epoch 21 (_base): lr=1.563e-07 | momentum=0.9000\n",
      "21/300 * Epoch 21 (train): auc/_mean=0.9940 | auc/class_0=0.9940 | loss=0.0071\n",
      "21/300 * Epoch 21 (valid): auc/_mean=0.8945 | auc/class_0=0.8945 | es_auc/_mean=0.8777 | es_auc/class_0=0.8777 | es_loss=0.0550 | it_auc/_mean=0.8526 | it_auc/class_0=0.8526 | it_loss=0.0622 | loss=0.0441 | tr_auc/_mean=0.9440 | tr_auc/class_0=0.9440 | tr_loss=0.0207\n",
      "21/300 * Epoch 21 (valid_es): auc/_mean=0.8777 | auc/class_0=0.8777 | loss=0.0550\n",
      "21/300 * Epoch 21 (valid_it): auc/_mean=0.8526 | auc/class_0=0.8526 | loss=0.0622\n",
      "21/300 * Epoch 21 (valid_tr): auc/_mean=0.9440 | auc/class_0=0.9440 | loss=0.0207\n",
      "22/300 * Epoch (train):  22% 2957/13288 [13:57<48:55,  3.52it/s, loss=1.932e-04] "
     ]
    }
   ],
   "source": [
    "CHECKPOINT = './checkpoints/from_v4.pth'\n",
    "project = \"jigsaw_v81\"\n",
    "num_epochs = 300\n",
    "\n",
    "group = datetime.now().strftime(\"%m_%d_%Y__%H_%M_%S\")\n",
    "\n",
    "    \n",
    "if SERVER:\n",
    "    group = f'{group}_server'\n",
    "    \n",
    "if STRIDE > 1:\n",
    "    group = f'{group}_str{STRIDE}'\n",
    "\n",
    "variants = {\n",
    "    'learn_both':{\n",
    "        'train': ['bias', 'toxic'],\n",
    "        'valid': 'valid',\n",
    "#         'valid2': 'valid'\n",
    "    },\n",
    "#     'learn_toxic':{\n",
    "#         'train': 'toxic',\n",
    "#         'valid': 'valid',\n",
    "# #         'valid2': 'bias',\n",
    "#     },\n",
    "#     'learn_bias':{\n",
    "#         'train': 'bias',\n",
    "#         'valid': 'valid',\n",
    "#         'valid2': 'toxic',\n",
    "#     },\n",
    "    \n",
    "    \n",
    "}\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "lr = 1e-5#0.0001\n",
    "group += f'_lr{lr}_dr0.35'\n",
    "\n",
    "group = group.replace('.', '')\n",
    "\n",
    "runner = SupervisedRunner(input_key=('features'), input_target_key=('targets'), output_key=('logits'))\n",
    "\n",
    "for experiment in variants.keys():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    logdir = f\"{LOG_PATH}/{project}/{group}/{experiment}\"\n",
    "\n",
    "    model = QuestModel(2)\n",
    "#     checkpoint = torch.load(CHECKPOINT)#, map_location=device)   \n",
    "\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     del checkpoint\n",
    "#     model = model.to(device)\n",
    "\n",
    "    # model runner\n",
    "    if 'valid2' in variants[experiment].keys():\n",
    "        test_var= variants[experiment]['valid2']\n",
    "    else:\n",
    "        test_var=None\n",
    "\n",
    "    loaders = get_loaders(variants[experiment]['train'], \n",
    "                          variants[experiment]['valid'],\n",
    "                          test_var, to_balance=True)\n",
    "            \n",
    "    \n",
    "    t_total = len(loaders['train'])//gradient_accumulation_steps*num_epochs\n",
    "    warmup_proportion = 0.01\n",
    "    num_warmup_steps = t_total * warmup_proportion\n",
    "    \n",
    "#     criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    criterion = FocalLoss(alpha=0.2, gamma=1.5, logits=True, reduce=True)\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "#     criterion = torch.nn.BCELoss()\n",
    "    optimizer = AdamW(model.parameters(), lr = lr)\n",
    "#     scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=t_total) \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.25)\n",
    "    print(f'----------------Experiment: {experiment}')\n",
    "\n",
    "    runner.train(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loaders=loaders,\n",
    "        logdir=logdir,\n",
    "        num_epochs=num_epochs,\n",
    "        verbose=True,\n",
    "        distributed=False if is_jupyter() else True,\n",
    "        callbacks=[\n",
    "            AlchemyLogger(\n",
    "                    token=token, # your Alchemy token\n",
    "                    project=project,\n",
    "                    experiment=experiment,\n",
    "                    group=group,\n",
    "                ),\n",
    "            MyAUCCallback()\n",
    "#             AUCCallback(input_key = 'targets',\n",
    "#                                 output_key = 'logits',\n",
    "#                                 prefix = 'auc',\n",
    "#                                 class_names = None,\n",
    "#                                 num_classes = 2,\n",
    "#                                 activation = 'Sigmoid',)\n",
    "        ],\n",
    "        main_metric='auc/_mean',\n",
    "        minimize_metric=False,\n",
    "#         fp16={\"opt_level\": \"O1\"}\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistilBertTokenizer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jig_env",
   "language": "python",
   "name": "jig_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "086421f7eec44c769f08f5f68fafafdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0e7c81c0da784e04b530236bad005c33": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18b951cd8c1447eaa1e12f972ac37d42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "  9%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9e743ecd26cd4877a539f9bbbd23d114",
       "max": 308,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e34fafda1e234d9c981322beff1068e7",
       "value": 29
      }
     },
     "35645b239e914aee807cfdc234fc5b75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dfdee9b109bc4176a0bc3a26ae38f7a3",
       "placeholder": "​",
       "style": "IPY_MODEL_086421f7eec44c769f08f5f68fafafdc",
       "value": " 29/308 [7:12:46&lt;66:18:40, 855.63s/it]"
      }
     },
     "554271f3ed5d48efa844608a0b72fdac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "685ecca481e3463d83a2465f15f7b33f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_80ae0d356c3e4b1bbe511c5f5a2694b5",
       "max": 4059,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a34804b28be74d7c9972a0a13410ba77",
       "value": 4059
      }
     },
     "7f49d7d685554687bc2d131959058cb3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80ae0d356c3e4b1bbe511c5f5a2694b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "840e74b11eb44a58b1ed0433e924dc82": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9e743ecd26cd4877a539f9bbbd23d114": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a34804b28be74d7c9972a0a13410ba77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "a61c8f56dbd74e29a155fad0d7095f79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_18b951cd8c1447eaa1e12f972ac37d42",
        "IPY_MODEL_35645b239e914aee807cfdc234fc5b75"
       ],
       "layout": "IPY_MODEL_7f49d7d685554687bc2d131959058cb3"
      }
     },
     "c9ab84b07a32426282543b5ff054ddec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_840e74b11eb44a58b1ed0433e924dc82",
       "placeholder": "​",
       "style": "IPY_MODEL_554271f3ed5d48efa844608a0b72fdac",
       "value": " 4059/4059 [14:53&lt;00:00,  4.54it/s]"
      }
     },
     "dfdee9b109bc4176a0bc3a26ae38f7a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e34fafda1e234d9c981322beff1068e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "fd692cff30e343aa8afa007a05d66acc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_685ecca481e3463d83a2465f15f7b33f",
        "IPY_MODEL_c9ab84b07a32426282543b5ff054ddec"
       ],
       "layout": "IPY_MODEL_0e7c81c0da784e04b530236bad005c33"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
