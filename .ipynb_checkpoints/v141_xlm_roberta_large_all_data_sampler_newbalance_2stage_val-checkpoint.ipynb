{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# !pip install ./sacremoses/sacremoses-master/\n",
    "# !pip install ./transformers/transformers-master/\n",
    "\n",
    "STRIDE = 1\n",
    "def is_jupyter():\n",
    "    try:\n",
    "        ipy_str = str(type(get_ipython()))\n",
    "        if 'zmqshell' in ipy_str:\n",
    "            return True\n",
    "        \n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Imports\n",
    "\n",
    "I've added imports that will be used in training too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kb/jig_env/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "/home/kb/jig_env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n",
      "/home/kb/jig_env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n",
      "/home/kb/jig_env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from random import shuffle as shfl\n",
    "from auc import MyAUCCallback\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max.columns', 500)\n",
    "import numpy as np\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] ='3'\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-10.1/lib64'\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from shutil import copyfile\n",
    "from catalyst.dl import SupervisedRunner, AlchemyLogger, CriterionCallback\n",
    "from catalyst.dl.callbacks.metrics import AUCCallback\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler,Dataset\n",
    "batch_size =64\n",
    "token = \"d1dd16f08d518293bcbeddd313b49aa4\"\n",
    "DATA_DIR = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on desktop\n"
     ]
    }
   ],
   "source": [
    "if os.uname()[1] == 'kb-Z370P-D3':\n",
    "    # desktop\n",
    "    LOG_PATH = '/media/ssd/logs/jigsaw'\n",
    "    SERVER = False\n",
    "    print('Working on desktop')\n",
    "elif os.uname()[1] == 'kb-server':\n",
    "    # server\n",
    "    LOG_PATH = '/home/kb/logs/jigsaw'\n",
    "    SERVER = True\n",
    "    print('Working on server')\n",
    "else:\n",
    "    raise Exception('which hostname???')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38476\n",
      "76956\n",
      "115436\n",
      "153902\n",
      "192382\n",
      "230862\n",
      "269347\n"
     ]
    }
   ],
   "source": [
    "df_train = None\n",
    "df_valid = None\n",
    "for lang in ['es', 'tr', 'it', 'ru', 'pt', 'fr']:\n",
    "    df_temp = pd.read_csv(f'/kaggle/input/jigsaw-multilingual-toxic-comment-classification/google/jigsaw-toxic-comment-train-google-{lang}-cleaned.csv')\n",
    "    \n",
    "    df_temp = pd.concat([\n",
    "        df_temp[['comment_text', 'toxic']].query('toxic==1'),\n",
    "        df_temp[['comment_text', 'toxic']].query('toxic==0').sample(n=21378, random_state=0)\n",
    "    ])\n",
    "\n",
    "    df_train_lng, df_valid_lng = train_test_split(df_temp, random_state=12345, \n",
    "                                         stratify=df_temp.toxic.values, test_size=0.1)\n",
    "    \n",
    "    if df_train is None:\n",
    "        df_train = df_train_lng.copy()\n",
    "        df_valid = df_valid_lng.copy()\n",
    "        \n",
    "    else:\n",
    "        df_train = df_train.append(df_train_lng[df_train.columns]).reset_index(drop=True)\n",
    "        df_valid = df_valid.append(df_valid_lng[df_train.columns]).reset_index(drop=True)\n",
    "        \n",
    "    print(len(df_train))\n",
    "    \n",
    "df_temp = pd.read_csv(f'/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv')\n",
    "df_temp = pd.concat([\n",
    "        df_temp[['comment_text', 'toxic']].query('toxic==1'),\n",
    "        df_temp[['comment_text', 'toxic']].query('toxic==0').sample(n=21378, random_state=0)\n",
    "    ])\n",
    "df_train_lng, df_valid_lng = train_test_split(df_temp, random_state=12345, \n",
    "                                         stratify=df_temp.toxic.values, test_size=0.1)\n",
    "df_train = df_train.append(df_train_lng[df_train.columns]).reset_index(drop=True)\n",
    "df_valid = df_valid.append(df_valid_lng[df_train.columns]).reset_index(drop=True)\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_3lng = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
    "sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\\\n",
    "    .rename(columns={'content':'comment_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'toxic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import time\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "#import torch.utils.data as data\n",
    "from torchvision import datasets, models, transforms\n",
    "from transformers import *\n",
    "import random\n",
    "from math import floor, ceil\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "MAX_LEN = 96#192#192#512\n",
    "SEP_TOKEN_ID = 102\n",
    "\n",
    "class QuestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, train_mode=True, labeled=True, train_transforms=None):\n",
    "        \n",
    "        self.train_transforms = train_transforms\n",
    "        self.df = df\n",
    "        if train_mode:\n",
    "            self.labels = df.toxic.values\n",
    "            self.toxic_inds = np.where(self.labels==1)[0]\n",
    "            self.normal_inds = np.where(self.labels==0)[0]            \n",
    "            \n",
    "            \n",
    "            print(f'Here is {len(self.labels)} samples, {len(self.toxic_inds)} samples and {len(self.normal_inds)} samples')\n",
    "            print(f'Class balance is {len(self.toxic_inds)/len(self.labels):.2f}')\n",
    "            \n",
    "        self.train_mode = train_mode\n",
    "        self.labeled = labeled\n",
    "\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')#, \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        token_ids = self.get_token_ids(row)\n",
    "        \n",
    "        if self.labeled:\n",
    "            labels = self.get_label(row)\n",
    "            return {'features': token_ids, 'targets': labels}\n",
    "\n",
    "        else:\n",
    "            return {'features': token_ids}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "        \n",
    "    def get_token_ids(self, row):\n",
    "        \n",
    "        text = row.comment_text\n",
    "        if self.train_transforms:\n",
    "            lang = 'en' if 'lang' not in list(row.keys().values) else row.lang\n",
    "            text, _ = self.train_transforms(data=(row.comment_text, lang))['data']\n",
    "        \n",
    "        if self.train_mode:\n",
    "            token_ids = self.tokenizer.encode(text, max_length=1024)\n",
    "            if len(token_ids) < MAX_LEN:\n",
    "                ids = torch.tensor(token_ids + [0] * (MAX_LEN - len(token_ids)))\n",
    "            else:\n",
    "                ind_beg = random.randint(0, len(token_ids)-MAX_LEN)\n",
    "                ids = torch.tensor(token_ids[ind_beg:ind_beg+MAX_LEN])\n",
    "        else:\n",
    "            token_ids = self.tokenizer.encode(text, max_length=MAX_LEN)\n",
    "            if len(token_ids) < MAX_LEN:\n",
    "                ids = torch.tensor(token_ids + [0] * (MAX_LEN - len(token_ids)))\n",
    "            else:\n",
    "                ids = torch.tensor(token_ids[:MAX_LEN])\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def get_label(self, row):\n",
    "        label = np.round(row[target_column])\n",
    "        return torch.tensor([1-label, label]).float()\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        token_ids = torch.stack([x[0] for x in batch])\n",
    "\n",
    "        if self.labeled:\n",
    "            labels = torch.stack([x[1] for x in batch])\n",
    "            return {'features': token_ids, 'targets': labels}\n",
    "        else:\n",
    "            return {'features': token_ids}\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class QuestModel(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        super(QuestModel, self).__init__()\n",
    "        self.model_name = 'QuestModel'\n",
    "    \n",
    "        self.bert_model = XLMRobertaModel.from_pretrained('xlm-roberta-large')\n",
    "\n",
    "        self.fc = nn.Linear(1024, n_classes)\n",
    "\n",
    "    def forward(self, ids):\n",
    "\n",
    "        layers = self.bert_model(input_ids=ids)\n",
    "        out = F.dropout(layers[0][:, 0, :], p=0.2, training=self.training)\n",
    "        logit = self.fc(out)\n",
    "        return logit\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model = XLMRobertaModel.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, dataset):\n",
    "       \n",
    "        self.toxic_inds = dataset.toxic_inds.copy()\n",
    "        self.normal_inds = dataset.normal_inds.copy()\n",
    "        \n",
    "        self.num_samples = 2*max(len(self.toxic_inds), len(self.normal_inds))\n",
    "        \n",
    "        shfl(self.toxic_inds)\n",
    "        shfl(self.normal_inds)\n",
    "        \n",
    "        self.inds = []\n",
    "        for i in range(max(len(self.toxic_inds), len(self.normal_inds))):\n",
    "            self.inds.append(self.normal_inds[i%len(self.normal_inds)])\n",
    "            self.inds.append(self.toxic_inds[i%len(self.toxic_inds)])\n",
    "\n",
    "    def __iter__(self):\n",
    "        #print ('\\tcalling Sampler:__iter__')\n",
    "        return iter(self.inds)\n",
    "\n",
    "    def __len__(self):\n",
    "        #print ('\\tcalling Sampler:__len__')\n",
    "        return self.num_samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(to_balance=True, shuffle_before=True):\n",
    "    if SERVER:\n",
    "        workers=1\n",
    "    else:\n",
    "        workers = 8    \n",
    "    \n",
    "    \n",
    "\n",
    "    train_dataset = QuestDataset(df_train, train_mode=True)#, train_transforms=get_train_transforms())\n",
    "    valid_dataset = QuestDataset(df_valid, train_mode=False)\n",
    "    \n",
    "       \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        num_workers=workers,\n",
    "        sampler=None,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        num_workers=workers,\n",
    "        batch_size=batch_size,\n",
    "    )    \n",
    "    \n",
    "       \n",
    "    loaders = {}\n",
    "    loaders['train'] = train_loader\n",
    "    loaders['valid'] = valid_loader\n",
    "    \n",
    "    \n",
    "    for i in ['es', 'it', 'tr']:\n",
    "        df = df_valid_3lng\n",
    "        df = df[df['lang']==i]\n",
    "\n",
    "        loaders['valid_'+ i] = DataLoader(\n",
    "            QuestDataset(df, train_mode=False),\n",
    "            num_workers=workers,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer bert_model.embeddings.word_embeddings.weight will train? - False\n",
      "layer bert_model.embeddings.position_embeddings.weight will train? - False\n",
      "layer bert_model.embeddings.token_type_embeddings.weight will train? - False\n",
      "layer bert_model.embeddings.LayerNorm.weight will train? - False\n",
      "layer bert_model.embeddings.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.0.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.0.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.0.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.0.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.0.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.0.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.0.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.0.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.0.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.0.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.0.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.0.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.0.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.0.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.0.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.0.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.1.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.1.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.1.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.1.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.1.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.1.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.1.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.1.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.1.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.1.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.1.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.1.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.1.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.1.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.1.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.1.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.2.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.2.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.2.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.2.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.2.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.2.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.2.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.2.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.2.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.2.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.2.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.2.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.2.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.2.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.2.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.2.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.3.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.3.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.3.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.3.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.3.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.3.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.3.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.3.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.3.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.3.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.3.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.3.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.3.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.3.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.3.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.3.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.4.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.4.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.4.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.4.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.4.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.4.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.4.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.4.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.4.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.4.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.4.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.4.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.4.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.4.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.4.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.4.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.5.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.5.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.5.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.5.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.5.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.5.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.5.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.5.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.5.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.5.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.5.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.5.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.5.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.5.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.5.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.5.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.6.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.6.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.6.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.6.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.6.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.6.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.6.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.6.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.6.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.6.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.6.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.6.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.6.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.6.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.6.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.6.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.7.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.7.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.7.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.7.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.7.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.7.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.7.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.7.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.7.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.7.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.7.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.7.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.7.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.7.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.7.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.7.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.8.attention.self.query.weight will train? - True\n",
      "layer bert_model.encoder.layer.8.attention.self.query.bias will train? - True\n",
      "layer bert_model.encoder.layer.8.attention.self.key.weight will train? - True\n",
      "layer bert_model.encoder.layer.8.attention.self.key.bias will train? - True\n",
      "layer bert_model.encoder.layer.8.attention.self.value.weight will train? - True\n",
      "layer bert_model.encoder.layer.8.attention.self.value.bias will train? - True\n",
      "layer bert_model.encoder.layer.8.attention.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.8.attention.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.8.attention.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.8.attention.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.8.intermediate.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.8.intermediate.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.8.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.8.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.8.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.8.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.9.attention.self.query.weight will train? - True\n",
      "layer bert_model.encoder.layer.9.attention.self.query.bias will train? - True\n",
      "layer bert_model.encoder.layer.9.attention.self.key.weight will train? - True\n",
      "layer bert_model.encoder.layer.9.attention.self.key.bias will train? - True\n",
      "layer bert_model.encoder.layer.9.attention.self.value.weight will train? - True\n",
      "layer bert_model.encoder.layer.9.attention.self.value.bias will train? - True\n",
      "layer bert_model.encoder.layer.9.attention.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.9.attention.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.9.attention.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.9.attention.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.9.intermediate.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.9.intermediate.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.9.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.9.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.9.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.9.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.10.attention.self.query.weight will train? - True\n",
      "layer bert_model.encoder.layer.10.attention.self.query.bias will train? - True\n",
      "layer bert_model.encoder.layer.10.attention.self.key.weight will train? - True\n",
      "layer bert_model.encoder.layer.10.attention.self.key.bias will train? - True\n",
      "layer bert_model.encoder.layer.10.attention.self.value.weight will train? - True\n",
      "layer bert_model.encoder.layer.10.attention.self.value.bias will train? - True\n",
      "layer bert_model.encoder.layer.10.attention.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.10.attention.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.10.attention.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.10.attention.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.10.intermediate.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.10.intermediate.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.10.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.10.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.10.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.10.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.11.attention.self.query.weight will train? - True\n",
      "layer bert_model.encoder.layer.11.attention.self.query.bias will train? - True\n",
      "layer bert_model.encoder.layer.11.attention.self.key.weight will train? - True\n",
      "layer bert_model.encoder.layer.11.attention.self.key.bias will train? - True\n",
      "layer bert_model.encoder.layer.11.attention.self.value.weight will train? - True\n",
      "layer bert_model.encoder.layer.11.attention.self.value.bias will train? - True\n",
      "layer bert_model.encoder.layer.11.attention.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.11.attention.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.11.attention.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.11.attention.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.11.intermediate.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.11.intermediate.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.11.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.11.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.11.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.11.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.12.attention.self.query.weight will train? - True\n",
      "layer bert_model.encoder.layer.12.attention.self.query.bias will train? - True\n",
      "layer bert_model.encoder.layer.12.attention.self.key.weight will train? - True\n",
      "layer bert_model.encoder.layer.12.attention.self.key.bias will train? - True\n",
      "layer bert_model.encoder.layer.12.attention.self.value.weight will train? - True\n",
      "layer bert_model.encoder.layer.12.attention.self.value.bias will train? - True\n",
      "layer bert_model.encoder.layer.12.attention.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.12.attention.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.12.attention.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.12.attention.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.12.intermediate.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.12.intermediate.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.12.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.12.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.12.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.12.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.13.attention.self.query.weight will train? - True\n",
      "layer bert_model.encoder.layer.13.attention.self.query.bias will train? - True\n",
      "layer bert_model.encoder.layer.13.attention.self.key.weight will train? - True\n",
      "layer bert_model.encoder.layer.13.attention.self.key.bias will train? - True\n",
      "layer bert_model.encoder.layer.13.attention.self.value.weight will train? - True\n",
      "layer bert_model.encoder.layer.13.attention.self.value.bias will train? - True\n",
      "layer bert_model.encoder.layer.13.attention.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.13.attention.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.13.attention.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.13.attention.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.13.intermediate.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.13.intermediate.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.13.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.13.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.13.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.13.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.14.attention.self.query.weight will train? - True\n",
      "layer bert_model.encoder.layer.14.attention.self.query.bias will train? - True\n",
      "layer bert_model.encoder.layer.14.attention.self.key.weight will train? - True\n",
      "layer bert_model.encoder.layer.14.attention.self.key.bias will train? - True\n",
      "layer bert_model.encoder.layer.14.attention.self.value.weight will train? - True\n",
      "layer bert_model.encoder.layer.14.attention.self.value.bias will train? - True\n",
      "layer bert_model.encoder.layer.14.attention.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.14.attention.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.14.attention.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.14.attention.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.14.intermediate.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.14.intermediate.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.14.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.14.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.14.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.14.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.15.attention.self.query.weight will train? - True\n",
      "layer bert_model.encoder.layer.15.attention.self.query.bias will train? - True\n",
      "layer bert_model.encoder.layer.15.attention.self.key.weight will train? - True\n",
      "layer bert_model.encoder.layer.15.attention.self.key.bias will train? - True\n",
      "layer bert_model.encoder.layer.15.attention.self.value.weight will train? - True\n",
      "layer bert_model.encoder.layer.15.attention.self.value.bias will train? - True\n",
      "layer bert_model.encoder.layer.15.attention.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.15.attention.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.15.attention.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.15.attention.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.15.intermediate.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.15.intermediate.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.15.output.dense.weight will train? - True\n",
      "layer bert_model.encoder.layer.15.output.dense.bias will train? - True\n",
      "layer bert_model.encoder.layer.15.output.LayerNorm.weight will train? - True\n",
      "layer bert_model.encoder.layer.15.output.LayerNorm.bias will train? - True\n",
      "layer bert_model.encoder.layer.16.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.16.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.16.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.16.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.16.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.16.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.16.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.16.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.16.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.16.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.16.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.16.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.16.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.16.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.16.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.16.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.17.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.17.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.17.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.17.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.17.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.17.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.17.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.17.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.17.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.17.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.17.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.17.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.17.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.17.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.17.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.17.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.18.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.18.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.18.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.18.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.18.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.18.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.18.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.18.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.18.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.18.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.18.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.18.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.18.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.18.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.18.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.18.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.19.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.19.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.19.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.19.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.19.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.19.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.19.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.19.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.19.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.19.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.19.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.19.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.19.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.19.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.19.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.19.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.20.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.20.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.20.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.20.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.20.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.20.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.20.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.20.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.20.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.20.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.20.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.20.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.20.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.20.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.20.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.20.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.21.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.21.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.21.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.21.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.21.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.21.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.21.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.21.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.21.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.21.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.21.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.21.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.21.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.21.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.21.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.21.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.22.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.22.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.22.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.22.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.22.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.22.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.22.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.22.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.22.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.22.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.22.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.22.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.22.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.22.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.22.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.22.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.23.attention.self.query.weight will train? - False\n",
      "layer bert_model.encoder.layer.23.attention.self.query.bias will train? - False\n",
      "layer bert_model.encoder.layer.23.attention.self.key.weight will train? - False\n",
      "layer bert_model.encoder.layer.23.attention.self.key.bias will train? - False\n",
      "layer bert_model.encoder.layer.23.attention.self.value.weight will train? - False\n",
      "layer bert_model.encoder.layer.23.attention.self.value.bias will train? - False\n",
      "layer bert_model.encoder.layer.23.attention.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.23.attention.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.23.attention.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.23.attention.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.encoder.layer.23.intermediate.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.23.intermediate.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.23.output.dense.weight will train? - False\n",
      "layer bert_model.encoder.layer.23.output.dense.bias will train? - False\n",
      "layer bert_model.encoder.layer.23.output.LayerNorm.weight will train? - False\n",
      "layer bert_model.encoder.layer.23.output.LayerNorm.bias will train? - False\n",
      "layer bert_model.pooler.dense.weight will train? - False\n",
      "layer bert_model.pooler.dense.bias will train? - False\n",
      "layer fc.weight will train? - False\n",
      "layer fc.bias will train? - False\n",
      "Here is 269347 samples, 134667 samples and 134680 samples\n",
      "Class balance is 0.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Experiment: simple\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
      "1/300 * Epoch (train):   0% 1/4209 [00:00<54:36,  1.28it/s, loss=0.034]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kb/jig_env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning:\n",
      "\n",
      "size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "\n",
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning:\n",
      "\n",
      "This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/300 * Epoch (train):   2% 65/4209 [00:20<20:49,  3.32it/s, loss=0.020]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "1/300 * Epoch (train):   2% 79/4209 [00:24<20:47,  3.31it/s, loss=0.037]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "1/300 * Epoch (train):  51% 2142/4209 [10:57<10:41,  3.22it/s, loss=0.014]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "1/300 * Epoch (train): 100% 4209/4209 [21:36<00:00,  3.25it/s, loss=0.015]\n",
      "1/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.15it/s, loss=0.019]\n",
      "1/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.70it/s, loss=0.014]\n",
      "1/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.68it/s, loss=0.011]\n",
      "1/300 * Epoch (valid_tr): 100% 47/47 [00:07<00:00,  6.60it/s, loss=0.008]\n",
      "[2020-05-06 23:50:44,849] \n",
      "1/300 * Epoch 1 (_base): lr=3.000e-05 | momentum=0.9000\n",
      "1/300 * Epoch 1 (train): auc/_mean=0.9601 | auc/class_0=0.9601 | loss=0.0188\n",
      "1/300 * Epoch 1 (valid): auc/_mean=0.9610 | auc/class_0=0.9610 | es_auc/_mean=0.9064 | es_auc/class_0=0.9064 | es_loss=0.0273 | it_auc/_mean=0.8801 | it_auc/class_0=0.8801 | it_loss=0.0328 | loss=0.0195 | tr_auc/_mean=0.9771 | tr_auc/class_0=0.9771 | tr_loss=0.0086\n",
      "1/300 * Epoch 1 (valid_es): auc/_mean=0.9064 | auc/class_0=0.9064 | loss=0.0273\n",
      "1/300 * Epoch 1 (valid_it): auc/_mean=0.8801 | auc/class_0=0.8801 | loss=0.0328\n",
      "1/300 * Epoch 1 (valid_tr): auc/_mean=0.9771 | auc/class_0=0.9771 | loss=0.0086\n",
      "2/300 * Epoch (train): 100% 4209/4209 [21:45<00:00,  3.22it/s, loss=0.013]\n",
      "2/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.12it/s, loss=0.016]\n",
      "2/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.68it/s, loss=0.008]\n",
      "2/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.78it/s, loss=0.006]\n",
      "2/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.74it/s, loss=0.009]\n",
      "[2020-05-07 00:14:37,513] \n",
      "2/300 * Epoch 2 (_base): lr=3.000e-05 | momentum=0.9000\n",
      "2/300 * Epoch 2 (train): auc/_mean=0.9734 | auc/class_0=0.9734 | loss=0.0153\n",
      "2/300 * Epoch 2 (valid): auc/_mean=0.9717 | auc/class_0=0.9717 | es_auc/_mean=0.9015 | es_auc/class_0=0.9015 | es_loss=0.0279 | it_auc/_mean=0.8901 | it_auc/class_0=0.8901 | it_loss=0.0304 | loss=0.0167 | tr_auc/_mean=0.9768 | tr_auc/class_0=0.9768 | tr_loss=0.0087\n",
      "2/300 * Epoch 2 (valid_es): auc/_mean=0.9015 | auc/class_0=0.9015 | loss=0.0279\n",
      "2/300 * Epoch 2 (valid_it): auc/_mean=0.8901 | auc/class_0=0.8901 | loss=0.0304\n",
      "2/300 * Epoch 2 (valid_tr): auc/_mean=0.9768 | auc/class_0=0.9768 | loss=0.0087\n",
      "3/300 * Epoch (train):  41% 1712/4209 [08:47<12:52,  3.23it/s, loss=0.026]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "3/300 * Epoch (train): 100% 4209/4209 [21:43<00:00,  3.23it/s, loss=0.006]\n",
      "3/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.17it/s, loss=0.017]\n",
      "3/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.79it/s, loss=0.002]\n",
      "3/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.72it/s, loss=0.008]\n",
      "3/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.76it/s, loss=0.008]\n",
      "[2020-05-07 00:39:03,068] \n",
      "3/300 * Epoch 3 (_base): lr=3.000e-05 | momentum=0.9000\n",
      "3/300 * Epoch 3 (train): auc/_mean=0.9809 | auc/class_0=0.9809 | loss=0.0129\n",
      "3/300 * Epoch 3 (valid): auc/_mean=0.9760 | auc/class_0=0.9760 | es_auc/_mean=0.8989 | es_auc/class_0=0.8989 | es_loss=0.0307 | it_auc/_mean=0.8865 | it_auc/class_0=0.8865 | it_loss=0.0312 | loss=0.0157 | tr_auc/_mean=0.9778 | tr_auc/class_0=0.9778 | tr_loss=0.0090\n",
      "3/300 * Epoch 3 (valid_es): auc/_mean=0.8989 | auc/class_0=0.8989 | loss=0.0307\n",
      "3/300 * Epoch 3 (valid_it): auc/_mean=0.8865 | auc/class_0=0.8865 | loss=0.0312\n",
      "3/300 * Epoch 3 (valid_tr): auc/_mean=0.9778 | auc/class_0=0.9778 | loss=0.0090\n",
      "4/300 * Epoch (train):  73% 3088/4209 [15:49<05:45,  3.24it/s, loss=0.020]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "4/300 * Epoch (train):  80% 3378/4209 [17:19<04:16,  3.24it/s, loss=0.005]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "4/300 * Epoch (train):  87% 3661/4209 [18:46<02:48,  3.25it/s, loss=0.013]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "4/300 * Epoch (train): 100% 4209/4209 [21:35<00:00,  3.25it/s, loss=0.003]\n",
      "4/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.17it/s, loss=0.015]\n",
      "4/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.78it/s, loss=3.129e-04]\n",
      "4/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.78it/s, loss=0.009]\n",
      "4/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.74it/s, loss=0.008]\n",
      "[2020-05-07 01:03:19,223] \n",
      "4/300 * Epoch 4 (_base): lr=3.000e-05 | momentum=0.9000\n",
      "4/300 * Epoch 4 (train): auc/_mean=0.9876 | auc/class_0=0.9876 | loss=0.0103\n",
      "4/300 * Epoch 4 (valid): auc/_mean=0.9776 | auc/class_0=0.9776 | es_auc/_mean=0.8900 | es_auc/class_0=0.8900 | es_loss=0.0338 | it_auc/_mean=0.8805 | it_auc/class_0=0.8805 | it_loss=0.0344 | loss=0.0154 | tr_auc/_mean=0.9720 | tr_auc/class_0=0.9720 | tr_loss=0.0102\n",
      "4/300 * Epoch 4 (valid_es): auc/_mean=0.8900 | auc/class_0=0.8900 | loss=0.0338\n",
      "4/300 * Epoch 4 (valid_it): auc/_mean=0.8805 | auc/class_0=0.8805 | loss=0.0344\n",
      "4/300 * Epoch 4 (valid_tr): auc/_mean=0.9720 | auc/class_0=0.9720 | loss=0.0102\n",
      "5/300 * Epoch (train):  43% 1813/4209 [09:17<12:18,  3.25it/s, loss=0.005]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "5/300 * Epoch (train): 100% 4209/4209 [21:34<00:00,  3.25it/s, loss=0.002]    \n",
      "5/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.16it/s, loss=0.021]\n",
      "5/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.82it/s, loss=1.606e-04]\n",
      "5/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.81it/s, loss=0.010]\n",
      "5/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.77it/s, loss=0.018]\n",
      "[2020-05-07 01:27:34,027] \n",
      "5/300 * Epoch 5 (_base): lr=3.000e-05 | momentum=0.9000\n",
      "5/300 * Epoch 5 (train): auc/_mean=0.9920 | auc/class_0=0.9920 | loss=0.0083\n",
      "5/300 * Epoch 5 (valid): auc/_mean=0.9790 | auc/class_0=0.9790 | es_auc/_mean=0.8893 | es_auc/class_0=0.8893 | es_loss=0.0389 | it_auc/_mean=0.8748 | it_auc/class_0=0.8748 | it_loss=0.0414 | loss=0.0179 | tr_auc/_mean=0.9598 | tr_auc/class_0=0.9598 | tr_loss=0.0156\n",
      "5/300 * Epoch 5 (valid_es): auc/_mean=0.8893 | auc/class_0=0.8893 | loss=0.0389\n",
      "5/300 * Epoch 5 (valid_it): auc/_mean=0.8748 | auc/class_0=0.8748 | loss=0.0414\n",
      "5/300 * Epoch 5 (valid_tr): auc/_mean=0.9598 | auc/class_0=0.9598 | loss=0.0156\n",
      "6/300 * Epoch (train):  81% 3410/4209 [17:29<04:06,  3.25it/s, loss=0.004]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "6/300 * Epoch (train): 100% 4209/4209 [21:35<00:00,  3.25it/s, loss=5.298e-04]\n",
      "6/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.17it/s, loss=0.009]    \n",
      "6/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.81it/s, loss=2.474e-04]\n",
      "6/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.83it/s, loss=0.034]\n",
      "6/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.81it/s, loss=0.021]\n",
      "[2020-05-07 01:51:49,458] \n",
      "6/300 * Epoch 6 (_base): lr=3.000e-05 | momentum=0.9000\n",
      "6/300 * Epoch 6 (train): auc/_mean=0.9946 | auc/class_0=0.9946 | loss=0.0068\n",
      "6/300 * Epoch 6 (valid): auc/_mean=0.9807 | auc/class_0=0.9807 | es_auc/_mean=0.8934 | es_auc/class_0=0.8934 | es_loss=0.0425 | it_auc/_mean=0.8720 | it_auc/class_0=0.8720 | it_loss=0.0476 | loss=0.0184 | tr_auc/_mean=0.9576 | tr_auc/class_0=0.9576 | tr_loss=0.0180\n",
      "6/300 * Epoch 6 (valid_es): auc/_mean=0.8934 | auc/class_0=0.8934 | loss=0.0425\n",
      "6/300 * Epoch 6 (valid_it): auc/_mean=0.8720 | auc/class_0=0.8720 | loss=0.0476\n",
      "6/300 * Epoch 6 (valid_tr): auc/_mean=0.9576 | auc/class_0=0.9576 | loss=0.0180\n",
      "7/300 * Epoch (train):  41% 1711/4209 [08:46<12:50,  3.24it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "7/300 * Epoch (train): 100% 4209/4209 [21:35<00:00,  3.25it/s, loss=5.049e-04]\n",
      "7/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.17it/s, loss=0.009]    \n",
      "7/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.80it/s, loss=6.771e-05]\n",
      "7/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.80it/s, loss=0.021]\n",
      "7/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.74it/s, loss=0.010]    \n",
      "[2020-05-07 02:16:05,038] \n",
      "7/300 * Epoch 7 (_base): lr=7.500e-06 | momentum=0.9000\n",
      "7/300 * Epoch 7 (train): auc/_mean=0.9964 | auc/class_0=0.9964 | loss=0.0055\n",
      "7/300 * Epoch 7 (valid): auc/_mean=0.9812 | auc/class_0=0.9812 | es_auc/_mean=0.8865 | es_auc/class_0=0.8865 | es_loss=0.0484 | it_auc/_mean=0.8694 | it_auc/class_0=0.8694 | it_loss=0.0518 | loss=0.0170 | tr_auc/_mean=0.9548 | tr_auc/class_0=0.9548 | tr_loss=0.0169\n",
      "7/300 * Epoch 7 (valid_es): auc/_mean=0.8865 | auc/class_0=0.8865 | loss=0.0484\n",
      "7/300 * Epoch 7 (valid_it): auc/_mean=0.8694 | auc/class_0=0.8694 | loss=0.0518\n",
      "7/300 * Epoch 7 (valid_tr): auc/_mean=0.9548 | auc/class_0=0.9548 | loss=0.0169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/300 * Epoch (train):  18% 775/4209 [03:58<17:36,  3.25it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "8/300 * Epoch (train):  90% 3775/4209 [19:21<02:13,  3.24it/s, loss=3.001e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "8/300 * Epoch (train): 100% 4209/4209 [21:35<00:00,  3.25it/s, loss=1.491e-04]\n",
      "8/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.16it/s, loss=0.013]    \n",
      "8/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.82it/s, loss=5.521e-06]\n",
      "8/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.79it/s, loss=0.009]\n",
      "8/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.77it/s, loss=0.019]\n",
      "[2020-05-07 02:40:20,935] \n",
      "8/300 * Epoch 8 (_base): lr=7.500e-06 | momentum=0.9000\n",
      "8/300 * Epoch 8 (train): auc/_mean=0.9986 | auc/class_0=0.9986 | loss=0.0033\n",
      "8/300 * Epoch 8 (valid): auc/_mean=0.9838 | auc/class_0=0.9838 | es_auc/_mean=0.8782 | es_auc/class_0=0.8782 | es_loss=0.0563 | it_auc/_mean=0.8706 | it_auc/class_0=0.8706 | it_loss=0.0574 | loss=0.0181 | tr_auc/_mean=0.9391 | tr_auc/class_0=0.9391 | tr_loss=0.0233\n",
      "8/300 * Epoch 8 (valid_es): auc/_mean=0.8782 | auc/class_0=0.8782 | loss=0.0563\n",
      "8/300 * Epoch 8 (valid_it): auc/_mean=0.8706 | auc/class_0=0.8706 | loss=0.0574\n",
      "8/300 * Epoch 8 (valid_tr): auc/_mean=0.9391 | auc/class_0=0.9391 | loss=0.0233\n",
      "9/300 * Epoch (train): 100% 4209/4209 [21:35<00:00,  3.25it/s, loss=8.263e-05]\n",
      "9/300 * Epoch (valid): 100% 468/468 [01:04<00:00,  7.20it/s, loss=0.012]    \n",
      "9/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.89it/s, loss=5.517e-06]\n",
      "9/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.90it/s, loss=0.020]\n",
      "9/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.82it/s, loss=0.022]    \n",
      "[2020-05-07 03:04:35,078] \n",
      "9/300 * Epoch 9 (_base): lr=7.500e-06 | momentum=0.9000\n",
      "9/300 * Epoch 9 (train): auc/_mean=0.9991 | auc/class_0=0.9991 | loss=0.0026\n",
      "9/300 * Epoch 9 (valid): auc/_mean=0.9837 | auc/class_0=0.9837 | es_auc/_mean=0.8717 | es_auc/class_0=0.8717 | es_loss=0.0634 | it_auc/_mean=0.8631 | it_auc/class_0=0.8631 | it_loss=0.0663 | loss=0.0189 | tr_auc/_mean=0.9311 | tr_auc/class_0=0.9311 | tr_loss=0.0240\n",
      "9/300 * Epoch 9 (valid_es): auc/_mean=0.8717 | auc/class_0=0.8717 | loss=0.0634\n",
      "9/300 * Epoch 9 (valid_it): auc/_mean=0.8631 | auc/class_0=0.8631 | loss=0.0663\n",
      "9/300 * Epoch 9 (valid_tr): auc/_mean=0.9311 | auc/class_0=0.9311 | loss=0.0240\n",
      "10/300 * Epoch (train):  28% 1163/4209 [05:57<15:36,  3.25it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "10/300 * Epoch (train): 100% 4209/4209 [21:36<00:00,  3.25it/s, loss=8.695e-05]\n",
      "10/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.16it/s, loss=0.008]    \n",
      "10/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.79it/s, loss=4.425e-06]\n",
      "10/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.78it/s, loss=0.024]\n",
      "10/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.76it/s, loss=0.022]    \n",
      "[2020-05-07 03:28:07,309] \n",
      "10/300 * Epoch 10 (_base): lr=7.500e-06 | momentum=0.9000\n",
      "10/300 * Epoch 10 (train): auc/_mean=0.9994 | auc/class_0=0.9994 | loss=0.0022\n",
      "10/300 * Epoch 10 (valid): auc/_mean=0.9837 | auc/class_0=0.9837 | es_auc/_mean=0.8676 | es_auc/class_0=0.8676 | es_loss=0.0624 | it_auc/_mean=0.8660 | it_auc/class_0=0.8660 | it_loss=0.0656 | loss=0.0193 | tr_auc/_mean=0.9228 | tr_auc/class_0=0.9228 | tr_loss=0.0268\n",
      "10/300 * Epoch 10 (valid_es): auc/_mean=0.8676 | auc/class_0=0.8676 | loss=0.0624\n",
      "10/300 * Epoch 10 (valid_it): auc/_mean=0.8660 | auc/class_0=0.8660 | loss=0.0656\n",
      "10/300 * Epoch 10 (valid_tr): auc/_mean=0.9228 | auc/class_0=0.9228 | loss=0.0268\n",
      "11/300 * Epoch (train):   6% 234/4209 [01:12<20:21,  3.25it/s, loss=2.560e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "11/300 * Epoch (train): 100% 4209/4209 [21:35<00:00,  3.25it/s, loss=1.471e-04]\n",
      "11/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.15it/s, loss=0.015]    \n",
      "11/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.86it/s, loss=3.231e-06]\n",
      "11/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.82it/s, loss=0.009]\n",
      "11/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.72it/s, loss=0.023]\n",
      "[2020-05-07 03:51:39,560] \n",
      "11/300 * Epoch 11 (_base): lr=7.500e-06 | momentum=0.9000\n",
      "11/300 * Epoch 11 (train): auc/_mean=0.9995 | auc/class_0=0.9995 | loss=0.0019\n",
      "11/300 * Epoch 11 (valid): auc/_mean=0.9847 | auc/class_0=0.9847 | es_auc/_mean=0.8660 | es_auc/class_0=0.8660 | es_loss=0.0664 | it_auc/_mean=0.8673 | it_auc/class_0=0.8673 | it_loss=0.0657 | loss=0.0190 | tr_auc/_mean=0.9307 | tr_auc/class_0=0.9307 | tr_loss=0.0247\n",
      "11/300 * Epoch 11 (valid_es): auc/_mean=0.8660 | auc/class_0=0.8660 | loss=0.0664\n",
      "11/300 * Epoch 11 (valid_it): auc/_mean=0.8673 | auc/class_0=0.8673 | loss=0.0657\n",
      "11/300 * Epoch 11 (valid_tr): auc/_mean=0.9307 | auc/class_0=0.9307 | loss=0.0247\n",
      "12/300 * Epoch (train):   2% 86/4209 [00:26<20:58,  3.28it/s, loss=0.001]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n",
      "12/300 * Epoch (train):   6% 234/4209 [01:11<20:17,  3.26it/s, loss=8.208e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "12/300 * Epoch (train): 100% 4209/4209 [21:35<00:00,  3.25it/s, loss=1.695e-04]\n",
      "12/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.18it/s, loss=0.013]    \n",
      "12/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.92it/s, loss=3.696e-06]\n",
      "12/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.88it/s, loss=0.005]\n",
      "12/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.82it/s, loss=0.022]    \n",
      "[2020-05-07 04:15:54,452] \n",
      "12/300 * Epoch 12 (_base): lr=7.500e-06 | momentum=0.9000\n",
      "12/300 * Epoch 12 (train): auc/_mean=0.9997 | auc/class_0=0.9997 | loss=0.0016\n",
      "12/300 * Epoch 12 (valid): auc/_mean=0.9848 | auc/class_0=0.9848 | es_auc/_mean=0.8656 | es_auc/class_0=0.8656 | es_loss=0.0694 | it_auc/_mean=0.8656 | it_auc/class_0=0.8656 | it_loss=0.0730 | loss=0.0199 | tr_auc/_mean=0.9114 | tr_auc/class_0=0.9114 | tr_loss=0.0263\n",
      "12/300 * Epoch 12 (valid_es): auc/_mean=0.8656 | auc/class_0=0.8656 | loss=0.0694\n",
      "12/300 * Epoch 12 (valid_it): auc/_mean=0.8656 | auc/class_0=0.8656 | loss=0.0730\n",
      "12/300 * Epoch 12 (valid_tr): auc/_mean=0.9114 | auc/class_0=0.9114 | loss=0.0263\n",
      "13/300 * Epoch (train):   4% 153/4209 [00:47<20:39,  3.27it/s, loss=7.855e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n",
      "13/300 * Epoch (train):   4% 156/4209 [00:47<20:21,  3.32it/s, loss=0.005]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "13/300 * Epoch (train): 100% 4209/4209 [21:35<00:00,  3.25it/s, loss=1.376e-04]\n",
      "13/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.15it/s, loss=0.013]    \n",
      "13/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.84it/s, loss=2.775e-06]\n",
      "13/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.79it/s, loss=0.018]\n",
      "13/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.75it/s, loss=0.028]    \n",
      "[2020-05-07 04:40:08,565] \n",
      "13/300 * Epoch 13 (_base): lr=1.875e-06 | momentum=0.9000\n",
      "13/300 * Epoch 13 (train): auc/_mean=0.9997 | auc/class_0=0.9997 | loss=0.0015\n",
      "13/300 * Epoch 13 (valid): auc/_mean=0.9837 | auc/class_0=0.9837 | es_auc/_mean=0.8656 | es_auc/class_0=0.8656 | es_loss=0.0679 | it_auc/_mean=0.8592 | it_auc/class_0=0.8592 | it_loss=0.0746 | loss=0.0215 | tr_auc/_mean=0.9264 | tr_auc/class_0=0.9264 | tr_loss=0.0285\n",
      "13/300 * Epoch 13 (valid_es): auc/_mean=0.8656 | auc/class_0=0.8656 | loss=0.0679\n",
      "13/300 * Epoch 13 (valid_it): auc/_mean=0.8592 | auc/class_0=0.8592 | loss=0.0746\n",
      "13/300 * Epoch 13 (valid_tr): auc/_mean=0.9264 | auc/class_0=0.9264 | loss=0.0285\n",
      "14/300 * Epoch (train):   6% 250/4209 [01:17<20:16,  3.25it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n",
      "14/300 * Epoch (train):  34% 1439/4209 [07:23<14:14,  3.24it/s, loss=1.786e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "14/300 * Epoch (train): 100% 4209/4209 [21:36<00:00,  3.25it/s, loss=4.023e-05]\n",
      "14/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.15it/s, loss=0.011]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.78it/s, loss=2.623e-06]\n",
      "14/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.82it/s, loss=0.004]\n",
      "14/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.74it/s, loss=0.027]    \n",
      "[2020-05-07 05:03:40,810] \n",
      "14/300 * Epoch 14 (_base): lr=1.875e-06 | momentum=0.9000\n",
      "14/300 * Epoch 14 (train): auc/_mean=0.9998 | auc/class_0=0.9998 | loss=0.0012\n",
      "14/300 * Epoch 14 (valid): auc/_mean=0.9853 | auc/class_0=0.9853 | es_auc/_mean=0.8681 | es_auc/class_0=0.8681 | es_loss=0.0731 | it_auc/_mean=0.8661 | it_auc/class_0=0.8661 | it_loss=0.0781 | loss=0.0201 | tr_auc/_mean=0.9306 | tr_auc/class_0=0.9306 | tr_loss=0.0261\n",
      "14/300 * Epoch 14 (valid_es): auc/_mean=0.8681 | auc/class_0=0.8681 | loss=0.0731\n",
      "14/300 * Epoch 14 (valid_it): auc/_mean=0.8661 | auc/class_0=0.8661 | loss=0.0781\n",
      "14/300 * Epoch 14 (valid_tr): auc/_mean=0.9306 | auc/class_0=0.9306 | loss=0.0261\n",
      "15/300 * Epoch (train):  40% 1692/4209 [08:40<12:55,  3.25it/s, loss=2.473e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n",
      "15/300 * Epoch (train):  90% 3789/4209 [19:25<02:09,  3.25it/s, loss=4.745e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n",
      "15/300 * Epoch (train):  96% 4044/4209 [20:44<00:50,  3.25it/s, loss=0.001]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "15/300 * Epoch (train): 100% 4209/4209 [21:35<00:00,  3.25it/s, loss=6.441e-05]\n",
      "15/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.20it/s, loss=0.007]    \n",
      "15/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.91it/s, loss=2.756e-06]\n",
      "15/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.89it/s, loss=0.006]\n",
      "15/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.85it/s, loss=0.025]    \n",
      "[2020-05-07 05:27:55,808] \n",
      "15/300 * Epoch 15 (_base): lr=1.875e-06 | momentum=0.9000\n",
      "15/300 * Epoch 15 (train): auc/_mean=0.9998 | auc/class_0=0.9998 | loss=0.0011\n",
      "15/300 * Epoch 15 (valid): auc/_mean=0.9856 | auc/class_0=0.9856 | es_auc/_mean=0.8705 | es_auc/class_0=0.8705 | es_loss=0.0716 | it_auc/_mean=0.8651 | it_auc/class_0=0.8651 | it_loss=0.0775 | loss=0.0197 | tr_auc/_mean=0.9262 | tr_auc/class_0=0.9262 | tr_loss=0.0266\n",
      "15/300 * Epoch 15 (valid_es): auc/_mean=0.8705 | auc/class_0=0.8705 | loss=0.0716\n",
      "15/300 * Epoch 15 (valid_it): auc/_mean=0.8651 | auc/class_0=0.8651 | loss=0.0775\n",
      "15/300 * Epoch 15 (valid_tr): auc/_mean=0.9262 | auc/class_0=0.9262 | loss=0.0266\n",
      "16/300 * Epoch (train): 100% 4209/4209 [21:35<00:00,  3.25it/s, loss=3.471e-04]\n",
      "16/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.14it/s, loss=0.009]    \n",
      "16/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.82it/s, loss=2.739e-06]\n",
      "16/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.82it/s, loss=0.005]\n",
      "16/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.78it/s, loss=0.026]    \n",
      "[2020-05-07 05:52:09,627] \n",
      "16/300 * Epoch 16 (_base): lr=1.875e-06 | momentum=0.9000\n",
      "16/300 * Epoch 16 (train): auc/_mean=0.9998 | auc/class_0=0.9998 | loss=0.0010\n",
      "16/300 * Epoch 16 (valid): auc/_mean=0.9854 | auc/class_0=0.9854 | es_auc/_mean=0.8672 | es_auc/class_0=0.8672 | es_loss=0.0740 | it_auc/_mean=0.8653 | it_auc/class_0=0.8653 | it_loss=0.0792 | loss=0.0203 | tr_auc/_mean=0.9239 | tr_auc/class_0=0.9239 | tr_loss=0.0272\n",
      "16/300 * Epoch 16 (valid_es): auc/_mean=0.8672 | auc/class_0=0.8672 | loss=0.0740\n",
      "16/300 * Epoch 16 (valid_it): auc/_mean=0.8653 | auc/class_0=0.8653 | loss=0.0792\n",
      "16/300 * Epoch 16 (valid_tr): auc/_mean=0.9239 | auc/class_0=0.9239 | loss=0.0272\n",
      "17/300 * Epoch (train):   0% 20/4209 [00:06<21:23,  3.26it/s, loss=3.364e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n",
      "17/300 * Epoch (train):  56% 2358/4209 [12:06<09:30,  3.24it/s, loss=1.391e-05]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n",
      "17/300 * Epoch (train): 100% 4209/4209 [21:36<00:00,  3.25it/s, loss=1.802e-05]\n",
      "17/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.14it/s, loss=0.010]    \n",
      "17/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.80it/s, loss=2.728e-06]\n",
      "17/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.79it/s, loss=0.009]\n",
      "17/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.73it/s, loss=0.022]    \n",
      "[2020-05-07 06:15:42,212] \n",
      "17/300 * Epoch 17 (_base): lr=1.875e-06 | momentum=0.9000\n",
      "17/300 * Epoch 17 (train): auc/_mean=0.9999 | auc/class_0=0.9999 | loss=0.0009\n",
      "17/300 * Epoch 17 (valid): auc/_mean=0.9857 | auc/class_0=0.9857 | es_auc/_mean=0.8667 | es_auc/class_0=0.8667 | es_loss=0.0730 | it_auc/_mean=0.8643 | it_auc/class_0=0.8643 | it_loss=0.0773 | loss=0.0199 | tr_auc/_mean=0.9284 | tr_auc/class_0=0.9284 | tr_loss=0.0271\n",
      "17/300 * Epoch 17 (valid_es): auc/_mean=0.8667 | auc/class_0=0.8667 | loss=0.0730\n",
      "17/300 * Epoch 17 (valid_it): auc/_mean=0.8643 | auc/class_0=0.8643 | loss=0.0773\n",
      "17/300 * Epoch 17 (valid_tr): auc/_mean=0.9284 | auc/class_0=0.9284 | loss=0.0271\n",
      "18/300 * Epoch (train):   4% 168/4209 [00:51<20:35,  3.27it/s, loss=1.484e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n",
      "18/300 * Epoch (train):  59% 2480/4209 [12:42<08:53,  3.24it/s, loss=3.269e-05]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n",
      "18/300 * Epoch (train): 100% 4209/4209 [21:35<00:00,  3.25it/s, loss=2.052e-05]\n",
      "18/300 * Epoch (valid): 100% 468/468 [01:04<00:00,  7.20it/s, loss=0.008]    \n",
      "18/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.88it/s, loss=2.448e-06]\n",
      "18/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.88it/s, loss=0.006]\n",
      "18/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.83it/s, loss=0.022]\n",
      "[2020-05-07 06:39:55,852] \n",
      "18/300 * Epoch 18 (_base): lr=1.875e-06 | momentum=0.9000\n",
      "18/300 * Epoch 18 (train): auc/_mean=0.9999 | auc/class_0=0.9999 | loss=0.0008\n",
      "18/300 * Epoch 18 (valid): auc/_mean=0.9856 | auc/class_0=0.9856 | es_auc/_mean=0.8671 | es_auc/class_0=0.8671 | es_loss=0.0737 | it_auc/_mean=0.8644 | it_auc/class_0=0.8644 | it_loss=0.0790 | loss=0.0203 | tr_auc/_mean=0.9186 | tr_auc/class_0=0.9186 | tr_loss=0.0283\n",
      "18/300 * Epoch 18 (valid_es): auc/_mean=0.8671 | auc/class_0=0.8671 | loss=0.0737\n",
      "18/300 * Epoch 18 (valid_it): auc/_mean=0.8644 | auc/class_0=0.8644 | loss=0.0790\n",
      "18/300 * Epoch 18 (valid_tr): auc/_mean=0.9186 | auc/class_0=0.9186 | loss=0.0283\n",
      "19/300 * Epoch (train):   1% 41/4209 [00:12<21:09,  3.28it/s, loss=7.018e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "19/300 * Epoch (train):  76% 3200/4209 [16:25<05:11,  3.24it/s, loss=3.059e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "19/300 * Epoch (train): 100% 4209/4209 [21:36<00:00,  3.25it/s, loss=1.519e-05]\n",
      "19/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.15it/s, loss=0.010]    \n",
      "19/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.77it/s, loss=2.568e-06]\n",
      "19/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.80it/s, loss=0.006]\n",
      "19/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.74it/s, loss=0.025]\n",
      "[2020-05-07 07:03:28,068] \n",
      "19/300 * Epoch 19 (_base): lr=4.688e-07 | momentum=0.9000\n",
      "19/300 * Epoch 19 (train): auc/_mean=0.9999 | auc/class_0=0.9999 | loss=0.0008\n",
      "19/300 * Epoch 19 (valid): auc/_mean=0.9854 | auc/class_0=0.9854 | es_auc/_mean=0.8672 | es_auc/class_0=0.8672 | es_loss=0.0780 | it_auc/_mean=0.8640 | it_auc/class_0=0.8640 | it_loss=0.0810 | loss=0.0210 | tr_auc/_mean=0.9192 | tr_auc/class_0=0.9192 | tr_loss=0.0288\n",
      "19/300 * Epoch 19 (valid_es): auc/_mean=0.8672 | auc/class_0=0.8672 | loss=0.0780\n",
      "19/300 * Epoch 19 (valid_it): auc/_mean=0.8640 | auc/class_0=0.8640 | loss=0.0810\n",
      "19/300 * Epoch 19 (valid_tr): auc/_mean=0.9192 | auc/class_0=0.9192 | loss=0.0288\n",
      "20/300 * Epoch (train):  71% 3006/4209 [15:25<06:10,  3.25it/s, loss=2.743e-05]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n",
      "20/300 * Epoch (train): 100% 4209/4209 [21:35<00:00,  3.25it/s, loss=4.696e-05]\n",
      "20/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.15it/s, loss=0.009]    \n",
      "20/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.80it/s, loss=2.393e-06]\n",
      "20/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.80it/s, loss=0.009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.72it/s, loss=0.025]\n",
      "[2020-05-07 07:26:59,035] \n",
      "20/300 * Epoch 20 (_base): lr=4.688e-07 | momentum=0.9000\n",
      "20/300 * Epoch 20 (train): auc/_mean=0.9999 | auc/class_0=0.9999 | loss=0.0008\n",
      "20/300 * Epoch 20 (valid): auc/_mean=0.9854 | auc/class_0=0.9854 | es_auc/_mean=0.8665 | es_auc/class_0=0.8665 | es_loss=0.0793 | it_auc/_mean=0.8634 | it_auc/class_0=0.8634 | it_loss=0.0822 | loss=0.0211 | tr_auc/_mean=0.9178 | tr_auc/class_0=0.9178 | tr_loss=0.0293\n",
      "20/300 * Epoch 20 (valid_es): auc/_mean=0.8665 | auc/class_0=0.8665 | loss=0.0793\n",
      "20/300 * Epoch 20 (valid_it): auc/_mean=0.8634 | auc/class_0=0.8634 | loss=0.0822\n",
      "20/300 * Epoch 20 (valid_tr): auc/_mean=0.9178 | auc/class_0=0.9178 | loss=0.0293\n",
      "21/300 * Epoch (train):   4% 176/4209 [00:54<20:35,  3.27it/s, loss=1.903e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "21/300 * Epoch (train): 100% 4209/4209 [21:37<00:00,  3.24it/s, loss=9.814e-05]\n",
      "21/300 * Epoch (valid): 100% 468/468 [01:05<00:00,  7.17it/s, loss=0.008]    \n",
      "21/300 * Epoch (valid_es): 100% 40/40 [00:05<00:00,  6.89it/s, loss=2.416e-06]\n",
      "21/300 * Epoch (valid_it): 100% 40/40 [00:05<00:00,  6.89it/s, loss=0.010]\n",
      "21/300 * Epoch (valid_tr): 100% 47/47 [00:06<00:00,  6.82it/s, loss=0.026]\n",
      "[2020-05-07 07:50:40,082] \n",
      "21/300 * Epoch 21 (_base): lr=4.688e-07 | momentum=0.9000\n",
      "21/300 * Epoch 21 (train): auc/_mean=0.9999 | auc/class_0=0.9999 | loss=0.0008\n",
      "21/300 * Epoch 21 (valid): auc/_mean=0.9855 | auc/class_0=0.9855 | es_auc/_mean=0.8675 | es_auc/class_0=0.8675 | es_loss=0.0787 | it_auc/_mean=0.8642 | it_auc/class_0=0.8642 | it_loss=0.0813 | loss=0.0209 | tr_auc/_mean=0.9203 | tr_auc/class_0=0.9203 | tr_loss=0.0286\n",
      "21/300 * Epoch 21 (valid_es): auc/_mean=0.8675 | auc/class_0=0.8675 | loss=0.0787\n",
      "21/300 * Epoch 21 (valid_it): auc/_mean=0.8642 | auc/class_0=0.8642 | loss=0.0813\n",
      "21/300 * Epoch 21 (valid_tr): auc/_mean=0.9203 | auc/class_0=0.9203 | loss=0.0286\n",
      "22/300 * Epoch (train):   1% 28/4209 [00:08<21:13,  3.28it/s, loss=3.252e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n",
      "22/300 * Epoch (train):  17% 700/4209 [03:35<18:05,  3.23it/s, loss=8.638e-05]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "22/300 * Epoch (train):  34% 1423/4209 [07:18<14:17,  3.25it/s, loss=6.668e-05]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "22/300 * Epoch (train):  53% 2234/4209 [11:28<10:14,  3.22it/s, loss=0.006]    "
     ]
    }
   ],
   "source": [
    "def freeze_until(net, param_name):\n",
    "    found_name = False\n",
    "    for name, params in net.named_parameters():\n",
    "        if name == param_name:\n",
    "            found_name = True\n",
    "        params.requires_grad = found_name\n",
    "\n",
    "def train_from_to(net, param_name_from, param_name_to):\n",
    "    \n",
    "    found_1_name = False\n",
    "    found_2_name = False\n",
    "    train_params = False\n",
    "    \n",
    "    for name, params in net.named_parameters():\n",
    "        \n",
    "        if not found_1_name:\n",
    "            if name == param_name_from:\n",
    "                train_params = True\n",
    "                found_1_name = True\n",
    "            \n",
    "        elif not found_2_name:\n",
    "            if name == param_name_to:\n",
    "                train_params = False\n",
    "                found_2_name = True\n",
    "#         print(f'layer {name} will train? - {train_params}')\n",
    "        params.requires_grad = train_params\n",
    "        \n",
    "# param_name = 'bert_model.encoder.layer.10.attention.self.query.weight'\n",
    "param_name_from = 'bert_model.encoder.layer.8.attention.self.query.weight'\n",
    "param_name_to = 'bert_model.encoder.layer.16.attention.self.query.weight'\n",
    "\n",
    "CHECKPOINT = '/media/ssd/logs/jigsaw/js_v13_engbias/05062020_214819_lr3e-05/simple/checkpoints/best.pth'\n",
    "\n",
    "project = \"js_v13_engbias\"\n",
    "num_epochs = 300\n",
    "\n",
    "group = datetime.now().strftime(\"%m%d%Y_%H%M%S\")\n",
    "\n",
    "    \n",
    "if SERVER:\n",
    "    group = f'{group}_srv'\n",
    "    \n",
    "if STRIDE > 1:\n",
    "    group = f'{group}_str{STRIDE}'\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "lr = 3e-5#1e-5#0.0001\n",
    "group += f'_lr{lr}_2stage'\n",
    "\n",
    "group = group.replace('.', '')\n",
    "\n",
    "runner = SupervisedRunner(input_key=('features'), input_target_key=('targets'), output_key=('logits'))\n",
    "\n",
    "experiment = 'simple'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "logdir = f\"{LOG_PATH}/{project}/{group}/{experiment}\"\n",
    "\n",
    "model = QuestModel(2)\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT)#, map_location=device)   \n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "del checkpoint\n",
    "\n",
    "# freeze_until(model, param_name)\n",
    "train_from_to(model, param_name_from, param_name_to)\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "loaders = get_loaders(to_balance=True)\n",
    "\n",
    "\n",
    "t_total = len(loaders['train'])//gradient_accumulation_steps*num_epochs\n",
    "warmup_proportion = 0.01\n",
    "num_warmup_steps = t_total * warmup_proportion\n",
    "\n",
    "#     criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion = FocalLoss(alpha=0.2, gamma=1.5, logits=True, reduce=True)\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "#     criterion = torch.nn.BCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr = lr)\n",
    "#    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=t_total) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.25)\n",
    "print(f'----------------Experiment: {experiment}')\n",
    "\n",
    "runner.train(\n",
    "    fp16=dict(opt_level=\"O1\") ,\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    verbose=True,\n",
    "    distributed=False if is_jupyter() else True,\n",
    "    callbacks=[\n",
    "        AlchemyLogger(\n",
    "                token=token, # your Alchemy token\n",
    "                project=project,\n",
    "                experiment=experiment,\n",
    "                group=group,\n",
    "            ),\n",
    "        MyAUCCallback()\n",
    "\n",
    "    ],\n",
    "    main_metric='auc/_mean',\n",
    "    minimize_metric=False,\n",
    "    \n",
    "    #fp16={\"opt_level\": \"O1\"}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, params in model.named_parameters():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in df_train.iterrows():\n",
    "#     print(list(row.keys().values))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = '/media/ssd/logs/jigsaw/js_v935_xlmrbtlg_10fr_pseudo/05_04_2020__14_16_58_lr1e-06_pseudo_train/simple/checkpoints/best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuestModel(2)\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT)#, map_location=device)   \n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\\\n",
    "    .rename(columns={'content':'comment_text'})\n",
    "ds = QuestDataset(df_test, train_mode=False, labeled=False)\n",
    "loader = DataLoader(\n",
    "        ds,\n",
    "        num_workers=8,\n",
    "        batch_size=batch_size,\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.zeros( len(ds), 2).to(device)\n",
    "\n",
    "j = 0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(loader):\n",
    "        x = data['features'].to(device)\n",
    "        thish_batch_size = len(x)            \n",
    "        \n",
    "        \n",
    "        y_pred[j:j+thish_batch_size] = model(x)\n",
    "            \n",
    "        j += thish_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import sigmoid\n",
    "y_pred_sigmoid = sigmoid(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sigmoid_one_np = y_pred_sigmoid.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['toxic'] = y_pred_sigmoid_one_np[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jig_env",
   "language": "python",
   "name": "jig_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "086421f7eec44c769f08f5f68fafafdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0e7c81c0da784e04b530236bad005c33": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18b951cd8c1447eaa1e12f972ac37d42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "  9%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9e743ecd26cd4877a539f9bbbd23d114",
       "max": 308,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e34fafda1e234d9c981322beff1068e7",
       "value": 29
      }
     },
     "35645b239e914aee807cfdc234fc5b75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dfdee9b109bc4176a0bc3a26ae38f7a3",
       "placeholder": "​",
       "style": "IPY_MODEL_086421f7eec44c769f08f5f68fafafdc",
       "value": " 29/308 [7:12:46&lt;66:18:40, 855.63s/it]"
      }
     },
     "554271f3ed5d48efa844608a0b72fdac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "685ecca481e3463d83a2465f15f7b33f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_80ae0d356c3e4b1bbe511c5f5a2694b5",
       "max": 4059,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a34804b28be74d7c9972a0a13410ba77",
       "value": 4059
      }
     },
     "7f49d7d685554687bc2d131959058cb3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80ae0d356c3e4b1bbe511c5f5a2694b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "840e74b11eb44a58b1ed0433e924dc82": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9e743ecd26cd4877a539f9bbbd23d114": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a34804b28be74d7c9972a0a13410ba77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "a61c8f56dbd74e29a155fad0d7095f79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_18b951cd8c1447eaa1e12f972ac37d42",
        "IPY_MODEL_35645b239e914aee807cfdc234fc5b75"
       ],
       "layout": "IPY_MODEL_7f49d7d685554687bc2d131959058cb3"
      }
     },
     "c9ab84b07a32426282543b5ff054ddec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_840e74b11eb44a58b1ed0433e924dc82",
       "placeholder": "​",
       "style": "IPY_MODEL_554271f3ed5d48efa844608a0b72fdac",
       "value": " 4059/4059 [14:53&lt;00:00,  4.54it/s]"
      }
     },
     "dfdee9b109bc4176a0bc3a26ae38f7a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e34fafda1e234d9c981322beff1068e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "fd692cff30e343aa8afa007a05d66acc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_685ecca481e3463d83a2465f15f7b33f",
        "IPY_MODEL_c9ab84b07a32426282543b5ff054ddec"
       ],
       "layout": "IPY_MODEL_0e7c81c0da784e04b530236bad005c33"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
