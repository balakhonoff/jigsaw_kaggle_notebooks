{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pytorch BERT baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, I convert https://www.kaggle.com/akensert/bert-base-tf2-0-minimalistic into pytorch version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please upvote the kernel if you find it helpful**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are not allowed to use internet I've created required datasets and commands to setup Hugging Face Transformers setup in offline mode. You can find the required github codebases in the datasets.\n",
    "\n",
    "* sacremoses dependency - https://www.kaggle.com/axel81/sacremoses\n",
    "* transformers - https://www.kaggle.com/axel81/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from albumentations.core.transforms_interface import DualTransform, BasicTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# !pip install ./sacremoses/sacremoses-master/\n",
    "# !pip install ./transformers/transformers-master/\n",
    "\n",
    "STRIDE = 1\n",
    "def is_jupyter():\n",
    "    try:\n",
    "        ipy_str = str(type(get_ipython()))\n",
    "        if 'zmqshell' in ipy_str:\n",
    "            return True\n",
    "        \n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Imports\n",
    "\n",
    "I've added imports that will be used in training too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kb/jig_env/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "/home/kb/jig_env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n",
      "/home/kb/jig_env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from random import shuffle as shfl\n",
    "from auc import MyAUCCallback\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max.columns', 500)\n",
    "import numpy as np\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] ='3'\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-10.1/lib64'\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from shutil import copyfile\n",
    "from catalyst.dl import SupervisedRunner, AlchemyLogger, CriterionCallback\n",
    "from catalyst.dl.callbacks.metrics import AUCCallback\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler,Dataset\n",
    "batch_size =32\n",
    "token = \"d1dd16f08d518293bcbeddd313b49aa4\"\n",
    "DATA_DIR = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on desktop\n"
     ]
    }
   ],
   "source": [
    "if os.uname()[1] == 'kb-Z370P-D3':\n",
    "    # desktop\n",
    "    LOG_PATH = '/media/ssd/logs/jigsaw'\n",
    "    SERVER = False\n",
    "    print('Working on desktop')\n",
    "elif os.uname()[1] == 'kb-server':\n",
    "    # server\n",
    "    LOG_PATH = '/home/kb/logs/jigsaw'\n",
    "    SERVER = True\n",
    "    print('Working on server')\n",
    "else:\n",
    "    raise Exception('which hostname???')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n",
    "train2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\n",
    "train2.toxic = train2.toxic.round().astype(int)\n",
    "\n",
    "df_valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
    "sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train1 with a subset of train2\n",
    "df_train = pd.concat([\n",
    "    train1[['comment_text', 'toxic']],\n",
    "    train2[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['lang']='en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.173887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     toxic\n",
       "0   0  0.000060\n",
       "1   1  0.000020\n",
       "2   2  0.173887\n",
       "3   3  0.000015\n",
       "4   4  0.000019"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_pseudo = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/pseudo_labelling_09459.csv')\n",
    "df_pseudo = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/pseudo_labelling_09393.csv')\n",
    "df_pseudo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.020938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.269427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.008064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.007442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     toxic\n",
       "0   0  0.004235\n",
       "1   1  0.020938\n",
       "2   2  0.269427\n",
       "3   3  0.008064\n",
       "4   4  0.007442"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pseudo2 = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/pseudo_labelling_09459.csv')\n",
    "df_pseudo2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f94e231f1d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_pseudo.toxic.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9504315d90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAUE0lEQVR4nO3ccayd9X3f8fcndkgZSQoJ6RUCr2aKu80JKqFX4CrTdhtWMFSKqcYiEA1OyuKqgandrKmk+4MsBIloItFAJK0zLExFY1jazFbqzLMoR6jTTHAKxRjGuCVOsUdgxYb0BpXM2Xd/nJ+zI/te3+Nzj8/1vff9ko7uc77P7/md3/f62h8/z3nuSVUhSVra3jbfC5AkzT/DQJJkGEiSDANJEoaBJAlYPt8LGNS5555bK1euHOjYH/7wh5x11lnDXdBpzp6XBnteGubS83e+852/rqr3HVtfsGGwcuVK9uzZM9CxnU6HiYmJ4S7oNGfPS4M9Lw1z6TnJ96are5lIkmQYSJIMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEks4N9Anou9B9/gE7f+yXH1/Xf+yjysRpLmn2cGkiTDQJJkGEiSMAwkSRgGkiQMA0kShoEkCcNAkkQfYZDkp5J8O8lfJNmX5N+1+oVJHk8ymeShJGe0+jva88m2f2XPXJ9p9eeTXNlTX9tqk0luHX6bkqQT6efM4C3gI1X188DFwNoka4AvAF+qqvcDh4Gb2vibgMOt/qU2jiSrgeuADwBrgS8nWZZkGXAvcBWwGri+jZUkjcisYVBdU+3p29ujgI8AX2/1LcA1bXtde07bf3mStPrWqnqrqr4LTAKXtsdkVb1YVT8CtraxkqQR6euzidr/3r8DvJ/u/+L/Eni9qo60IQeA89v2+cBLAFV1JMkbwHtbfXfPtL3HvHRM/bIZ1rEB2AAwNjZGp9PpZ/nHGTsTNl505Lj6oPMtBFNTU4u6v+nY89Jgz8PRVxhU1Y+Bi5OcDXwD+AdDXUWfqmoTsAlgfHy8JiYmBprnnge3cdfe41vff8Ng8y0EnU6HQb9fC5U9Lw32PBwndTdRVb0OPAr8InB2kqP/ol4AHGzbB4EVAG3/TwOv9daPOWamuiRpRPq5m+h97YyAJGcCvww8RzcUrm3D1gPb2vb29py2/0+rqlr9una30YXAKuDbwBPAqnZ30hl032TePozmJEn96ecy0XnAlva+wduAh6vqm0meBbYm+TzwJHBfG38f8AdJJoFDdP9xp6r2JXkYeBY4AtzcLj+R5BZgJ7AM2FxV+4bWoSRpVrOGQVU9DXxomvqLdO8EOrb+t8A/n2GuO4A7pqnvAHb0sV5J0ingbyBLkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiT6CIMkK5I8muTZJPuS/FarfzbJwSRPtcfVPcd8JslkkueTXNlTX9tqk0lu7alfmOTxVn8oyRnDblSSNLN+zgyOABurajWwBrg5yeq270tVdXF77ABo+64DPgCsBb6cZFmSZcC9wFXAauD6nnm+0OZ6P3AYuGlI/UmS+jBrGFTVy1X15237b4DngPNPcMg6YGtVvVVV3wUmgUvbY7KqXqyqHwFbgXVJAnwE+Ho7fgtwzaANSZJO3vKTGZxkJfAh4HHgw8AtSW4E9tA9ezhMNyh29xx2gP8fHi8dU78MeC/welUdmWb8sa+/AdgAMDY2RqfTOZnl/8TYmbDxoiPH1QedbyGYmppa1P1Nx56XBnsejr7DIMk7gT8CfruqfpDkK8DtQLWvdwG/PtTVHaOqNgGbAMbHx2tiYmKgee55cBt37T2+9f03DDbfQtDpdBj0+7VQ2fPSYM/D0VcYJHk73SB4sKr+GKCqXunZ/1Xgm+3pQWBFz+EXtBoz1F8Dzk6yvJ0d9I6XJI1AP3cTBbgPeK6qvthTP69n2K8Cz7Tt7cB1Sd6R5EJgFfBt4AlgVbtz6Ay6bzJvr6oCHgWubcevB7bNrS1J0sno58zgw8DHgb1Jnmq136V7N9DFdC8T7Qd+A6Cq9iV5GHiW7p1IN1fVjwGS3ALsBJYBm6tqX5vvd4CtST4PPEk3fCRJIzJrGFTVnwGZZteOExxzB3DHNPUd0x1XVS/SvdtIkjQP/A1kSZJhIEkyDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJoo8wSLIiyaNJnk2yL8lvtfp7kuxK8kL7ek6rJ8ndSSaTPJ3kkp651rfxLyRZ31P/hSR72zF3J8mpaFaSNL1+zgyOABurajWwBrg5yWrgVuCRqloFPNKeA1wFrGqPDcBXoBsewG3AZcClwG1HA6SN+VTPcWvn3pokqV+zhkFVvVxVf962/wZ4DjgfWAdsacO2ANe07XXAA9W1Gzg7yXnAlcCuqjpUVYeBXcDatu/dVbW7qgp4oGcuSdIILD+ZwUlWAh8CHgfGqurltuv7wFjbPh94qeewA612ovqBaerTvf4GumcbjI2N0el0Tmb5PzF2Jmy86Mhx9UHnWwimpqYWdX/TseelwZ6Ho+8wSPJO4I+A366qH/Re1q+qSlJDXdk0qmoTsAlgfHy8JiYmBprnnge3cdfe41vff8Ng8y0EnU6HQb9fC5U9Lw32PBx93U2U5O10g+DBqvrjVn6lXeKhfX211Q8CK3oOv6DVTlS/YJq6JGlE+rmbKMB9wHNV9cWeXduBo3cErQe29dRvbHcVrQHeaJeTdgJXJDmnvXF8BbCz7ftBkjXttW7smUuSNAL9XCb6MPBxYG+Sp1rtd4E7gYeT3AR8D/hY27cDuBqYBN4EPglQVYeS3A480cZ9rqoOte1PA/cDZwLfag9J0ojMGgZV9WfATPf9Xz7N+AJunmGuzcDmaep7gA/OthZJ0qnhbyBLkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiT6CIMkm5O8muSZntpnkxxM8lR7XN2z7zNJJpM8n+TKnvraVptMcmtP/cIkj7f6Q0nOGGaDkqTZ9XNmcD+wdpr6l6rq4vbYAZBkNXAd8IF2zJeTLEuyDLgXuApYDVzfxgJ8oc31fuAwcNNcGpIknbxZw6CqHgMO9TnfOmBrVb1VVd8FJoFL22Oyql6sqh8BW4F1SQJ8BPh6O34LcM1J9iBJmqPlczj2liQ3AnuAjVV1GDgf2N0z5kCrAbx0TP0y4L3A61V1ZJrxx0myAdgAMDY2RqfTGWjhY2fCxouOHFcfdL6FYGpqalH3Nx17XhrseTgGDYOvALcD1b7eBfz6sBY1k6raBGwCGB8fr4mJiYHmuefBbdy19/jW998w2HwLQafTYdDv10Jlz0uDPQ/HQGFQVa8c3U7yVeCb7elBYEXP0AtajRnqrwFnJ1nezg56x0uSRmSgW0uTnNfz9FeBo3cabQeuS/KOJBcCq4BvA08Aq9qdQ2fQfZN5e1UV8ChwbTt+PbBtkDVJkgY365lBkq8BE8C5SQ4AtwETSS6me5loP/AbAFW1L8nDwLPAEeDmqvpxm+cWYCewDNhcVfvaS/wOsDXJ54EngfuG1p0kqS+zhkFVXT9NecZ/sKvqDuCOaeo7gB3T1F+ke7eRJGme+BvIkiTDQJJkGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CSRB9hkGRzkleTPNNTe0+SXUleaF/PafUkuTvJZJKnk1zSc8z6Nv6FJOt76r+QZG875u4kGXaTkqQT6+fM4H5g7TG1W4FHqmoV8Eh7DnAVsKo9NgBfgW54ALcBlwGXArcdDZA25lM9xx37WpKkU2zWMKiqx4BDx5TXAVva9hbgmp76A9W1Gzg7yXnAlcCuqjpUVYeBXcDatu/dVbW7qgp4oGcuSdKILB/wuLGqerltfx8Ya9vnAy/1jDvQaieqH5imPq0kG+iecTA2Nkan0xls8WfCxouOHFcfdL6FYGpqalH3Nx17XhrseTgGDYOfqKpKUsNYTB+vtQnYBDA+Pl4TExMDzXPPg9u4a+/xre+/YbD5FoJOp8Og36+Fyp6XBnsejkHvJnqlXeKhfX211Q8CK3rGXdBqJ6pfME1dkjRCg4bBduDoHUHrgW099RvbXUVrgDfa5aSdwBVJzmlvHF8B7Gz7fpBkTbuL6MaeuSRJIzLrZaIkXwMmgHOTHKB7V9CdwMNJbgK+B3ysDd8BXA1MAm8CnwSoqkNJbgeeaOM+V1VH35T+NN07ls4EvtUekqQRmjUMqur6GXZdPs3YAm6eYZ7NwOZp6nuAD862DknSqeNvIEuSDANJkmEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJOYYBkn2J9mb5Kkke1rtPUl2JXmhfT2n1ZPk7iSTSZ5OcknPPOvb+BeSrJ9bS5KkkzWMM4NfqqqLq2q8Pb8VeKSqVgGPtOcAVwGr2mMD8BXohgdwG3AZcClw29EAkSSNxqm4TLQO2NK2twDX9NQfqK7dwNlJzgOuBHZV1aGqOgzsAtaegnVJkmawfI7HF/BfkxTw+1W1CRirqpfb/u8DY237fOClnmMPtNpM9eMk2UD3rIKxsTE6nc5Aix47EzZedOS4+qDzLQRTU1OLur/p2PPSYM/DMdcw+EdVdTDJzwC7kvyP3p1VVS0ohqKFzSaA8fHxmpiYGGieex7cxl17j299/w2DzbcQdDodBv1+LVT2vDTY83DM6TJRVR1sX18FvkH3mv8r7fIP7eurbfhBYEXP4Re02kx1SdKIDBwGSc5K8q6j28AVwDPAduDoHUHrgW1teztwY7uraA3wRructBO4Isk57Y3jK1pNkjQic7lMNAZ8I8nRef6wqv5LkieAh5PcBHwP+FgbvwO4GpgE3gQ+CVBVh5LcDjzRxn2uqg7NYV2SpJM0cBhU1YvAz09Tfw24fJp6ATfPMNdmYPOga5EkzY2/gSxJMgwkSYaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJzP2D6haVlbf+ybT1/Xf+yohXIkmj5ZmBJMkwkCQZBpIkDANJEoaBJAnDQJKEt5b2xVtOJS12nhlIkjwzmAvPGCQtFp4ZSJI8MzgVZjpjmIlnEtLSdbr8e2EYnAZO9ocBDBBJw2UYLFAnGyD3rz3rFK1E0mJgGCwRew++wScGOAM5lmck0uJkGOikDHJJa77MdDbkXWCaD6f7353TJgySrAX+A7AM+I9Vdec8L0kL3MmeDZ3uf1n7sfGiI9P2bNAN32L4eel1WoRBkmXAvcAvAweAJ5Jsr6pn53dl0uKw2P7h6jVTAOrknC6/Z3ApMFlVL1bVj4CtwLp5XpMkLRmpqvleA0muBdZW1b9ozz8OXFZVtxwzbgOwoT39+8DzA77kucBfD3jsQmXPS4M9Lw1z6flnq+p9xxZPi8tE/aqqTcCmuc6TZE9VjQ9hSQuGPS8N9rw0nIqeT5fLRAeBFT3PL2g1SdIInC5h8ASwKsmFSc4ArgO2z/OaJGnJOC0uE1XVkSS3ADvp3lq6uar2ncKXnPOlpgXInpcGe14aht7zafEGsiRpfp0ul4kkSfPIMJAkLe4wSLI2yfNJJpPcOs3+dyR5qO1/PMnK0a9yuPro+V8neTbJ00keSfKz87HOYZqt555x/yxJJVnwtyH203OSj7U/631J/nDUaxy2Pn62/26SR5M82X6+r56PdQ5Lks1JXk3yzAz7k+Tu9v14Osklc3rBqlqUD7pvRP8l8PeAM4C/AFYfM+bTwO+17euAh+Z73SPo+ZeAv9O2f3Mp9NzGvQt4DNgNjM/3ukfw57wKeBI4pz3/mfle9wh63gT8ZtteDeyf73XPsed/DFwCPDPD/quBbwEB1gCPz+X1FvOZQT8fcbEO2NK2vw5cniQjXOOwzdpzVT1aVW+2p7vp/k7HQtbvR5ncDnwB+NtRLu4U6afnTwH3VtVhgKp6dcRrHLZ+ei7g3W37p4H/NcL1DV1VPQYcOsGQdcAD1bUbODvJeYO+3mIOg/OBl3qeH2i1acdU1RHgDeC9I1ndqdFPz71uovs/i4Vs1p7b6fOKqlosn2bWz5/zzwE/l+S/JdndPhV4Ieun588Cv5bkALAD+JejWdq8Odm/7yd0WvyegUYvya8B48A/me+1nEpJ3gZ8EfjEPC9l1JbTvVQ0Qffs77EkF1XV6/O6qlPreuD+qroryS8Cf5Dkg1X1f+d7YQvBYj4z6OcjLn4yJslyuqeWr41kdadGXx/rkeSfAv8W+GhVvTWitZ0qs/X8LuCDQCfJfrrXVrcv8DeR+/lzPgBsr6r/U1XfBf4n3XBYqPrp+SbgYYCq+u/AT9H9QLfFaqgf47OYw6Cfj7jYDqxv29cCf1rtnZkFataek3wI+H26QbDQryPDLD1X1RtVdW5VrayqlXTfJ/loVe2Zn+UORT8/2/+Z7lkBSc6le9noxVEucsj66fmvgMsBkvxDumHwv0e6ytHaDtzY7ipaA7xRVS8POtmivUxUM3zERZLPAXuqajtwH91TyUm6b9RcN38rnrs+e/73wDuB/9TeK/+rqvrovC16jvrseVHps+edwBVJngV+DPybqlqwZ7199rwR+GqSf0X3zeRPLOT/3CX5Gt1AP7e9D3Ib8HaAqvo9uu+LXA1MAm8Cn5zT6y3g75UkaUgW82UiSVKfDANJkmEgSTIMJEkYBpIkDANJEoaBJAn4f8nyGS67YEWUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_pseudo2.toxic.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic_x</th>\n",
       "      <th>toxic_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.004235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.020938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.173887</td>\n",
       "      <td>0.269427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.008064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.007442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   toxic_x   toxic_y\n",
       "0   0  0.000060  0.004235\n",
       "1   1  0.000020  0.020938\n",
       "2   2  0.173887  0.269427\n",
       "3   3  0.000015  0.008064\n",
       "4   4  0.000019  0.007442"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pseudo = df_pseudo.merge(df_pseudo2, on='id')\n",
    "df_pseudo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pseudo = df_pseudo[((df_pseudo['toxic_x']>0.9) & (df_pseudo['toxic_y']>0.9)) | ((df_pseudo['toxic_x']<0.1) & (df_pseudo['toxic_y']<0.1))]\n",
    "df_pseudo['toxic'] = (df_pseudo['toxic_y'] + df_pseudo['toxic_x'])/2\n",
    "df_pseudo = df_pseudo[['id', 'toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pseudo = df_pseudo[(df_pseudo['toxic']>0.9) | (df_pseudo['toxic']<0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pseudo.loc[df_pseudo['toxic']>0.5, 'toxic'] = 1.0\n",
    "df_pseudo.loc[df_pseudo['toxic']<=0.5, 'toxic'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42421"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_pseudo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Doctor Who adlı viki başlığına 12. doctor olar...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Вполне возможно, но я пока не вижу необходимо...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Malesef gerçekleştirilmedi ancak şöyle bir şey...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>:Resim:Seldabagcan.jpg resminde kaynak sorunu ...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Le truc le plus important dans ta tirade c est...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  toxic                                       comment_text lang\n",
       "0   0    0.0  Doctor Who adlı viki başlığına 12. doctor olar...   tr\n",
       "1   1    0.0   Вполне возможно, но я пока не вижу необходимо...   ru\n",
       "2   3    0.0  Malesef gerçekleştirilmedi ancak şöyle bir şey...   tr\n",
       "3   4    0.0  :Resim:Seldabagcan.jpg resminde kaynak sorunu ...   tr\n",
       "4   5    0.0  Le truc le plus important dans ta tirade c est...   fr"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pseudo = df_pseudo.merge(df_test, on='id').rename(columns={'content':'comment_text'})\n",
    "df_pseudo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Este usuario ni siquiera llega al rango de    ...</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Il testo di questa voce pare esser scopiazzato...</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Vale. Sólo expongo mi pasado. Todo tiempo pasa...</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bu maddenin alt başlığı olarak  uluslararası i...</td>\n",
       "      <td>tr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Belçika nın şehirlerinin yanında ilçe ve belde...</td>\n",
       "      <td>tr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       comment_text lang  toxic\n",
       "0   0  Este usuario ni siquiera llega al rango de    ...   es      0\n",
       "1   1  Il testo di questa voce pare esser scopiazzato...   it      0\n",
       "2   2  Vale. Sólo expongo mi pasado. Todo tiempo pasa...   es      1\n",
       "3   3  Bu maddenin alt başlığı olarak  uluslararası i...   tr      0\n",
       "4   4  Belçika nın şehirlerinin yanında ilçe ve belde...   tr      0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_my_back_trans = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/toxic_pt0_all_lng.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>açıklama\\nNeden kullanıcı adım Hardcore Metall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D'aww! O görünüşte ile sıkışmış ediyorum bu ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey dostum, gerçekten düzenleme savaşa çalışmı...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nDaha\\nbölüm istatistikleri sonradan olmalı ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sen, benim kahraman efendim. Herhangi bir şans...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                       comment_text  \n",
       "0              0  açıklama\\nNeden kullanıcı adım Hardcore Metall...  \n",
       "1              0  D'aww! O görünüşte ile sıkışmış ediyorum bu ar...  \n",
       "2              0  Hey dostum, gerçekten düzenleme savaşa çalışmı...  \n",
       "3              0  \"\\nDaha\\nbölüm istatistikleri sonradan olmalı ...  \n",
       "4              0  Sen, benim kahraman efendim. Herhangi bir şans...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_my_back_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_my_back_trans['lang']='unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic lang\n",
       "0  Explanation\\nWhy the edits made under my usern...    0.0   en\n",
       "1  D'aww! He matches this background colour I'm s...    0.0   en\n",
       "2  Hey man, I'm really not trying to edit war. It...    0.0   en\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...    0.0   en\n",
       "4  You, sir, are my hero. Any chance you remember...    0.0   en"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_train = df_train\\\n",
    "    .append(df_my_back_trans[df_train.columns])\\\n",
    "    .append(df_pseudo[df_train.columns])\\\n",
    "    .reset_index(drop=True)\n",
    "df_train.head()\n",
    "# df_valid_sp = df_valid_sp.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612320\n",
      "655072\n",
      "697828\n",
      "740584\n",
      "783324\n",
      "826080\n",
      "868836\n",
      "911598\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "for lang in ['es', 'tr', 'it', 'ru', 'pt', 'fr']:\n",
    "    df_temp = pd.read_csv(f'/kaggle/input/jigsaw-multilingual-toxic-comment-classification/google/jigsaw-toxic-comment-train-google-{lang}-cleaned.csv')\n",
    "    df_temp['lang'] = lang\n",
    "    \n",
    "    df_temp = pd.concat([\n",
    "        df_temp[['comment_text', 'toxic','lang']].query('toxic==1'),\n",
    "        df_temp[['comment_text', 'toxic','lang']].query('toxic==0').sample(n=21378, random_state=0)\n",
    "    ])\n",
    "\n",
    "    df_train = df_train.append(df_temp[df_train.columns]).reset_index(drop=True)\n",
    "        \n",
    "    print(len(df_train))\n",
    "    \n",
    "df_temp = pd.read_csv(f'/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv')\n",
    "df_temp['lang'] = 'en'\n",
    "df_temp = pd.concat([\n",
    "        df_temp[['comment_text', 'toxic','lang']].query('toxic==1'),\n",
    "        df_temp[['comment_text', 'toxic','lang']].query('toxic==0').sample(n=21378, random_state=0)\n",
    "    ])\n",
    "\n",
    "df_train = df_train.append(df_temp[df_train.columns]).reset_index(drop=True)\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang  toxic\n",
       "es    0.0       4133\n",
       "      1.0        919\n",
       "fr    0.0       5379\n",
       "      1.0        600\n",
       "it    0.0       5179\n",
       "      1.0        447\n",
       "pt    0.0       6616\n",
       "      1.0        481\n",
       "ru    0.0       6019\n",
       "      1.0        452\n",
       "tr    0.0      11803\n",
       "      1.0        393\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pseudo.groupby(['lang', 'toxic'])['id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  toxic\n",
       "0   0    0.5\n",
       "1   1    0.5\n",
       "2   2    0.5\n",
       "3   3    0.5\n",
       "4   4    0.5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'toxic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import time\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "#import torch.utils.data as data\n",
    "from torchvision import datasets, models, transforms\n",
    "from transformers import *\n",
    "import random\n",
    "from math import floor, ceil\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "MAX_LEN = 96#192#192#512\n",
    "SEP_TOKEN_ID = 102\n",
    "\n",
    "class QuestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, train_mode=True, labeled=True, train_transforms=None):\n",
    "        \n",
    "        self.train_transforms = train_transforms\n",
    "        self.df = df\n",
    "        if train_mode:\n",
    "            self.labels = df.toxic.values\n",
    "            self.toxic_inds = np.where(self.labels==1)[0]\n",
    "            self.normal_inds = np.where(self.labels==0)[0]            \n",
    "            \n",
    "            \n",
    "            print(f'Here is {len(self.labels)} samples, {len(self.toxic_inds)} samples and {len(self.normal_inds)} samples')\n",
    "            print(f'Class balance is {len(self.toxic_inds)/len(self.labels):.2f}')\n",
    "            \n",
    "        self.train_mode = train_mode\n",
    "        self.labeled = labeled\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#         self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "#         self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')#, \n",
    "#                                                              return_attention_masks=False, \n",
    "#                                                                 return_token_type_ids=False,\n",
    "#                                                                 pad_to_max_length=True,\n",
    "#                                                                 max_length=MAX_LEN)\n",
    "#         self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased',\n",
    "#                                                                 do_lower_case=False,\n",
    "#                                                                 do_basic_tokenize=True,\n",
    "#                                                                 never_split=None,\n",
    "#                                                                 unk_token='[UNK]',\n",
    "#                                                                 sep_token='[SEP]',\n",
    "#                                                                 pad_token='[PAD]',\n",
    "#                                                                 cls_token='[CLS]',\n",
    "#                                                                 mask_token='[MASK]',\n",
    "#                                                                 tokenize_chinese_chars=True,)\n",
    "        #distil\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        token_ids = self.get_token_ids(row)\n",
    "        \n",
    "        if self.labeled:\n",
    "            labels = self.get_label(row)\n",
    "            return {'features': token_ids, 'targets': labels}\n",
    "\n",
    "        else:\n",
    "            return {'features': token_ids}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    " \n",
    "        \n",
    "    def get_token_ids(self, row):\n",
    "        \n",
    "        text = row.comment_text\n",
    "   \n",
    "        if self.train_mode:\n",
    "            \n",
    "            token_ids = self.tokenizer.convert_tokens_to_ids(['CLS',row.lang,'[SEP]']+self.tokenizer.tokenize(text)+['[SEP]'])\n",
    "            \n",
    "            if len(token_ids) < MAX_LEN:\n",
    "                ids = torch.tensor(token_ids + [0] * (MAX_LEN - len(token_ids)))\n",
    "            else:\n",
    "                ind_beg = random.randint(0, len(token_ids)-MAX_LEN)\n",
    "                ids = torch.tensor(token_ids[ind_beg:ind_beg+MAX_LEN])\n",
    "        else:\n",
    "            token_ids = self.tokenizer.convert_tokens_to_ids(['CLS',row.lang,'[SEP]']+self.tokenizer.tokenize(text)+['[SEP]'])\n",
    "            if len(token_ids) < MAX_LEN:\n",
    "                ids = torch.tensor(token_ids + [0] * (MAX_LEN - len(token_ids)))\n",
    "            else:\n",
    "                ids = torch.tensor(token_ids[:MAX_LEN])\n",
    "                \n",
    "        \n",
    "        return ids\n",
    "\n",
    "    def get_label(self, row):\n",
    "#         label = torch.tensor(row[target_column].astype(np.long))\n",
    "        label = np.round(row[target_column])\n",
    "        return torch.tensor([1-label, label]).float()\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        token_ids = torch.stack([x[0] for x in batch])\n",
    "\n",
    "        if self.labeled:\n",
    "            labels = torch.stack([x[1] for x in batch])\n",
    "            return {'features': token_ids, 'targets': labels}\n",
    "        else:\n",
    "            return {'features': token_ids}\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text=\"\"\"Ситуация с заболеваемостью COVID-19 в России постепенно стабилизируется, заявил главный эпидемиолог Минздрава, академик РАН Николай Брико. По его прогнозам, начало снижения заболеваемости возможно в июне.\"\"\"\n",
    "# tokenized = tokenizer.encode(text, max_length=MAX_LEN)\n",
    "# len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid['len_token'] = df_valid['comment_text'].apply(lambda x: len(tokenizer.encode(x, max_length=1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid.len_token.hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid[df_valid.len_token==860]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class QuestModel(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        super(QuestModel, self).__init__()\n",
    "        self.model_name = 'QuestModel'\n",
    "        \n",
    "#         self.bert_model = BertModel.from_pretrained('bert-base-uncased') \n",
    "#         self.bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "#         self.bert_model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "#         self.bert_model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "#        self.bert_model = XLMRobertaModel.from_pretrained('xlm-roberta-base', \n",
    "                                                          #output_hidden_states=False, \n",
    "                                                          #output_attentions=False)\n",
    "#         self.bert_model = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
    "        self.bert_model = XLMRobertaModel.from_pretrained('xlm-roberta-large')#,\n",
    "#                                                           output_hidden_states=False, \n",
    "#                                                           output_attentions=False)\n",
    "    \n",
    "#         self.fc = nn.Linear(768, n_classes)\n",
    "        self.fc = nn.Linear(1024, n_classes)\n",
    "\n",
    "    def forward(self, ids):\n",
    "#         attention_mask = (ids > 0)\n",
    "#         print(ids.shape)\n",
    "        layers = self.bert_model(input_ids=ids)#, attention_mask=attention_mask)\n",
    "#         print(layers[0].shape)\n",
    "#         out = F.dropout(layers[0][:, 0, :], p=0.2, training=self.training)\n",
    "#         print(layers[0].shape)\n",
    "#         print([l.shape for l in layers])\n",
    "#         out = F.dropout(layers[-1][:, 0, :], p=0.35, training=self.training)\n",
    "        out = F.dropout(layers[0][:, 0, :], p=0.2, training=self.training)\n",
    "        logit = self.fc(out)#.unsqueeze(1)\n",
    "        return logit #, 'for_auc': logit[:, 1]}#[:,1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model = XLMRobertaModel.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BalancedSampler(torch.utils.data.sampler.Sampler):\n",
    "#     def __init__(self, dataset):\n",
    "       \n",
    "#         self.toxic_inds = dataset.toxic_inds.copy()\n",
    "#         self.normal_inds = dataset.normal_inds.copy()\n",
    "        \n",
    "#         self.num_samples = 2*min(len(self.toxic_inds), len(self.normal_inds))\n",
    "        \n",
    "#         shfl(self.toxic_inds)\n",
    "#         shfl(self.normal_inds)\n",
    "        \n",
    "#         self.inds = []\n",
    "#         for i in range(min(len(self.toxic_inds), len(self.normal_inds))):\n",
    "#             self.inds.append(self.normal_inds[i%len(self.normal_inds)])\n",
    "#             self.inds.append(self.toxic_inds[i%len(self.toxic_inds)])\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         #print ('\\tcalling Sampler:__iter__')\n",
    "#         return iter(self.inds)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         #print ('\\tcalling Sampler:__len__')\n",
    "#         return self.num_samples\n",
    "\n",
    "class BalancedSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, dataset):\n",
    "       \n",
    "        self.toxic_inds = dataset.toxic_inds.copy()\n",
    "        self.normal_inds = dataset.normal_inds.copy()\n",
    "        \n",
    "        self.num_samples = 2*max(len(self.toxic_inds), len(self.normal_inds))\n",
    "        \n",
    "        random.Random(12345).shuffle(self.toxic_inds)\n",
    "        random.Random(12345).shuffle(self.normal_inds)\n",
    "\n",
    "        \n",
    "        self.inds = []\n",
    "        for i in range(max(len(self.toxic_inds), len(self.normal_inds))):\n",
    "            self.inds.append(self.normal_inds[i%len(self.normal_inds)])\n",
    "            self.inds.append(self.toxic_inds[i%len(self.toxic_inds)])\n",
    "\n",
    "    def __iter__(self):\n",
    "        #print ('\\tcalling Sampler:__iter__')\n",
    "        return iter(self.inds)\n",
    "\n",
    "    def __len__(self):\n",
    "        #print ('\\tcalling Sampler:__len__')\n",
    "        return self.num_samples\n",
    "\n",
    "class BalancedSamplerRnd(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, dataset):\n",
    "       \n",
    "        self.toxic_inds = dataset.toxic_inds.copy()\n",
    "        self.normal_inds = dataset.normal_inds.copy()\n",
    "        \n",
    "        self.num_samples = 2* len(self.normal_inds)\n",
    "        \n",
    "        random.Random(12345).shuffle(self.toxic_inds)\n",
    "        random.Random(12345).shuffle(self.normal_inds)\n",
    "        \n",
    "        N = int(len(self.normal_inds) / len(self.toxic_inds))\n",
    "        generated_seq = []\n",
    "        for k in range(N):\n",
    "            tmp = list(range(len(self.toxic_inds)))\n",
    "            random.Random(12345).shuffle(tmp)\n",
    "            generated_seq += tmp \n",
    "            \n",
    "        generated_seq += tmp[:len(self.normal_inds)-len(generated_seq)]\n",
    "        \n",
    "        self.inds = []\n",
    "        for i in range(len(self.normal_inds)):\n",
    "            self.inds.append(self.normal_inds[i])\n",
    "            self.inds.append(self.toxic_inds[generated_seq[i]])\n",
    "\n",
    "    def __iter__(self):\n",
    "        #print ('\\tcalling Sampler:__iter__')\n",
    "        return iter(self.inds)\n",
    "\n",
    "    def __len__(self):\n",
    "        #print ('\\tcalling Sampler:__len__')\n",
    "        return self.num_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "        callback_get_label func: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None, callback_get_label=None):\n",
    "                \n",
    "        # if indices is not provided, \n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # define custom callback\n",
    "        self.callback_get_label = callback_get_label\n",
    "\n",
    "        # if num_samples is not provided, \n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "            \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "                \n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        if self.callback_get_label:\n",
    "            return self.callback_get_label(dataset, idx)\n",
    "        else:\n",
    "            dataset.labels[idx]\n",
    "#             raise NotImplementedError\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "            self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(to_balance=True, shuffle_before=True):\n",
    "    if SERVER:\n",
    "        workers=1\n",
    "    else:\n",
    "        workers = 6    \n",
    "    \n",
    "    \n",
    "\n",
    "    train_dataset = QuestDataset(df_train, train_mode=True)#, train_transforms=get_train_transforms())\n",
    "    valid_dataset = QuestDataset(df_valid, train_mode=False)\n",
    "    \n",
    "    \n",
    "#     train_dataset = QuestDataset(df_pseudo, train_mode=True, train_transforms=get_train_transforms())\n",
    "#     valid_dataset = QuestDataset(df_valid, train_mode=False)\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        num_workers=workers,\n",
    "        sampler=BalancedSamplerRnd(train_dataset) if to_balance else None,#ImbalancedDatasetSampler(train_dataset) if to_balance else None,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        num_workers=workers,\n",
    "        batch_size=batch_size,\n",
    "    )    \n",
    "    \n",
    "       \n",
    "    loaders = {}\n",
    "    loaders['train'] = train_loader\n",
    "    loaders['valid'] = valid_loader\n",
    "    \n",
    "    \n",
    "    for i in ['es', 'it', 'tr']:\n",
    "        df = df_valid\n",
    "        df = df[df['lang']==i]\n",
    "\n",
    "        loaders['valid_'+ i] = DataLoader(\n",
    "            QuestDataset(df, train_mode=False),\n",
    "            num_workers=workers,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is 911598 samples, 299560 samples and 612038 samples\n",
      "Class balance is 0.33\n",
      "----------------Experiment: simple\n",
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
      "1/300 * Epoch (train):   0% 1/38253 [00:00<8:07:38,  1.31it/s, loss=0.060]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kb/jig_env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning:\n",
      "\n",
      "size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "\n",
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning:\n",
      "\n",
      "This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/300 * Epoch (train):   0% 48/38253 [00:11<2:27:41,  4.31it/s, loss=0.056]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "1/300 * Epoch (train):   0% 86/38253 [00:20<2:27:13,  4.32it/s, loss=0.055]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "1/300 * Epoch (train):   0% 132/38253 [00:31<2:27:37,  4.30it/s, loss=0.049]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "1/300 * Epoch (train):   1% 341/38253 [01:19<2:27:32,  4.28it/s, loss=0.053]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "1/300 * Epoch (train):   3% 1178/38253 [04:35<2:24:26,  4.28it/s, loss=0.031]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "1/300 * Epoch (train):   3% 1199/38253 [04:40<2:24:39,  4.27it/s, loss=0.030]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "1/300 * Epoch (train):  20% 7499/38253 [29:12<2:00:09,  4.27it/s, loss=0.031]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "1/300 * Epoch (train):  22% 8316/38253 [32:23<1:56:45,  4.27it/s, loss=0.026]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "1/300 * Epoch (train):  25% 9556/38253 [37:14<1:52:04,  4.27it/s, loss=0.020]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "1/300 * Epoch (train):  40% 15425/38253 [1:00:05<1:29:16,  4.26it/s, loss=0.017]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "1/300 * Epoch (train):  44% 17012/38253 [1:06:17<1:23:04,  4.26it/s, loss=0.041]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "1/300 * Epoch (train):  46% 17629/38253 [1:08:41<1:20:30,  4.27it/s, loss=0.023]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
      "1/300 * Epoch (train):  49% 18670/38253 [1:12:45<1:16:55,  4.24it/s, loss=0.020]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0\n",
      "1/300 * Epoch (train):  66% 25277/38253 [1:38:34<50:35,  4.27it/s, loss=0.017]  Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "1/300 * Epoch (train):  68% 25860/38253 [1:40:51<48:25,  4.27it/s, loss=0.023]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
      "1/300 * Epoch (train):  79% 30088/38253 [1:57:23<31:59,  4.25it/s, loss=0.006]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "1/300 * Epoch (train):  80% 30659/38253 [1:59:37<29:40,  4.26it/s, loss=0.009]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
      "1/300 * Epoch (train): 100% 38253/38253 [2:29:19<00:00,  4.27it/s, loss=0.005]\n",
      "1/300 * Epoch (valid): 100% 250/250 [00:15<00:00, 16.27it/s, loss=0.006]\n",
      "1/300 * Epoch (valid_es): 100% 79/79 [00:05<00:00, 15.56it/s, loss=0.026]\n",
      "1/300 * Epoch (valid_it): 100% 79/79 [00:05<00:00, 15.54it/s, loss=0.003]\n",
      "1/300 * Epoch (valid_tr): 100% 94/94 [00:06<00:00, 15.64it/s, loss=0.003]    \n",
      "[2020-05-12 15:36:42,070] \n",
      "1/300 * Epoch 1 (_base): lr=1.000e-06 | momentum=0.9000\n",
      "1/300 * Epoch 1 (train): auc/_mean=0.9596 | auc/class_0=0.9596 | loss=0.0189\n",
      "1/300 * Epoch 1 (valid): auc/_mean=0.9312 | auc/class_0=0.9312 | es_auc/_mean=0.9127 | es_auc/class_0=0.9127 | es_loss=0.0330 | it_auc/_mean=0.8923 | it_auc/class_0=0.8923 | it_loss=0.0336 | loss=0.0241 | tr_auc/_mean=0.9801 | tr_auc/class_0=0.9801 | tr_loss=0.0083\n",
      "1/300 * Epoch 1 (valid_es): auc/_mean=0.9127 | auc/class_0=0.9127 | loss=0.0330\n",
      "1/300 * Epoch 1 (valid_it): auc/_mean=0.8923 | auc/class_0=0.8923 | loss=0.0336\n",
      "1/300 * Epoch 1 (valid_tr): auc/_mean=0.9801 | auc/class_0=0.9801 | loss=0.0083\n",
      "2/300 * Epoch (train):   2% 614/38253 [02:23<2:26:15,  4.29it/s, loss=0.012]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "2/300 * Epoch (train):   2% 616/38253 [02:23<2:17:29,  4.56it/s, loss=0.013]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "2/300 * Epoch (train):   4% 1686/38253 [06:33<2:22:24,  4.28it/s, loss=0.010]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "2/300 * Epoch (train):  23% 8772/38253 [34:08<1:54:41,  4.28it/s, loss=0.009]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "2/300 * Epoch (train):  29% 11280/38253 [43:55<1:45:08,  4.28it/s, loss=0.020]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "2/300 * Epoch (train):  31% 12016/38253 [46:47<1:42:17,  4.27it/s, loss=0.010]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "2/300 * Epoch (train):  45% 17114/38253 [1:06:41<1:22:45,  4.26it/s, loss=0.012]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "2/300 * Epoch (train):  46% 17734/38253 [1:09:06<1:20:17,  4.26it/s, loss=0.011]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "2/300 * Epoch (train):  61% 23147/38253 [1:30:16<59:03,  4.26it/s, loss=0.013]  Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "2/300 * Epoch (train):  69% 26214/38253 [1:42:16<47:08,  4.26it/s, loss=0.009]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "2/300 * Epoch (train):  69% 26224/38253 [1:42:18<46:53,  4.28it/s, loss=0.014]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "2/300 * Epoch (train):  75% 28500/38253 [1:51:13<38:10,  4.26it/s, loss=0.037]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "2/300 * Epoch (train):  80% 30775/38253 [2:00:07<29:19,  4.25it/s, loss=0.006]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "2/300 * Epoch (train):  88% 33568/38253 [2:11:03<18:20,  4.26it/s, loss=0.015]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "2/300 * Epoch (train):  88% 33609/38253 [2:11:13<18:13,  4.25it/s, loss=0.006]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "2/300 * Epoch (train): 100% 38253/38253 [2:29:25<00:00,  4.27it/s, loss=0.004]\n",
      "2/300 * Epoch (valid): 100% 250/250 [00:15<00:00, 16.26it/s, loss=0.003]\n",
      "2/300 * Epoch (valid_es): 100% 79/79 [00:05<00:00, 15.53it/s, loss=0.011]   \n",
      "2/300 * Epoch (valid_it): 100% 79/79 [00:05<00:00, 15.51it/s, loss=0.008]\n",
      "2/300 * Epoch (valid_tr): 100% 94/94 [00:06<00:00, 15.58it/s, loss=0.003]    \n",
      "[2020-05-12 18:07:52,483] \n",
      "2/300 * Epoch 2 (_base): lr=1.000e-06 | momentum=0.9000\n",
      "2/300 * Epoch 2 (train): auc/_mean=0.9800 | auc/class_0=0.9800 | loss=0.0132\n",
      "2/300 * Epoch 2 (valid): auc/_mean=0.9316 | auc/class_0=0.9316 | es_auc/_mean=0.9162 | es_auc/class_0=0.9162 | es_loss=0.0317 | it_auc/_mean=0.8965 | it_auc/class_0=0.8965 | it_loss=0.0340 | loss=0.0254 | tr_auc/_mean=0.9772 | tr_auc/class_0=0.9772 | tr_loss=0.0125\n",
      "2/300 * Epoch 2 (valid_es): auc/_mean=0.9162 | auc/class_0=0.9162 | loss=0.0317\n",
      "2/300 * Epoch 2 (valid_it): auc/_mean=0.8965 | auc/class_0=0.8965 | loss=0.0340\n",
      "2/300 * Epoch 2 (valid_tr): auc/_mean=0.9772 | auc/class_0=0.9772 | loss=0.0125\n",
      "3/300 * Epoch (train):   6% 2197/38253 [08:32<2:20:25,  4.28it/s, loss=0.005]  Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "3/300 * Epoch (train):   6% 2332/38253 [09:03<2:19:53,  4.28it/s, loss=0.014]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "3/300 * Epoch (train):  11% 4213/38253 [16:23<2:12:29,  4.28it/s, loss=0.005]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "3/300 * Epoch (train):  24% 8999/38253 [35:01<1:54:06,  4.27it/s, loss=0.018]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "3/300 * Epoch (train):  24% 9255/38253 [36:01<1:52:57,  4.28it/s, loss=0.015]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "3/300 * Epoch (train):  35% 13500/38253 [52:34<1:36:24,  4.28it/s, loss=0.006]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "3/300 * Epoch (train):  38% 14400/38253 [56:04<1:33:28,  4.25it/s, loss=0.008]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/300 * Epoch (train):  43% 16634/38253 [1:04:47<1:24:08,  4.28it/s, loss=0.008]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "3/300 * Epoch (train):  49% 18780/38253 [1:13:10<1:16:11,  4.26it/s, loss=0.004]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "3/300 * Epoch (train):  49% 18832/38253 [1:13:22<1:15:46,  4.27it/s, loss=0.008]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
      "3/300 * Epoch (train):  72% 27409/38253 [1:46:52<42:31,  4.25it/s, loss=0.005]      Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "3/300 * Epoch (train):  76% 28956/38253 [1:52:56<36:19,  4.27it/s, loss=0.014]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "3/300 * Epoch (train):  82% 31284/38253 [2:02:04<27:22,  4.24it/s, loss=0.014]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "3/300 * Epoch (train):  85% 32328/38253 [2:06:09<23:14,  4.25it/s, loss=0.019]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "3/300 * Epoch (train):  94% 35956/38253 [2:20:21<09:00,  4.25it/s, loss=0.007]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "3/300 * Epoch (train): 100% 38253/38253 [2:29:21<00:00,  4.27it/s, loss=9.990e-04]\n",
      "3/300 * Epoch (valid): 100% 250/250 [00:15<00:00, 16.31it/s, loss=0.002]\n",
      "3/300 * Epoch (valid_es): 100% 79/79 [00:05<00:00, 15.50it/s, loss=0.004]\n",
      "3/300 * Epoch (valid_it): 100% 79/79 [00:05<00:00, 15.52it/s, loss=0.004]\n",
      "3/300 * Epoch (valid_tr): 100% 94/94 [00:06<00:00, 15.65it/s, loss=0.002]    \n",
      "[2020-05-12 20:39:37,250] \n",
      "3/300 * Epoch 3 (_base): lr=1.000e-06 | momentum=0.9000\n",
      "3/300 * Epoch 3 (train): auc/_mean=0.9842 | auc/class_0=0.9842 | loss=0.0116\n",
      "3/300 * Epoch 3 (valid): auc/_mean=0.9302 | auc/class_0=0.9302 | es_auc/_mean=0.9147 | es_auc/class_0=0.9147 | es_loss=0.0347 | it_auc/_mean=0.8936 | it_auc/class_0=0.8936 | it_loss=0.0398 | loss=0.0283 | tr_auc/_mean=0.9764 | tr_auc/class_0=0.9764 | tr_loss=0.0129\n",
      "3/300 * Epoch 3 (valid_es): auc/_mean=0.9147 | auc/class_0=0.9147 | loss=0.0347\n",
      "3/300 * Epoch 3 (valid_it): auc/_mean=0.8936 | auc/class_0=0.8936 | loss=0.0398\n",
      "3/300 * Epoch 3 (valid_tr): auc/_mean=0.9764 | auc/class_0=0.9764 | loss=0.0129\n",
      "4/300 * Epoch (train):   8% 3123/38253 [12:07<2:16:30,  4.29it/s, loss=0.005]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "4/300 * Epoch (train):  30% 11412/38253 [44:22<1:43:58,  4.30it/s, loss=0.019]   Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "4/300 * Epoch (train):  37% 14313/38253 [55:37<1:33:18,  4.28it/s, loss=0.005]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "4/300 * Epoch (train):  48% 18338/38253 [1:11:19<1:17:34,  4.28it/s, loss=0.008]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "4/300 * Epoch (train):  48% 18346/38253 [1:11:21<1:17:16,  4.29it/s, loss=0.018]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "4/300 * Epoch (train):  48% 18532/38253 [1:12:05<1:16:58,  4.27it/s, loss=0.009]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "4/300 * Epoch (train):  54% 20471/38253 [1:19:38<1:09:04,  4.29it/s, loss=0.012]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "4/300 * Epoch (train):  59% 22598/38253 [1:27:53<1:00:42,  4.30it/s, loss=0.026]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "4/300 * Epoch (train):  64% 24532/38253 [1:35:31<55:04,  4.15it/s, loss=0.010]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "4/300 * Epoch (train):  92% 35256/38253 [2:18:17<11:57,  4.18it/s, loss=0.017]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "4/300 * Epoch (train):  98% 37399/38253 [2:26:49<03:23,  4.19it/s, loss=0.013]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "4/300 * Epoch (train): 100% 38253/38253 [2:30:15<00:00,  4.24it/s, loss=0.003]\n",
      "4/300 * Epoch (valid): 100% 250/250 [00:15<00:00, 15.98it/s, loss=0.001]\n",
      "4/300 * Epoch (valid_es): 100% 79/79 [00:05<00:00, 14.93it/s, loss=0.005]   \n",
      "4/300 * Epoch (valid_it): 100% 79/79 [00:05<00:00, 15.08it/s, loss=0.002]\n",
      "4/300 * Epoch (valid_tr): 100% 94/94 [00:06<00:00, 15.05it/s, loss=7.171e-04]\n",
      "[2020-05-12 23:11:30,467] \n",
      "4/300 * Epoch 4 (_base): lr=1.000e-06 | momentum=0.9000\n",
      "4/300 * Epoch 4 (train): auc/_mean=0.9876 | auc/class_0=0.9876 | loss=0.0101\n",
      "4/300 * Epoch 4 (valid): auc/_mean=0.9275 | auc/class_0=0.9275 | es_auc/_mean=0.9114 | es_auc/class_0=0.9114 | es_loss=0.0423 | it_auc/_mean=0.8935 | it_auc/class_0=0.8935 | it_loss=0.0481 | loss=0.0340 | tr_auc/_mean=0.9729 | tr_auc/class_0=0.9729 | tr_loss=0.0144\n",
      "4/300 * Epoch 4 (valid_es): auc/_mean=0.9114 | auc/class_0=0.9114 | loss=0.0423\n",
      "4/300 * Epoch 4 (valid_it): auc/_mean=0.8935 | auc/class_0=0.8935 | loss=0.0481\n",
      "4/300 * Epoch 4 (valid_tr): auc/_mean=0.9729 | auc/class_0=0.9729 | loss=0.0144\n",
      "5/300 * Epoch (train):   3% 1151/38253 [04:32<2:26:51,  4.21it/s, loss=0.016]   Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "5/300 * Epoch (train):   3% 1335/38253 [05:15<2:25:58,  4.22it/s, loss=0.017]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "5/300 * Epoch (train):  12% 4589/38253 [18:01<2:13:54,  4.19it/s, loss=0.004]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "5/300 * Epoch (train):  13% 5057/38253 [19:51<2:11:30,  4.21it/s, loss=0.006]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "5/300 * Epoch (train):  21% 8016/38253 [31:28<1:59:03,  4.23it/s, loss=0.017]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "5/300 * Epoch (train):  33% 12566/38253 [49:16<1:40:28,  4.26it/s, loss=0.007]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "5/300 * Epoch (train):  33% 12605/38253 [49:25<1:40:23,  4.26it/s, loss=0.005]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "5/300 * Epoch (train):  33% 12772/38253 [50:04<1:39:43,  4.26it/s, loss=0.004]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "5/300 * Epoch (train):  54% 20702/38253 [1:21:01<1:08:39,  4.26it/s, loss=0.006]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "5/300 * Epoch (train):  55% 20936/38253 [1:21:56<1:07:31,  4.27it/s, loss=0.012]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "5/300 * Epoch (train):  62% 23716/38253 [1:32:48<56:47,  4.27it/s, loss=0.003]      Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "5/300 * Epoch (train):  74% 28161/38253 [1:50:11<39:34,  4.25it/s, loss=0.011]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "5/300 * Epoch (train):  80% 30431/38253 [1:59:04<30:33,  4.27it/s, loss=0.009]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "5/300 * Epoch (train):  87% 33470/38253 [2:10:59<18:46,  4.25it/s, loss=0.008]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "5/300 * Epoch (train):  98% 37513/38253 [2:26:49<02:54,  4.25it/s, loss=0.020]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "5/300 * Epoch (train): 100% 38253/38253 [2:29:43<00:00,  4.26it/s, loss=3.869e-04]\n",
      "5/300 * Epoch (valid): 100% 250/250 [00:15<00:00, 16.36it/s, loss=0.001]  \n",
      "5/300 * Epoch (valid_es): 100% 79/79 [00:05<00:00, 15.60it/s, loss=3.308e-04]\n",
      "5/300 * Epoch (valid_it): 100% 79/79 [00:05<00:00, 15.54it/s, loss=0.005]\n",
      "5/300 * Epoch (valid_tr): 100% 94/94 [00:06<00:00, 15.63it/s, loss=3.833e-04]\n",
      "[2020-05-13 01:42:34,576] \n",
      "5/300 * Epoch 5 (_base): lr=1.000e-06 | momentum=0.9000\n",
      "5/300 * Epoch 5 (train): auc/_mean=0.9903 | auc/class_0=0.9903 | loss=0.0089\n",
      "5/300 * Epoch 5 (valid): auc/_mean=0.9220 | auc/class_0=0.9220 | es_auc/_mean=0.9068 | es_auc/class_0=0.9068 | es_loss=0.0458 | it_auc/_mean=0.8883 | it_auc/class_0=0.8883 | it_loss=0.0521 | loss=0.0379 | tr_auc/_mean=0.9678 | tr_auc/class_0=0.9678 | tr_loss=0.0186\n",
      "5/300 * Epoch 5 (valid_es): auc/_mean=0.9068 | auc/class_0=0.9068 | loss=0.0458\n",
      "5/300 * Epoch 5 (valid_it): auc/_mean=0.8883 | auc/class_0=0.8883 | loss=0.0521\n",
      "5/300 * Epoch 5 (valid_tr): auc/_mean=0.9678 | auc/class_0=0.9678 | loss=0.0186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/300 * Epoch (train):   3% 1278/38253 [04:55<2:22:24,  4.33it/s, loss=0.008]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "6/300 * Epoch (train):   5% 1878/38253 [07:14<2:20:02,  4.33it/s, loss=0.033]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "6/300 * Epoch (train):   8% 2962/38253 [11:25<2:16:22,  4.31it/s, loss=0.004]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "6/300 * Epoch (train):  20% 7539/38253 [29:10<1:59:36,  4.28it/s, loss=0.012]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "6/300 * Epoch (train):  26% 9763/38253 [37:49<1:51:00,  4.28it/s, loss=0.009]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "6/300 * Epoch (train):  27% 10209/38253 [39:33<1:49:14,  4.28it/s, loss=0.008]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "6/300 * Epoch (train):  37% 14118/38253 [54:47<1:34:04,  4.28it/s, loss=0.004]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "6/300 * Epoch (train):  45% 17310/38253 [1:07:10<1:21:09,  4.30it/s, loss=0.007]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "6/300 * Epoch (train):  56% 21472/38253 [1:23:21<1:05:34,  4.27it/s, loss=0.011]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "6/300 * Epoch (train):  64% 24453/38253 [1:35:00<53:48,  4.28it/s, loss=0.004]      Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "6/300 * Epoch (train):  69% 26227/38253 [1:41:55<46:57,  4.27it/s, loss=0.006]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "6/300 * Epoch (train):  74% 28376/38253 [1:50:19<38:35,  4.27it/s, loss=0.001]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "6/300 * Epoch (train):  83% 31925/38253 [2:04:11<24:44,  4.26it/s, loss=0.013]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "6/300 * Epoch (train):  93% 35504/38253 [2:18:10<10:44,  4.26it/s, loss=0.004]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "6/300 * Epoch (train): 100% 38253/38253 [2:28:55<00:00,  4.28it/s, loss=3.006e-04]\n",
      "6/300 * Epoch (valid): 100% 250/250 [00:15<00:00, 16.34it/s, loss=0.002]    \n",
      "6/300 * Epoch (valid_es): 100% 79/79 [00:05<00:00, 15.66it/s, loss=0.001]   \n",
      "6/300 * Epoch (valid_it): 100% 79/79 [00:05<00:00, 15.48it/s, loss=0.013]\n",
      "6/300 * Epoch (valid_tr): 100% 94/94 [00:06<00:00, 15.58it/s, loss=8.603e-05]\n",
      "[2020-05-13 04:12:49,680] \n",
      "6/300 * Epoch 6 (_base): lr=1.000e-06 | momentum=0.9000\n",
      "6/300 * Epoch 6 (train): auc/_mean=0.9923 | auc/class_0=0.9923 | loss=0.0079\n",
      "6/300 * Epoch 6 (valid): auc/_mean=0.9275 | auc/class_0=0.9275 | es_auc/_mean=0.9101 | es_auc/class_0=0.9101 | es_loss=0.0497 | it_auc/_mean=0.8910 | it_auc/class_0=0.8910 | it_loss=0.0534 | loss=0.0384 | tr_auc/_mean=0.9743 | tr_auc/class_0=0.9743 | tr_loss=0.0155\n",
      "6/300 * Epoch 6 (valid_es): auc/_mean=0.9101 | auc/class_0=0.9101 | loss=0.0497\n",
      "6/300 * Epoch 6 (valid_it): auc/_mean=0.8910 | auc/class_0=0.8910 | loss=0.0534\n",
      "6/300 * Epoch 6 (valid_tr): auc/_mean=0.9743 | auc/class_0=0.9743 | loss=0.0155\n",
      "7/300 * Epoch (train):   1% 419/38253 [01:37<2:26:29,  4.30it/s, loss=0.012]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "7/300 * Epoch (train):   8% 3178/38253 [12:18<2:15:53,  4.30it/s, loss=0.004]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "7/300 * Epoch (train):  22% 8317/38253 [32:06<1:55:16,  4.33it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "7/300 * Epoch (train):  28% 10824/38253 [41:48<1:45:47,  4.32it/s, loss=0.007]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "7/300 * Epoch (train):  39% 14792/38253 [57:08<1:30:56,  4.30it/s, loss=0.001]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "7/300 * Epoch (train):  44% 16902/38253 [1:05:19<1:22:44,  4.30it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "7/300 * Epoch (train):  48% 18315/38253 [1:10:48<1:17:36,  4.28it/s, loss=0.005]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "7/300 * Epoch (train):  55% 21079/38253 [1:21:32<1:06:17,  4.32it/s, loss=0.019]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "7/300 * Epoch (train):  55% 21117/38253 [1:21:41<1:06:09,  4.32it/s, loss=0.010]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "7/300 * Epoch (train):  59% 22572/38253 [1:27:18<1:00:33,  4.32it/s, loss=0.004]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "7/300 * Epoch (train):  73% 27996/38253 [1:48:20<39:55,  4.28it/s, loss=0.015]      Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "7/300 * Epoch (train):  81% 30901/38253 [1:59:38<28:26,  4.31it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "7/300 * Epoch (train):  89% 33925/38253 [2:11:25<16:53,  4.27it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "7/300 * Epoch (train): 100% 38119/38253 [2:27:46<00:31,  4.27it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "7/300 * Epoch (train): 100% 38253/38253 [2:28:18<00:00,  4.30it/s, loss=1.253e-04]\n",
      "7/300 * Epoch (valid): 100% 250/250 [00:15<00:00, 16.26it/s, loss=0.003]    \n",
      "7/300 * Epoch (valid_es): 100% 79/79 [00:05<00:00, 15.44it/s, loss=1.705e-05]\n",
      "7/300 * Epoch (valid_it): 100% 79/79 [00:05<00:00, 15.56it/s, loss=0.018]   \n",
      "7/300 * Epoch (valid_tr): 100% 94/94 [00:06<00:00, 15.58it/s, loss=2.855e-04]\n",
      "[2020-05-13 06:42:27,838] \n",
      "7/300 * Epoch 7 (_base): lr=1.000e-06 | momentum=0.9000\n",
      "7/300 * Epoch 7 (train): auc/_mean=0.9939 | auc/class_0=0.9939 | loss=0.0069\n",
      "7/300 * Epoch 7 (valid): auc/_mean=0.9180 | auc/class_0=0.9180 | es_auc/_mean=0.9020 | es_auc/class_0=0.9020 | es_loss=0.0552 | it_auc/_mean=0.8854 | it_auc/class_0=0.8854 | it_loss=0.0628 | loss=0.0461 | tr_auc/_mean=0.9613 | tr_auc/class_0=0.9613 | tr_loss=0.0235\n",
      "7/300 * Epoch 7 (valid_es): auc/_mean=0.9020 | auc/class_0=0.9020 | loss=0.0552\n",
      "7/300 * Epoch 7 (valid_it): auc/_mean=0.8854 | auc/class_0=0.8854 | loss=0.0628\n",
      "7/300 * Epoch 7 (valid_tr): auc/_mean=0.9613 | auc/class_0=0.9613 | loss=0.0235\n",
      "8/300 * Epoch (train):   2% 926/38253 [03:35<2:24:59,  4.29it/s, loss=0.018]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "8/300 * Epoch (train):  17% 6506/38253 [25:17<2:04:15,  4.26it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "8/300 * Epoch (train):  29% 11029/38253 [42:58<1:45:53,  4.28it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "8/300 * Epoch (train):  29% 11272/38253 [43:54<1:44:47,  4.29it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "8/300 * Epoch (train):  30% 11614/38253 [45:14<1:43:55,  4.27it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "8/300 * Epoch (train):  47% 17796/38253 [1:09:19<1:19:57,  4.26it/s, loss=0.005]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "8/300 * Epoch (train):  53% 20162/38253 [1:18:32<1:10:08,  4.30it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "8/300 * Epoch (train):  53% 20192/38253 [1:18:39<1:10:05,  4.29it/s, loss=0.007]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "8/300 * Epoch (train):  57% 21834/38253 [1:25:01<1:03:44,  4.29it/s, loss=0.005]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "8/300 * Epoch (train):  63% 23971/38253 [1:33:19<55:25,  4.29it/s, loss=9.489e-04]  Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/300 * Epoch (train):  66% 25410/38253 [1:38:55<49:57,  4.28it/s, loss=0.006]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "8/300 * Epoch (train):  73% 27907/38253 [1:48:41<40:27,  4.26it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "8/300 * Epoch (train):  76% 29237/38253 [1:53:53<35:17,  4.26it/s, loss=0.006]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "8/300 * Epoch (train):  81% 30851/38253 [2:00:11<28:58,  4.26it/s, loss=0.006]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
      "8/300 * Epoch (train): 100% 38253/38253 [2:29:08<00:00,  4.27it/s, loss=6.456e-04]\n",
      "8/300 * Epoch (valid): 100% 250/250 [00:15<00:00, 16.30it/s, loss=0.003]    \n",
      "8/300 * Epoch (valid_es): 100% 79/79 [00:05<00:00, 15.54it/s, loss=2.743e-05]\n",
      "8/300 * Epoch (valid_it): 100% 79/79 [00:05<00:00, 15.57it/s, loss=0.011]\n",
      "8/300 * Epoch (valid_tr): 100% 94/94 [00:06<00:00, 15.63it/s, loss=0.002]    \n",
      "[2020-05-13 09:12:57,214] \n",
      "8/300 * Epoch 8 (_base): lr=1.000e-06 | momentum=0.9000\n",
      "8/300 * Epoch 8 (train): auc/_mean=0.9950 | auc/class_0=0.9950 | loss=0.0063\n",
      "8/300 * Epoch 8 (valid): auc/_mean=0.9180 | auc/class_0=0.9180 | es_auc/_mean=0.9015 | es_auc/class_0=0.9015 | es_loss=0.0546 | it_auc/_mean=0.8826 | it_auc/class_0=0.8826 | it_loss=0.0632 | loss=0.0457 | tr_auc/_mean=0.9652 | tr_auc/class_0=0.9652 | tr_loss=0.0226\n",
      "8/300 * Epoch 8 (valid_es): auc/_mean=0.9015 | auc/class_0=0.9015 | loss=0.0546\n",
      "8/300 * Epoch 8 (valid_it): auc/_mean=0.8826 | auc/class_0=0.8826 | loss=0.0632\n",
      "8/300 * Epoch 8 (valid_tr): auc/_mean=0.9652 | auc/class_0=0.9652 | loss=0.0226\n",
      "9/300 * Epoch (train):  11% 4236/38253 [16:23<2:12:03,  4.29it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "9/300 * Epoch (train):  20% 7504/38253 [29:05<1:58:52,  4.31it/s, loss=0.004]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "9/300 * Epoch (train):  21% 8074/38253 [31:17<1:56:39,  4.31it/s, loss=0.009]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "9/300 * Epoch (train):  29% 11268/38253 [43:38<1:44:36,  4.30it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "9/300 * Epoch (train):  37% 14111/38253 [54:41<1:33:30,  4.30it/s, loss=0.007]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "9/300 * Epoch (train):  37% 14334/38253 [55:32<1:32:37,  4.30it/s, loss=0.005]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "9/300 * Epoch (train):  54% 20523/38253 [1:19:35<1:08:52,  4.29it/s, loss=0.009]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "9/300 * Epoch (train):  67% 25687/38253 [1:39:41<49:08,  4.26it/s, loss=0.011]      Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "9/300 * Epoch (train):  69% 26234/38253 [1:41:50<47:04,  4.26it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "9/300 * Epoch (train):  69% 26427/38253 [1:42:35<46:17,  4.26it/s, loss=0.006]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "9/300 * Epoch (train):  81% 30795/38253 [1:59:39<29:10,  4.26it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "9/300 * Epoch (train):  87% 33453/38253 [2:10:03<18:46,  4.26it/s, loss=0.006]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "9/300 * Epoch (train):  96% 36618/38253 [2:22:27<06:25,  4.24it/s, loss=0.006]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "9/300 * Epoch (train): 100% 38253/38253 [2:28:51<00:00,  4.28it/s, loss=9.690e-04]\n",
      "9/300 * Epoch (valid): 100% 250/250 [00:15<00:00, 16.33it/s, loss=0.002]    \n",
      "9/300 * Epoch (valid_es): 100% 79/79 [00:05<00:00, 15.51it/s, loss=1.916e-06]\n",
      "9/300 * Epoch (valid_it): 100% 79/79 [00:05<00:00, 15.59it/s, loss=0.008]   \n",
      "9/300 * Epoch (valid_tr): 100% 94/94 [00:06<00:00, 15.63it/s, loss=9.598e-04]\n",
      "[2020-05-13 11:43:09,325] \n",
      "9/300 * Epoch 9 (_base): lr=1.000e-06 | momentum=0.9000\n",
      "9/300 * Epoch 9 (train): auc/_mean=0.9959 | auc/class_0=0.9959 | loss=0.0057\n",
      "9/300 * Epoch 9 (valid): auc/_mean=0.9130 | auc/class_0=0.9130 | es_auc/_mean=0.8981 | es_auc/class_0=0.8981 | es_loss=0.0602 | it_auc/_mean=0.8792 | it_auc/class_0=0.8792 | it_loss=0.0704 | loss=0.0514 | tr_auc/_mean=0.9576 | tr_auc/class_0=0.9576 | tr_loss=0.0269\n",
      "9/300 * Epoch 9 (valid_es): auc/_mean=0.8981 | auc/class_0=0.8981 | loss=0.0602\n",
      "9/300 * Epoch 9 (valid_it): auc/_mean=0.8792 | auc/class_0=0.8792 | loss=0.0704\n",
      "9/300 * Epoch 9 (valid_tr): auc/_mean=0.9576 | auc/class_0=0.9576 | loss=0.0269\n",
      "10/300 * Epoch (train):   3% 1220/38253 [04:44<2:23:39,  4.30it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "10/300 * Epoch (train):   9% 3616/38253 [14:02<2:14:32,  4.29it/s, loss=0.013]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "10/300 * Epoch (train):  22% 8324/38253 [32:15<1:55:51,  4.31it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "10/300 * Epoch (train):  22% 8384/38253 [32:29<1:55:34,  4.31it/s, loss=0.017]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "10/300 * Epoch (train):  33% 12566/38253 [48:44<1:39:57,  4.28it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "10/300 * Epoch (train):  39% 14902/38253 [57:51<1:31:00,  4.28it/s, loss=0.004]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "10/300 * Epoch (train):  44% 16732/38253 [1:04:59<1:23:55,  4.27it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "10/300 * Epoch (train):  55% 20881/38253 [1:21:12<1:07:36,  4.28it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "10/300 * Epoch (train):  61% 23175/38253 [1:30:06<58:29,  4.30it/s, loss=0.005]      Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "10/300 * Epoch (train):  61% 23341/38253 [1:30:45<57:56,  4.29it/s, loss=8.874e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "10/300 * Epoch (train):  69% 26258/38253 [1:42:08<46:55,  4.26it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "10/300 * Epoch (train):  80% 30570/38253 [1:58:59<30:05,  4.26it/s, loss=0.006]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "10/300 * Epoch (train):  85% 32390/38253 [2:06:07<22:59,  4.25it/s, loss=0.005]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "10/300 * Epoch (train):  90% 34526/38253 [2:14:28<14:39,  4.24it/s, loss=0.006]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "10/300 * Epoch (train):  95% 36183/38253 [2:20:58<08:06,  4.26it/s, loss=0.006]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "10/300 * Epoch (train):  96% 36582/38253 [2:22:31<06:33,  4.25it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "10/300 * Epoch (train): 100% 38253/38253 [2:29:04<00:00,  4.28it/s, loss=8.517e-05]\n",
      "10/300 * Epoch (valid): 100% 250/250 [00:15<00:00, 16.25it/s, loss=0.002]    \n",
      "10/300 * Epoch (valid_es): 100% 79/79 [00:05<00:00, 15.52it/s, loss=1.099e-06]\n",
      "10/300 * Epoch (valid_it): 100% 79/79 [00:05<00:00, 15.52it/s, loss=0.004]   \n",
      "10/300 * Epoch (valid_tr): 100% 94/94 [00:06<00:00, 15.64it/s, loss=0.002]    \n",
      "[2020-05-13 14:13:32,918] \n",
      "10/300 * Epoch 10 (_base): lr=1.000e-06 | momentum=0.9000\n",
      "10/300 * Epoch 10 (train): auc/_mean=0.9967 | auc/class_0=0.9967 | loss=0.0051\n",
      "10/300 * Epoch 10 (valid): auc/_mean=0.9168 | auc/class_0=0.9168 | es_auc/_mean=0.9009 | es_auc/class_0=0.9009 | es_loss=0.0639 | it_auc/_mean=0.8833 | it_auc/class_0=0.8833 | it_loss=0.0706 | loss=0.0517 | tr_auc/_mean=0.9628 | tr_auc/class_0=0.9628 | tr_loss=0.0246\n",
      "10/300 * Epoch 10 (valid_es): auc/_mean=0.9009 | auc/class_0=0.9009 | loss=0.0639\n",
      "10/300 * Epoch 10 (valid_it): auc/_mean=0.8833 | auc/class_0=0.8833 | loss=0.0706\n",
      "10/300 * Epoch 10 (valid_tr): auc/_mean=0.9628 | auc/class_0=0.9628 | loss=0.0246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/300 * Epoch (train):   3% 1266/38253 [04:54<2:23:27,  4.30it/s, loss=0.004]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "11/300 * Epoch (train):  20% 7476/38253 [29:01<2:00:27,  4.26it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "11/300 * Epoch (train):  20% 7681/38253 [29:48<1:58:39,  4.29it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "11/300 * Epoch (train):  31% 11843/38253 [45:58<1:42:42,  4.29it/s, loss=0.015]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "11/300 * Epoch (train):  31% 11957/38253 [46:24<1:42:08,  4.29it/s, loss=0.005]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "11/300 * Epoch (train):  44% 16959/38253 [1:05:52<1:22:51,  4.28it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "11/300 * Epoch (train):  60% 22971/38253 [1:29:17<59:39,  4.27it/s, loss=0.001]      Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "11/300 * Epoch (train):  60% 23078/38253 [1:29:42<59:14,  4.27it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "11/300 * Epoch (train):  66% 25220/38253 [1:38:03<50:55,  4.27it/s, loss=0.019]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "11/300 * Epoch (train):  67% 25508/38253 [1:39:11<49:42,  4.27it/s, loss=0.001]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "11/300 * Epoch (train):  68% 26160/38253 [1:41:43<47:11,  4.27it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "11/300 * Epoch (train):  89% 33970/38253 [2:12:12<16:44,  4.26it/s, loss=3.919e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "11/300 * Epoch (train):  95% 36206/38253 [2:20:57<08:01,  4.25it/s, loss=0.001]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "11/300 * Epoch (train):  95% 36283/38253 [2:21:15<07:41,  4.27it/s, loss=0.005]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "11/300 * Epoch (train): 100% 38253/38253 [2:28:57<00:00,  4.28it/s, loss=2.326e-04]\n",
      "11/300 * Epoch (valid): 100% 250/250 [00:15<00:00, 16.23it/s, loss=0.005]    \n",
      "11/300 * Epoch (valid_es): 100% 79/79 [00:05<00:00, 15.56it/s, loss=5.150e-06]\n",
      "11/300 * Epoch (valid_it): 100% 79/79 [00:05<00:00, 15.60it/s, loss=0.042]\n",
      "11/300 * Epoch (valid_tr): 100% 94/94 [00:06<00:00, 15.63it/s, loss=1.211e-04]\n",
      "[2020-05-13 16:43:50,239] \n",
      "11/300 * Epoch 11 (_base): lr=1.000e-06 | momentum=0.9000\n",
      "11/300 * Epoch 11 (train): auc/_mean=0.9973 | auc/class_0=0.9973 | loss=0.0046\n",
      "11/300 * Epoch 11 (valid): auc/_mean=0.9163 | auc/class_0=0.9163 | es_auc/_mean=0.9007 | es_auc/class_0=0.9007 | es_loss=0.0663 | it_auc/_mean=0.8831 | it_auc/class_0=0.8831 | it_loss=0.0767 | loss=0.0552 | tr_auc/_mean=0.9606 | tr_auc/class_0=0.9606 | tr_loss=0.0271\n",
      "11/300 * Epoch 11 (valid_es): auc/_mean=0.9007 | auc/class_0=0.9007 | loss=0.0663\n",
      "11/300 * Epoch 11 (valid_it): auc/_mean=0.8831 | auc/class_0=0.8831 | loss=0.0767\n",
      "11/300 * Epoch 11 (valid_tr): auc/_mean=0.9606 | auc/class_0=0.9606 | loss=0.0271\n",
      "12/300 * Epoch (train):  10% 3960/38253 [15:22<2:13:13,  4.29it/s, loss=0.001]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "12/300 * Epoch (train):  11% 4312/38253 [16:44<2:12:22,  4.27it/s, loss=0.001]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "12/300 * Epoch (train):  22% 8384/38253 [32:34<1:56:18,  4.28it/s, loss=0.009]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "12/300 * Epoch (train):  22% 8436/38253 [32:46<1:56:00,  4.28it/s, loss=0.005]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "12/300 * Epoch (train):  35% 13236/38253 [51:26<1:37:29,  4.28it/s, loss=0.010]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "12/300 * Epoch (train):  40% 15425/38253 [59:58<1:28:51,  4.28it/s, loss=0.004]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "12/300 * Epoch (train):  47% 17917/38253 [1:09:39<1:19:09,  4.28it/s, loss=0.013]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "12/300 * Epoch (train):  57% 21939/38253 [1:25:20<1:03:38,  4.27it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "12/300 * Epoch (train):  58% 22351/38253 [1:26:56<1:02:01,  4.27it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "12/300 * Epoch (train):  60% 22866/38253 [1:28:57<59:46,  4.29it/s, loss=0.003]      Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "12/300 * Epoch (train):  76% 28909/38253 [1:52:30<36:52,  4.22it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "12/300 * Epoch (train):  77% 29524/38253 [1:54:54<34:03,  4.27it/s, loss=0.014]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "12/300 * Epoch (train):  83% 31789/38253 [2:03:45<25:10,  4.28it/s, loss=0.004]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "12/300 * Epoch (train):  85% 32615/38253 [2:06:58<22:00,  4.27it/s, loss=0.005]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "12/300 * Epoch (train):  93% 35748/38253 [2:19:12<09:46,  4.27it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "12/300 * Epoch (train): 100% 38253/38253 [2:28:59<00:00,  4.28it/s, loss=3.331e-04]\n",
      "12/300 * Epoch (valid): 100% 250/250 [00:15<00:00, 16.23it/s, loss=0.004]    \n",
      "12/300 * Epoch (valid_es): 100% 79/79 [00:05<00:00, 15.49it/s, loss=2.670e-05]\n",
      "12/300 * Epoch (valid_it): 100% 79/79 [00:05<00:00, 15.43it/s, loss=0.032]   \n",
      "12/300 * Epoch (valid_tr): 100% 94/94 [00:06<00:00, 15.55it/s, loss=1.424e-04]\n",
      "[2020-05-13 19:14:09,272] \n",
      "12/300 * Epoch 12 (_base): lr=1.000e-06 | momentum=0.9000\n",
      "12/300 * Epoch 12 (train): auc/_mean=0.9977 | auc/class_0=0.9977 | loss=0.0042\n",
      "12/300 * Epoch 12 (valid): auc/_mean=0.9124 | auc/class_0=0.9124 | es_auc/_mean=0.8942 | es_auc/class_0=0.8942 | es_loss=0.0698 | it_auc/_mean=0.8820 | it_auc/class_0=0.8820 | it_loss=0.0768 | loss=0.0567 | tr_auc/_mean=0.9571 | tr_auc/class_0=0.9571 | tr_loss=0.0280\n",
      "12/300 * Epoch 12 (valid_es): auc/_mean=0.8942 | auc/class_0=0.8942 | loss=0.0698\n",
      "12/300 * Epoch 12 (valid_it): auc/_mean=0.8820 | auc/class_0=0.8820 | loss=0.0768\n",
      "12/300 * Epoch 12 (valid_tr): auc/_mean=0.9571 | auc/class_0=0.9571 | loss=0.0280\n",
      "13/300 * Epoch (train):   1% 449/38253 [01:44<2:26:38,  4.30it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "13/300 * Epoch (train):  18% 6702/38253 [26:01<2:03:06,  4.27it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "13/300 * Epoch (train):  22% 8435/38253 [32:45<1:55:38,  4.30it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "13/300 * Epoch (train):  27% 10486/38253 [40:43<1:48:09,  4.28it/s, loss=0.005]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "13/300 * Epoch (train):  38% 14646/38253 [56:54<1:31:46,  4.29it/s, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "13/300 * Epoch (train):  43% 16278/38253 [1:03:15<1:25:46,  4.27it/s, loss=6.887e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "13/300 * Epoch (train):  46% 17470/38253 [1:07:53<1:20:52,  4.28it/s, loss=0.008]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "13/300 * Epoch (train):  54% 20802/38253 [1:20:51<1:07:55,  4.28it/s, loss=6.416e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/300 * Epoch (train):  67% 25577/38253 [1:39:28<49:41,  4.25it/s, loss=0.004]      Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "13/300 * Epoch (train):  67% 25758/38253 [1:40:10<48:48,  4.27it/s, loss=0.009]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "13/300 * Epoch (train):  78% 29662/38253 [1:55:24<33:26,  4.28it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "13/300 * Epoch (train):  83% 31787/38253 [2:03:42<25:13,  4.27it/s, loss=7.974e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "13/300 * Epoch (train):  94% 36144/38253 [2:20:41<08:14,  4.26it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "13/300 * Epoch (train):  97% 37169/38253 [2:24:41<04:13,  4.27it/s, loss=3.776e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "13/300 * Epoch (train): 100% 38253/38253 [2:28:56<00:00,  4.28it/s, loss=4.689e-04]\n",
      "13/300 * Epoch (valid): 100% 250/250 [00:15<00:00, 16.28it/s, loss=0.002]    \n",
      "13/300 * Epoch (valid_es): 100% 79/79 [00:05<00:00, 15.52it/s, loss=6.115e-06]\n",
      "13/300 * Epoch (valid_it): 100% 79/79 [00:05<00:00, 15.58it/s, loss=0.004]    \n",
      "13/300 * Epoch (valid_tr): 100% 94/94 [00:06<00:00, 15.50it/s, loss=0.003]    \n",
      "[2020-05-13 21:44:26,561] \n",
      "13/300 * Epoch 13 (_base): lr=1.000e-06 | momentum=0.9000\n",
      "13/300 * Epoch 13 (train): auc/_mean=0.9981 | auc/class_0=0.9981 | loss=0.0038\n",
      "13/300 * Epoch 13 (valid): auc/_mean=0.9179 | auc/class_0=0.9179 | es_auc/_mean=0.9005 | es_auc/class_0=0.9005 | es_loss=0.0650 | it_auc/_mean=0.8883 | it_auc/class_0=0.8883 | it_loss=0.0729 | loss=0.0548 | tr_auc/_mean=0.9594 | tr_auc/class_0=0.9594 | tr_loss=0.0300\n",
      "13/300 * Epoch 13 (valid_es): auc/_mean=0.9005 | auc/class_0=0.9005 | loss=0.0650\n",
      "13/300 * Epoch 13 (valid_it): auc/_mean=0.8883 | auc/class_0=0.8883 | loss=0.0729\n",
      "13/300 * Epoch 13 (valid_tr): auc/_mean=0.9594 | auc/class_0=0.9594 | loss=0.0300\n",
      "14/300 * Epoch (train):   7% 2539/38253 [09:51<2:18:53,  4.29it/s, loss=2.301e-04]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "14/300 * Epoch (train):  17% 6677/38253 [26:02<2:05:38,  4.19it/s, loss=0.014]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "14/300 * Epoch (train):  17% 6681/38253 [26:03<2:01:55,  4.32it/s, loss=0.003]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "14/300 * Epoch (train):  20% 7587/38253 [29:40<2:02:43,  4.16it/s, loss=0.010]    "
     ]
    }
   ],
   "source": [
    "def freeze_until(net, param_name):\n",
    "    found_name = False\n",
    "    for name, params in net.named_parameters():\n",
    "        if name == param_name:\n",
    "            found_name = True\n",
    "        params.requires_grad = found_name\n",
    "        \n",
    "# param_name = 'bert_model.encoder.layer.10.attention.self.query.weight'\n",
    "param_name = 'bert_model.encoder.layer.1.attention.self.query.weight'\n",
    "\n",
    "# CHECKPOINT = './checkpoints/v93_xlm_roberta_large_ml96/best.pth'\n",
    "\n",
    "project = \"js_v10y_xlmrbtlg_less_freeze\"\n",
    "num_epochs = 300\n",
    "\n",
    "group = datetime.now().strftime(\"%m_%d_%Y__%H_%M_%S\")\n",
    "\n",
    "    \n",
    "if SERVER:\n",
    "    group = f'{group}_server'\n",
    "    \n",
    "if STRIDE > 1:\n",
    "    group = f'{group}_str{STRIDE}'\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "lr = 1e-6#1e-5#0.0001\n",
    "group += f'_lr{lr}'\n",
    "\n",
    "group = group.replace('.', '')\n",
    "\n",
    "runner = SupervisedRunner(input_key=('features'), input_target_key=('targets'), output_key=('logits'))\n",
    "\n",
    "experiment = 'simple'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "logdir = f\"{LOG_PATH}/{project}/{group}/{experiment}\"\n",
    "\n",
    "model = QuestModel(2)\n",
    "\n",
    "# checkpoint = torch.load(CHECKPOINT)#, map_location=device)   \n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# del checkpoint\n",
    "\n",
    "freeze_until(model, param_name)\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "loaders = get_loaders(to_balance=True)\n",
    "\n",
    "\n",
    "t_total = len(loaders['train'])//gradient_accumulation_steps*num_epochs\n",
    "warmup_proportion = 0.01\n",
    "num_warmup_steps = t_total * warmup_proportion\n",
    "\n",
    "#     criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion = FocalLoss(alpha=0.2, gamma=1.5, logits=True, reduce=True)\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "#     criterion = torch.nn.BCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr = lr)\n",
    "#    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=t_total) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.25)\n",
    "print(f'----------------Experiment: {experiment}')\n",
    "\n",
    "runner.train(\n",
    "    fp16=dict(opt_level=\"O2\") ,\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    verbose=True,\n",
    "    distributed=False if is_jupyter() else True,\n",
    "    callbacks=[\n",
    "        AlchemyLogger(\n",
    "                token=token, # your Alchemy token\n",
    "                project=project,\n",
    "                experiment=experiment,\n",
    "                group=group,\n",
    "            ),\n",
    "        MyAUCCallback()\n",
    "\n",
    "    ],\n",
    "    main_metric='auc/_mean',\n",
    "    minimize_metric=False,\n",
    "    \n",
    "    #fp16={\"opt_level\": \"O1\"}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in df_train.iterrows():\n",
    "#     print(list(row.keys().values))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = '/media/ssd/logs/jigsaw/js_v10_xlmrbtlg_less_freeze/05_09_2020__22_37_32_lr1e-05_vld_vlden_psd_myback_in_trn/simple/'+'/checkpoints/best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuestModel(2)\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT)#, map_location=device)   \n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv').rename(columns={'content':'comment_text'})\n",
    "ds = QuestDataset(df_test, train_mode=False, labeled=False)\n",
    "loader = DataLoader(\n",
    "        ds,\n",
    "        num_workers=8,\n",
    "        batch_size=batch_size,\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.zeros( len(ds), 2).to(device)\n",
    "\n",
    "j = 0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(loader):\n",
    "        x = data['features'].to(device)\n",
    "        thish_batch_size = len(x)            \n",
    "        \n",
    "        \n",
    "        y_pred[j:j+thish_batch_size] = model(x)\n",
    "            \n",
    "        j += thish_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import sigmoid\n",
    "y_pred_sigmoid = sigmoid(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sigmoid_one_np = y_pred_sigmoid.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['toxic'] = y_pred_sigmoid_one_np[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submission_v10x_0955.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel('/media/ssd/yadisk/boost2b/data/sites/moscow.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[~df['Сайт'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['Сайт'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/ssd/yadisk/boost2b/data/sites/'\n",
    "df = None\n",
    "for f in tqdm(os.listdir(path)):\n",
    "    try:\n",
    "        if df is None:\n",
    "            df = pd.read_excel(os.path.join(path,f))\n",
    "        else:\n",
    "            df = df.append(pd.read_excel(os.path.join(path,f)))\n",
    "    except:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Сайт'].nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jig_env",
   "language": "python",
   "name": "jig_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "086421f7eec44c769f08f5f68fafafdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0e7c81c0da784e04b530236bad005c33": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18b951cd8c1447eaa1e12f972ac37d42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "  9%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9e743ecd26cd4877a539f9bbbd23d114",
       "max": 308,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e34fafda1e234d9c981322beff1068e7",
       "value": 29
      }
     },
     "35645b239e914aee807cfdc234fc5b75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dfdee9b109bc4176a0bc3a26ae38f7a3",
       "placeholder": "​",
       "style": "IPY_MODEL_086421f7eec44c769f08f5f68fafafdc",
       "value": " 29/308 [7:12:46&lt;66:18:40, 855.63s/it]"
      }
     },
     "554271f3ed5d48efa844608a0b72fdac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "685ecca481e3463d83a2465f15f7b33f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_80ae0d356c3e4b1bbe511c5f5a2694b5",
       "max": 4059,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a34804b28be74d7c9972a0a13410ba77",
       "value": 4059
      }
     },
     "7f49d7d685554687bc2d131959058cb3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80ae0d356c3e4b1bbe511c5f5a2694b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "840e74b11eb44a58b1ed0433e924dc82": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9e743ecd26cd4877a539f9bbbd23d114": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a34804b28be74d7c9972a0a13410ba77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "a61c8f56dbd74e29a155fad0d7095f79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_18b951cd8c1447eaa1e12f972ac37d42",
        "IPY_MODEL_35645b239e914aee807cfdc234fc5b75"
       ],
       "layout": "IPY_MODEL_7f49d7d685554687bc2d131959058cb3"
      }
     },
     "c9ab84b07a32426282543b5ff054ddec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_840e74b11eb44a58b1ed0433e924dc82",
       "placeholder": "​",
       "style": "IPY_MODEL_554271f3ed5d48efa844608a0b72fdac",
       "value": " 4059/4059 [14:53&lt;00:00,  4.54it/s]"
      }
     },
     "dfdee9b109bc4176a0bc3a26ae38f7a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e34fafda1e234d9c981322beff1068e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "fd692cff30e343aa8afa007a05d66acc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_685ecca481e3463d83a2465f15f7b33f",
        "IPY_MODEL_c9ab84b07a32426282543b5ff054ddec"
       ],
       "layout": "IPY_MODEL_0e7c81c0da784e04b530236bad005c33"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
